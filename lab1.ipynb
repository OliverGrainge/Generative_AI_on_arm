{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Optimizing Generative AI Workloads with Embedded ARM Processors\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the **Optimizing Generative AI Workloads with Embedded ARM Processors** lab! In this hands-on session, you will explore how ARMv8-A advanced vector processing capabilities can significantly accelerate computationally intensive tasks commonly found in artificial intelligence (AI) applications. By the end of this lab, you will gain a deep understanding of how low-level optimizations can enhance performance and how to leverage these optimizations within high-level AI frameworks like PyTorch.\n",
    "\n",
    "**Requirements**: To complete this lab, you will need an ARMv8-A 64-bit system running a Linux-based operating system, such as a Raspberry Pi 4 or 5. The lab has been thoroughly tested on the Raspberry Pi 5 for compatibility and performance.\n",
    "\n",
    "### **Why ARM Vector Instructions?**\n",
    "\n",
    "ARM processors are ubiquitous in modern computing, powering everything from smartphones to edge devices and increasingly, servers and supercomputers. Their architecture is designed for energy efficiency and performance, making them ideal for deploying AI models in diverse environments. One of the key features that enable ARM processors to excel in AI workloads is their support for **vector instructions**, such as NEON and i8mm, which allow for parallel processing of multiple data points in a single instruction cycle. ARMv8-A is a member of the ARM architecture family, representing the 8th generation of ARM's advanced architecture with a focus on 64-bit computing, high performance, and scalable designs. It builds upon the energy-efficient foundation of earlier ARM architectures while introducing advanced features such as support for AArch64 (the 64-bit instruction set), enhanced vector processing capabilities, and improved cryptographic extensions. ARMv8-A is widely adopted in devices a range of mobile and embedded device including the **raspberry pi 4/5**\n",
    "\n",
    "### **Lab Objectives**\n",
    "\n",
    "The objectives of this lab are as follows: \n",
    "\n",
    "1. **Matrix Multiplication Optimization**:\n",
    "   - Analyze three C-based matrix multiplication implementations (`naive`, `fp32_neon`, and `int8_neon`) to understand their performance differences.\n",
    "   - Utilize ARMv8's NEON vector processing capability to accelerate matrix multiplications. The fundamental operation of AI workloads. \n",
    "\n",
    "2. **Benchmark PyTorch Operations**:\n",
    "   - Measure the performance of PyTorch's matrix multiplication operations in different precisions. \n",
    "   - Examine the generated assembly code to identify how pytorch accelerates matrix multiplications\n",
    "\n",
    "3. **Run Inference on a Language Model**:\n",
    "   - Load and run inference on the state-of-the-art small language model, Llama3.2-1B.\n",
    "   - Explore its computational graph to understand the underlying operations.\n",
    "\n",
    "4. **Eager vs. Graph Execution**:\n",
    "   - Compare PyTorch's Eager mode and Graph Execution.\n",
    "   - Use `torch.compile` to minimize overhead and improve runtime performance.\n",
    "\n",
    "5. **Apply Integer Quantization**:\n",
    "   - Learn how integer quantization can reduce model size and accelerate inference.\n",
    "   - Implement and evaluate quantization on matrix multiplication operations.\n",
    "   - Apply Integer Quantization to Lamma3.2-1B and record it's latency speed ups and memory reductions. \n",
    "\n",
    "\n",
    "\n",
    "### **What You Will Learn**\n",
    "\n",
    "- **Low-Level Optimizations**: Understand how ARM's NEON SIMD instructions can accelerate matrix computations.\n",
    "- **Performance Benchmarking**: Develop skills to benchmark operations effectively and analyze performance trade-offs.\n",
    "- **Framework Integration**: Explore how low-level optimizations can complement high-level frameworks like PyTorch.\n",
    "- **Quantization Techniques**: Learn to implement quantization for efficient AI workloads on resource-constrained devices.\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "To follow this lab, you should have a basic understanding of:\n",
    "\n",
    "- **C Programming**: Familiarity with C syntax and basic memory management.\n",
    "- **Python Programming**: Experience writing and running Python scripts.\n",
    "- **Matrix Operations**: A general understanding of matrix multiplication\n",
    "\n",
    "### **Getting Started**\n",
    "\n",
    "This lab begins with an exploration of matrix multiplication operators, the core computational components of AI workloads. Understanding and optimizing these operations can significantly enhance the inference performance of generative AI (GenAI) models. In this section, we will implement and compare three matrix multiplication approaches. While they are mathematically identical, their differing implementations result in vastly different performance outcomes. We will implement: \n",
    "\n",
    "1. **Naive Kernel (`src/c/kernels/naive.c`)**  \n",
    "   - A simple, baseline implementation of the matrix multiplication to provide a reference point for performance.\n",
    "\n",
    "2. **FP32 NEON Kernel (`src/c/kernels/fp32_neon.c`)**  \n",
    "   - A matrix multiplication optimized for single-precision floating-point operations using ARM NEON SIMD instructions to leverage vectorized computation.\n",
    "\n",
    "3. **INT8 NEON Kernel (`src/c/kernels/int8_neon.c`)**  \n",
    "   - A integer matrix multiplication tailored for 8-bit operations, utilizing NEON SIMD to maximize throughput for lower-precision workloads.\n",
    "\n",
    "By analyzing these implementations, you will gain insight into the performance trade-offs and benefits of hardware-specific optimizations that ARM can offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematics of Matrix Multiplication and Its Importance in Generative AI Models**\n",
    "\n",
    "#### **Basic Mathematics of Matrix Multiplication**\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra with wide-ranging applications in computer science, engineering, and especially in machine learning and AI. Given two matrices **A** and **B**, the product **C = A × B** is defined only if the number of columns in **A** matches the number of rows in **B**.\n",
    "\n",
    "Mathematically, if **A** is an *m × n* matrix and **B** is an *n × p* matrix, then their product **C** will be an *m × p* matrix. The element at position *(i, j)* in matrix **C** is computed as:\n",
    "\n",
    "$$C_{i,j} = \\sum_{k=1}^{n} A_{i,k} \\times B_{k,j}$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider matrices **A** (2×3) and **B** (3×2):\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Their product **C = A × B** (2×2) is:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{11} &= a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} \\\\\n",
    "c_{12} &= a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} \\\\\n",
    "c_{21} &= a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} \\\\\n",
    "c_{22} &= a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### **Computational Complexity**\n",
    "\n",
    "The naive matrix multiplication algorithm has a time complexity of **O(n³)**, which becomes computationally expensive for large matrices. Optimizations, such as those leveraging vector instructions, aim to reduce the constant factors and improve cache utilization, thereby enhancing performance without altering the theoretical complexity.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### **Implementation of Naive Matrix Multiplication Kernel**\n",
    "\n",
    "The naive implementation of matrix multiplication directly follows the mathematical definition. It uses three nested loops to compute the dot product of rows from matrix **A** and columns from matrix **B** for every element in the resulting matrix **C**. This is the most straightforward and intuitive approach but is computationally expensive due to its **O(n³)** time complexity.\n",
    "\n",
    "#### **Code Explanation**\n",
    "\n",
    "1. **Inputs**:\n",
    "   - **A**, **B**: Flattened 2D matrices (stored as 1D arrays in row-major order) to be multiplied.\n",
    "   - **C**: Flattened 2D matrix (1D array, also in row-major) to store the result.\n",
    "   - **N**: Size of the square matrices (number of rows/columns).\n",
    "\n",
    "2. **Procedure**:\n",
    "   - The outer two loops iterate over the rows **i** and columns **j** of the resulting matrix **C**.\n",
    "   - The innermost loop calculates the dot product for each element **C[i, j]** by summing the product of corresponding elements from row **i** of **A** and column **j** of **B**.\n",
    "\n",
    "3. **Performance**:\n",
    "   - This naive implementation is simple, performing one operation at a time but does not leverage advanced optimization techniques, such as blocking, vectorization, or parallelism.\n",
    "\n",
    "#### **Naive Kernel Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src/c/kernels’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir src/c/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/naive.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/naive.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "void matrix_multiply_naive(float* A, float* B, float* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            C[i * N + j] = 0;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i * N + j] += A[i * N + k] * B[k * N + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to NEON SIMD Instructions**\n",
    "\n",
    "NEON (ARM Advanced SIMD) is a set of SIMD (Single Instruction, Multiple Data) instructions designed to accelerate data-parallel operations on ARM processors. By leveraging NEON, computations on matrices, such as matrix multiplication, can be vectorized to process multiple data points simultaneously, thereby reducing execution time and increasing throughput.\n",
    "\n",
    "#### **FP32 Matrix Multiplication Using NEON Instructions**\n",
    "\n",
    "1. **Vectorized Processing**:\n",
    "   - In this implementation, the function processes four `float32` elements at a time, utilizing NEON's 128-bit registers (`float32x4_t`).\n",
    "   - The accumulation is performed using fused multiply-add operations, which minimize intermediate memory accesses.\n",
    "\n",
    "2. **Key NEON Instructions Used**:\n",
    "   - **`vld1q_f32`**: Loads four 32-bit floating-point elements into a vector register.\n",
    "   - **`vmlaq_f32`**: Performs a fused multiply-add operation on vectors.\n",
    "     \n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - The vector accumulator is reduced into a scalar using:\n",
    "     - **`vadd_f32`**: Adds low and high parts of the vector.\n",
    "     - **`vpadd_f32`**: Horizontally adds remaining elements for a final scalar result.\n",
    "     - **`vget_lane_f32`**: Extracts a specific element (lane) from a vector, used here to retrieve the final scalar value from the result of `vpadd_f32`.\n",
    "       \n",
    "       \n",
    "4. **Advantages Over Naive FP32 Implementation**:\n",
    "   - **Efficient Memory Access**: The naive implementation loads a single value at a time from memory, which can result in significant memory latency. The NEON implementation processes four elements simultaneously, reducing memory fetch overhead.\n",
    "   - **Reduced Loop Iterations**: By processing multiple elements in parallel, the NEON implementation reduces the number of loop iterations required in the inner loop, significantly improving performance for large matrices.\n",
    "   - **Optimized Accumulation**: NEON's fused multiply-add (`vmlaq_f32`) performs multiplication and addition in a single instruction, minimizing intermediate storage and computation overhead, unlike the naive implementation, which performs these operations sequentially.\n",
    "   - **Hardware Acceleration**: NEON leverages specialized SIMD hardware in the ARM processor, making it much faster than the general-purpose computation used in the naive implementation.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for `float32` matrices by iterating over the rows and columns of the input matrices, processing four elements at a time in the inner loop. This approach uses NEON to accelerate the computation by leveraging SIMD parallelism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/fp32_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/fp32_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_fp32_neon(float* A, float* B, float* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            float32x4_t acc = vdupq_n_f32(0.0f); // Accumulator\n",
    "            for (int k = 0; k < N; k += 4) { // Process 4 elements at a time\n",
    "                float32x4_t a_vec = vld1q_f32(&A[i * N + k]); // Load row of A\n",
    "                float32x4_t b_vec = vld1q_f32(&B[k * N + j]); // Load column of B\n",
    "                acc = vmlaq_f32(acc, a_vec, b_vec); // Multiply-accumulate\n",
    "            }\n",
    "            // Reduce acc to a single value and store in C\n",
    "            float32x2_t sum1 = vadd_f32(vget_low_f32(acc), vget_high_f32(acc));\n",
    "            float sum = vget_lane_f32(vpadd_f32(sum1, sum1), 0);\n",
    "            C[i * N + j] = sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantized (INT8) Matrix Multiplication Using NEON SIMD**\n",
    "\n",
    "Quantized matrix multiplication uses low-precision. In this case, 8-bit integer representations to reduce memory bandwidth, power consumption, and computational complexity. ARM processors with NEON (integer 8-bit matrix multiplication) instructions provide significant acceleration by increasing the level of vectorization and benefiting from the efficiency of integer arithmetic.\n",
    "\n",
    "#### **INT8 Matrix Multiplication Using NEON Instructions**\n",
    "\n",
    "1. **Increased Levels of Vectorization**:\n",
    "   - The implementation processes eight `int8_t` elements at a time, leveraging the higher data packing density of 8-bit integers compared to `float32` (four elements at a time). This doubles the level of parallelism compared to the FP32 implementation.\n",
    "\n",
    "2. **Key NEON Instructions Used**:\n",
    "   - **`vld1_s8`**: Loads eight signed 8-bit integers into a vector register.\n",
    "   - **`vmlal_s8`**: Multiplies two vectors of signed 8-bit integers and accumulates the results into 16-bit integers.\n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - **`vaddvq_s16`**: Horizontally sums the elements of a 16-bit integer vector to produce a scalar result.\n",
    "\n",
    "4. **Advantages Over FP32 Implementation**:\n",
    "   - **Higher Vectorization**: Processes eight elements at a time versus four in the FP32 version.\n",
    "   - **Integer Arithmetic**: Integer operations are inherently faster than floating-point operations on most hardware due to simpler hardware requirements.\n",
    "   - **Lower Memory Usage**: `int8_t` data consumes four times less memory than `float32`, leading to reduced cache pressure and better memory bandwidth utilization.\n",
    "   - **Energy Efficiency**: Integer computations typically consume less power, making this approach ideal for energy-constrained environments.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for quantized `int8` matrices by iterating over the rows and columns of the input matrices. In the inner loop, eight elements are processed simultaneously using NEON SIMD instructions. The 16-bit intermediate results are accumulated, and the final reduction produces a 32-bit scalar result for each element of the output matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/int8_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/int8_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_int8_neon(int8_t* A, int8_t* B, int32_t* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            int16x8_t acc = vdupq_n_s16(0); // Initialize 16-bit accumulator\n",
    "\n",
    "            for (int k = 0; k < N; k += 8) { // Process 8 elements at a time\n",
    "                // Load 8 int8 elements from row of A and column of B\n",
    "                int8x8_t a_vec = vld1_s8(&A[i * N + k]);\n",
    "                int8x8_t b_vec = vld1_s8(&B[k * N + j]);\n",
    "\n",
    "                // Perform element-wise multiplication and accumulate\n",
    "                acc = vmlal_s8(acc, a_vec, b_vec);\n",
    "            }\n",
    "\n",
    "            // Reduce the 16-bit accumulator into a 32-bit scalar\n",
    "            int32_t sum = vaddvq_s16(acc); // Horizontally sum all elements in the vector\n",
    "            C[i * N + j] = sum; // Store the result in C\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compiling Each Kernel to Inspect Assembly Using GCC**\n",
    "\n",
    "With the kernel implementations ready, we can compile them into assembly code to examine how the compiler leverages NEON hardware-level optimizations to enhance performance. This step provides insights into how SIMD instructions are utilized for accelerating computations, particularly for matrix operations.\n",
    "\n",
    "#### **Steps to Compile Each Kernel**\n",
    "\n",
    "To inspect the generated assembly code for each kernel, use the following GCC command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘bin’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir bin\n",
    "!gcc -O0 -S -march=armv8-a+simd src/c/kernels/naive.c -o bin/naive.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/fp32_neon.c -o bin/fp32_neon.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/int8_neon.c -o bin/int8_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. **Naive Implementation**\n",
    "   - Uses scalar instructions exclusively, meaning each operation processes a single pair of data values at a time.\n",
    "   - Relies on the following types of ARM assembly instructions:\n",
    "     - **`ldr` (Load Register):** Loads a single value from memory into a register.\n",
    "     - **`str` (Store Register):** Stores a single value from a register into memory.\n",
    "     - **`mul` (Multiply):** Multiplies two values in registers.\n",
    "     - **`add` (Add):** Adds two values in registers.\n",
    "   - Does not utilize SIMD (Single Instruction Multiple Data) capabilities, which can process multiple data values simultaneously in a single instruction.\n",
    "   - Experiences significant overhead in memory operations due to the frequent use of `ldr` and `str` instructions for each operation, as no batching or parallelism is applied.\n",
    "\n",
    "#### Observations:\n",
    "   - Computational units are underutilized because operations are performed serially, one at a time.\n",
    "   - Memory bandwidth becomes a bottleneck as frequent loads and stores slow down processing.\n",
    "   - Best suited for small matrices or architectures without support for vectorized operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".L5:\n",
      "\tldr\tw1, [sp, 44]\n",
      "\tldr\tw0, [sp, 4]\n",
      "\tmul\tw1, w1, w0\n",
      "\tldr\tw0, [sp, 40]\n",
      "\tadd\tw0, w1, w0\n",
      "\tsxtw\tx0, w0\n",
      "\tlsl\tx0, x0, 2\n",
      "\tldr\tx1, [sp, 8]\n",
      "\tadd\tx0, x1, x0\n",
      "\tldr\ts1, [x0]\n",
      "\tldr\tw1, [sp, 44]\n",
      "\tldr\tw0, [sp, 4]\n",
      "\tmul\tw1, w1, w0\n",
      "\tldr\tw0, [sp, 36]\n",
      "\tadd\tw0, w1, w0\n",
      "\tsxtw\tx0, w0\n"
     ]
    }
   ],
   "source": [
    "!sed -n '34,50p' bin/naive.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. **NEON Vectorization**\n",
    "   - Leverages NEON SIMD instructions to perform parallel computations on multiple data values simultaneously:\n",
    "     - **`movi` (Move Immediate):** Initializes all elements of a NEON vector register to a specified immediate value (e.g., zero).\n",
    "     - **`fmul` (Floating-Point Multiply):** Multiplies corresponding elements in two NEON vector registers.\n",
    "     - **`fadd` (Floating-Point Add):** Adds corresponding elements in two NEON vector registers.\n",
    "     - **`dup` (Duplicate):** Copies a scalar value into all elements of a vector register or duplicates one element of a vector across a scalar register.\n",
    "     - **`faddp` (Floating-Point Add Pair):** Adds adjacent pairs of elements within a vector register, effectively reducing the vector size.\n",
    "   - Processes 128-bit registers, enabling parallel computation of up to 4 single-precision floating-point numbers in a single instruction.\n",
    "   - Utilizes optimized memory access patterns to minimize latency and bottlenecks.\n",
    "\n",
    "#### Observations:\n",
    "   - The extracted assembly demonstrates the use of NEON vector registers (e.g., `v0`, `v1`, `v2`) and instructions for efficient parallel floating-point computations.\n",
    "   - While the exact addresses or register assignments may vary slightly due to compiler differences, the core operations and use of NEON SIMD instructions remain consistent.\n",
    "\n",
    "***NOTE***\n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/fp32_neon.s` file to view the full assembly. Regardless of compiler variations, you should see similar operations (e.g., `movi`, `fmul`, `fadd`, `dup`, `faddp`) utilizing NEON vector registers to perform SIMD optimizations.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv1.4s, 0               // Initialize all elements of v1 to zeros\n",
    "fmul\tv0.4s, v0.4s, v2.4s   // Multiply corresponding elements of v0 and v2, store result in v0\n",
    "fadd\tv1.4s, v1.4s, v0.4s   // Add corresponding elements of v1 and v0, store result in v1\n",
    "dup\td0, v1.d[1]            // Duplicate the second 64-bit element of v1 into scalar register d0\n",
    "fadd\tv0.2s, v0.2s, v1.2s   // Add the lower two elements of v0 and v1, store result in v0.2s\n",
    "faddp\tv0.2s, v0.2s, v0.2s   // Pairwise add elements of v0.2s, reducing it to one scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv1.4s, 0\n",
      "\tfmul\tv0.4s, v0.4s, v2.4s\n",
      "\tfadd\tv1.4s, v1.4s, v0.4s\n",
      "\tdup\td0, v1.d[1]\n",
      "\tfadd\tv0.2s, v0.2s, v1.2s\n",
      "\tfaddp\tv0.2s, v0.2s, v0.2s\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/fp32_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **NEON Vectorization (Integer8-based)**\n",
    "\n",
    "This implementation leverages NEON SIMD instructions for efficient integer matrix multiplication. While it does **not** utilize i8mm-specific instructions, it achieves high performance with quantized data using the following NEON operations:\n",
    "\n",
    "#### Key NEON Instructions Observed:\n",
    "   - **`movi` (Move Immediate):** Initializes all elements of a NEON vector register to a specified immediate value, such as zero.\n",
    "   - **`smlal` (Signed Multiply-Add Long):** Multiplies pairs of 8-bit integers from two vector registers, producing 16-bit results, and accumulates them into a 16-bit vector register.\n",
    "   - **`addv` (Add Across Vector):** Horizontally sums all elements of a NEON vector register into a single scalar value.\n",
    "   - **`smov` (Scalar Move):** Moves the lowest element from a NEON vector register into a scalar general-purpose register.\n",
    "\n",
    "#### Characteristics:\n",
    "   - Processes **8 `int8_t` elements at a time** due to NEON's 128-bit vector registers, allowing for significantly higher throughput compared to `float32` implementations.\n",
    "   - Accumulates intermediate results in 16-bit registers (`int16`) to avoid overflow during the computation.\n",
    "   - Reduces the final 16-bit vector to a scalar using a horizontal sum (`addv`) followed by moving the scalar value to a general-purpose register (`smov`).\n",
    "\n",
    "#### Observations:\n",
    "   - **Performance Benefits**:\n",
    "     - Processes multiple `int8` elements per instruction, maximizing the benefits of vectorization.\n",
    "     - Smaller data types (`int8` vs. `float32`) reduce memory bandwidth requirements and improve efficiency.\n",
    "   - **Precision Limitations**:\n",
    "     - Integer arithmetic lacks the precision of floating-point operations, making it best suited for quantized workloads like neural network inference where reduced precision is acceptable.\n",
    "   - **Applications**:\n",
    "     - Ideal for embedded systems, mobile devices, and other environments requiring low-power, memory-efficient AI inference.\n",
    "\n",
    "***NOTE***  \n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/int8_neon.s` file to view the assembly. Regardless of compiler or system variations, the assembly will use similar operations (e.g., `movi`, `smlal`, `addv`, `smov`) to optimize the `int8` matrix multiplication using NEON SIMD instructions.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv0.4s, 0               // Initialize v0 to zeros\n",
    "smlal\tv0.8h, v2.8b, v1.8b   // Multiply 8-bit integers from v2 and v1, accumulate into 16-bit vector v0\n",
    "addv\th0, v0.8h             // Horizontally sum all elements in v0.8h into scalar h0\n",
    "smov\tw0, v0.h[0]           // Move the lowest 16-bit value from v0 to scalar register w0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv0.4s, 0\n",
      "\tsmlal\tv0.8h, v2.8b, v1.8b\n",
      "\taddv\th0, v0.8h\n",
      "\tsmov\tw0, v0.h[0]\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/int8_neon.s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BenchMarking**\n",
    "\n",
    "Lets now compute the latency of results of these three operators across different matrix sizes top emprically measure their differences. We can set the matrix sizes used in the benchmark by writing them out to **src/c/sizes.c** as seen below. Feel free to adapt the sizes yourself to see how it can effect latency, Bear in mind however large matrix multiplications are compute intensive operations!\n",
    "\n",
    "***Note***\n",
    "This code is tested on raspberry pi 5 (Cortex-A76) with matmul size of 1024 can take upto 45s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/sizes.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/sizes.c\n",
    "\n",
    "int sizes[] = {32, 64, 128, 256, 512};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the naive implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Matrix Multiplication (Size 32): 0.000298 seconds\n",
      "Naive Matrix Multiplication (Size 64): 0.002343 seconds\n",
      "Naive Matrix Multiplication (Size 128): 0.019973 seconds\n",
      "Naive Matrix Multiplication (Size 256): 0.116517 seconds\n",
      "Naive Matrix Multiplication (Size 512): 1.438271 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compile the C code with optimization level 3 and NEON SIMD extensions, outputting the binary to bin/benchmark\n",
    "!gcc -O0 src/c/benchmark_naive.c -o bin/benchmark_naive -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the floating point SIMD implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 NEON Matrix Multiplication (Size 32): 0.000008 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 64): 0.000059 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 128): 0.000462 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 256): 0.008002 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 512): 0.143779 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_fp32_neon.c -o bin/benchmark_fp32_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_fp32_neon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the integer-8 SIMD Impelmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 Neon Matrix Multiplication (Size 32): 0.000005 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 64): 0.000035 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 128): 0.000247 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 256): 0.001876 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 512): 0.032232 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_int8_neon.c -o bin/benchmark_int8_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_int8_neon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot the results** \n",
    "Great, now the operators have been run, lets plot how there latency scales with matrix size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data from CSV files\n",
    "naive_data = pd.read_csv('results/naive_latency_results.csv')\n",
    "fp32_neon_data = pd.read_csv('results/fp32_neon_latency_results.csv')\n",
    "int8_neon_data = pd.read_csv('results/int8_neon_latency_results.csv')\n",
    "\n",
    "# Extract sizes and times\n",
    "sizes = naive_data['Matrix Size']\n",
    "naive_times = naive_data['Latency (seconds)']\n",
    "fp32_neon_times = fp32_neon_data['Latency (seconds)']\n",
    "int8_neon_times = int8_neon_data['Latency (seconds)']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, naive_times, marker='o', linestyle='-', color='r', label='Naive fp32')\n",
    "plt.plot(sizes, fp32_neon_times, marker='o', linestyle='-', color='b', label='NEON SIMD fp32')\n",
    "plt.plot(sizes, int8_neon_times, marker='o', linestyle='-', color='g', label='NEON SIMD int8')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: fp32 Naive vs fp32 SIMD vs int8 SIMD Matrix Multiplication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph, the naive approach, which processes each matrix element individually, is the slowest. The SIMD floating-point implementation offers significant speedup, but the SIMD integer implementation is the fastest as it's the int8 lower bit-width allows a higher degree of parallelism. This highlights the advantage of using lower-precision operations in AI network forward passes for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pytorch**\n",
    "Having explored how ARM's SIMD capabilities in C can accelerate AI workloads, we now transition to PyTorch which is a versatile, high-level framework that balances flexibility and ease of use. PyTorch streamlines AI model development by abstracting low-level operations, enabling developers to focus on model architecture and experimentation rather than intricate implementation details.\n",
    "\n",
    "Unlike the manual coding required for SIMD operations in C, PyTorch offers built-in support for tensor computations and hardware acceleration. It automatically utilizes processor optimizations, including SIMD and ARM's NEON instructions, to enhance performance without requiring developers to write low-level code.\n",
    "\n",
    "In the next section, we will examine how PyTorch utilizes the ARMv8-A's vectorization capability and benchmark Python-based matrix multiplication using both `int8` and `fp32` precision. Building on this foundation, we will implement and optimize the inference of a state-of-the-art small language model, **Llama 3.2-1B**, showcasing PyTorch's powerful capabilities for handling advanced AI workloads on ARMv8.\n",
    "\n",
    "Let's start by checking the build configuration of our installed pytorch package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with:\n",
      "  - GCC 10.2\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: NO AVX\n",
      "  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-10/root/usr/bin/c++, CXX_FLAGS=-ffunction-sections -fdata-sections -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=2.5.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__config__.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output, you can see the build configuration for this version of PyTorch. Some key flags indicate how PyTorch is optimized for performance on the your device:\n",
    "\n",
    "- **`USE_OPENMP=ON`**: This library is used to parallelize matrix multiplications across threads, providing speedups on the Raspberry Pi's quad-core processor.\n",
    "- **`BLAS_INFO=ON`** BLAS (Basic Linear Algebra Subprograms) are used to perform efficient linear algebra on ARM utilizing vector processing. \n",
    "- **`USE_NNPACK=ON`**: A low-level library of operators that utilizes vectorized instructions on ARM processors to accelerate operations.\n",
    "- **`USE_MKLDNN=ON`**: While primarily designed for x86 processors, this library also includes vectorized implementations of operators optimized for the AArch64 architecture, making it compatible with devices like the Raspberry Pi 4 and 5.\n",
    "\n",
    "These build configurations show that PyTorch is equipped to take advantage of ARM's vectorization and multi-threading capabilities. \n",
    "\n",
    "Next, let's validate that PyTorch is successfully utilizing these low-level libraries to vectorize its tensor operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analyzing PyTorch's Utilization of ARM Vector Processing**\n",
    "\n",
    "To confirm PyTorch's use of ARM's vector processing capabilities, we will investigate its behavior under the hood by writing two simple scripts for matrix multiplication. These scripts replicate what we implemented in C earlier but leverage PyTorch's abstractions. \n",
    "\n",
    "The scripts will perform matrix multiplication using both 32-bit floating-point (`fp32`) and 8-bit integer (`int8`) precisions. PyTorch simplifies these operations by providing built-in functionality that abstracts away low-level details, enabling developers to focus on high-level design while still benefiting from hardware optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src/python’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir src/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/python/fp_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/fp_matmul.py\n",
    "\n",
    "import torch\n",
    "a = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "b = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "c = torch.mm(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/python/int8_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/int8_matmul.py\n",
    "import torch\n",
    "\n",
    "# Generate random int8 tensors\n",
    "a = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "b = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "c = torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using `perf` to Inspect Assembly**\n",
    "\n",
    "The following code cells use the `perf` tool to extract the assembly instructions executed by the Python scripts above. By capturing this assembly, we can analyze whether the low-level libraries that `torch.mm` relies on for matrix multiplication are effectively utilizing ARM's vector processing capabilities, such as NEON instructions.\n",
    "\n",
    "We will write the extracted assembly code to a text file for further analysis. This process allows us to verify if PyTorch's operations are optimized to leverage hardware acceleration on ARM-based systems.\n",
    "\n",
    "**Note**: In the following code cells, a 10-second timeout is applied. This is sufficient to capture a representative sample of the assembly instructions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract assembly for the floating point matmul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 23 times to write data ]\n",
      "[ perf record: Captured and wrote 5.843 MB perf.data (13457 samples) ]\n",
      "============================== Completed! ==============================\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Execution timed out!\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "signal.alarm(10)  # Set the alarm for 60 seconds\n",
    "\n",
    "try:\n",
    "    # Your long-running code here\n",
    "    !rm -rf *perf*\n",
    "    !rm -rf fp_matmul.txt\n",
    "    !sudo perf record -e instructions:u -g $(which python) src/python/fp_matmul.py\n",
    "    !sudo perf annotate > results/fp_matmul_instructions.txt\n",
    "except TimeoutError as e:\n",
    "    !rm -rf *perf*\n",
    "    print(\"============================== Completed! ==============================\")\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract assembly for the int8 matmul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 25 times to write data ]\n",
      "[ perf record: Captured and wrote 6.085 MB perf.data (13886 samples) ]\n",
      "============================== Completed! ==============================\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Execution timed out!\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "signal.alarm(10)  # Set the alarm for 60 seconds\n",
    "\n",
    "try:\n",
    "    # Your long-running code here\n",
    "    !rm -rf *perf*\n",
    "    !rm -rf int8_matmul.txt\n",
    "    !sudo perf record -e instructions:u -g $(which python) src/python/int8_matmul.py\n",
    "    !sudo perf annotate > results/int8_matmul_instructions.txt\n",
    "except TimeoutError as e:\n",
    "    !rm -rf *perf*\n",
    "    print(\"============================== Completed! ==============================\")\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets have a look inside the instructions used by pytorch for the floating point matrix multiply script and print them below. We will search for vectorized instructions such as `fmul` and `fadd` to detect their presence and ensure they are using the vector registers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -m 20 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/fp_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see printed out above some assembly instructions that the ARM processor will run. the structure of each line written out by `perf annotate` has the following format\n",
    "\n",
    "***timestamp :    instruction_address: operation:   destination_registor, source_register1, source_register2***\n",
    "\n",
    "The printout should show operations making use of the vector registers. Examples of operations you might see include.   \n",
    "\n",
    "#### **Operation Definitions**\n",
    "- **`fmul`**: Performs a floating-point multiplication operation between two operands.\n",
    "- **`fmla`**: Performs a floating-point fused multiply-add operation, where the result of the multiplication is added to an accumulator in a single instruction.\n",
    "\n",
    "- **Vector Registers**: Operands starting with a `v` (e.g., `v16.4s`, `v0.4s`, `v8.s[0]`) indicate that vector registers are being used. These registers contain multiple data lanes, enabling the instruction to process multiple elements simultaneously, thereby increasing throughput.\n",
    "  - Example: `fmla v16.4s, v0.4s, v8.s[0]` performs the fused multiply-add operation on four single-precision floating-point elements in parallel (one for each lane of the vector register `v16`)..\n",
    "\n",
    "By examining the register types (vector vs. scalar), it becomes clear whether the instruction leverages SIMD for parallel processing or operates in a scalar manner and therefore whether torch is uses ARM's NEON SIMD processing capabilities to accelerate the matrix muliply operation. \n",
    "\n",
    "***Note*** Should no vector registers show up in your instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00 :   3740150:        dup     v2.16b, w0\n",
      "    0.00 :   3740160:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   3740194:        dup     v1.8b, w0\n",
      "    0.00 :   37401a8:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   3740518:        movi    v0.4s, #0x0\n",
      "    0.00 :   374053c:        mla     v0.16b, v2.16b, v1.16b\n",
      "    0.00 :   3740548:        addv    b0, v0.16b\n",
      "    0.00 :   3740554:        umov    w5, v0.b[0]\n",
      "    0.00 :   3740594:        mul     v0.8b, v0.8b, v1.8b\n",
      "    0.00 :   3740598:        addv    b0, v0.8b\n",
      "    0.00 :   374059c:        umov    w12, v0.b[0]\n",
      "    0.00 :   374082c:        dup     v2.16b, w0\n",
      "    0.00 :   3740840:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   3740878:        dup     v1.8b, w0\n",
      "    0.00 :   374088c:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   3740b34:        movi    v3.4s, #0x0\n",
      "    0.00 :   3740b44:        mov     v22.16b, v3.16b\n",
      "    0.00 :   3740b48:        mov     v21.16b, v3.16b\n",
      "    0.00 :   3740b50:        mov     v9.16b, v3.16b\n",
      "    0.00 :   3740b6c:        ld4     {v4.16b-v7.16b}, [x7], #64\n"
     ]
    }
   ],
   "source": [
    "!grep -m 20 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/int8_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of instructions you might see in the output above are: \n",
    "- **`dup`**: Duplicates the value of a scalar register (e.g., `w0`) into all lanes of a SIMD vector register (e.g., `v2.16b`). This allows the same value to be broadcast across multiple lanes for parallel processing.\n",
    "- **`mla`**: Performs a fused multiply-accumulate operation. It multiplies corresponding elements from two SIMD vector registers and adds the results to the accumulator register. This operation is performed on all lanes in parallel.\n",
    "- **`addv`**: Adds all elements in a SIMD vector register and stores the resulting sum in a scalar register. This is typically used for reduction operations to aggregate data from multiple lanes.\n",
    "- **`umov`**: Extracts a specific lane from a SIMD vector register and moves it to a scalar register. This is useful for accessing individual elements after SIMD processing.\n",
    "\n",
    "These presence of these instructions shown above indicate that torch is utilizing ARMS SIMD capabilities. \n",
    "\n",
    "***Note*** Should no vector registers show up in the instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Benchmarking Pytorch Linear Layer**\n",
    "Great. Now you have verified that pytorch is utilizing the vector processing capability of ARM by defualt, we can benchmark it's operations. Specifically, let's benchmark the floating point and integer precision operations. Just like our above kernel examples writte in c. We should see a measurable latency reduction when processing in int8 precision vs fp32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a latency measuring function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "def benchmark(inputs, func, num_runs=10): \n",
    "    times = []\n",
    "    for _ in range(3): \n",
    "        func(inputs)\n",
    "\n",
    "    for _ in range(num_runs): \n",
    "        st = time.time()\n",
    "        func(inputs)\n",
    "        times.append(time.time() - st)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once again specify the size of matrices to evaluate**\n",
    "\n",
    "feel free to adjust the matrix sizes to identify how much computation large matrix multiply's require!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "sizes = [32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for a Full Precision Linear Layer**  \n",
    "The linear layer is a fundamental building block of generative AI models like transformers, performing dense matrix multiplications at its core. Given input $ X \\in \\mathbb{R}^{m \\times n} $ and weights $ W \\in \\mathbb{R}^{n \\times p} $, the output $ Y \\in \\mathbb{R}^{m \\times p} $ is computed as $ Y = XW + b $, where $ b $ is an optional bias term. Measuring the latency of these FP32 matrix multiplications provides a baseline for assessing performance and comparing with optimized or quantized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "float_times = []\n",
    "for size in sizes:\n",
    "    # floating point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    linear = nn.Linear(size, size, bias=False)\n",
    "    float_times.append(benchmark(x, linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for an INT8 Quantized Linear Layer**  \n",
    "In a quantized linear layer, the computation $ Y = XW + b $ is performed using INT8 precision for the operands $ X $ (input) and $ W $ (weights), while optionally adding a bias $ b $ in a higher precision (e.g., INT32 or FP32) to preserve accuracy. Quantization maps the original floating-point values to 8-bit integers using a scale $ S $ and zero-point $ Z $, such that $ x_\\text{quant} = \\text{round}(x / S) + Z $. This allows efficient matrix multiplications in reduced precision while maintaining a close approximation of the original computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.quantized as nnq\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "int8_times = []\n",
    "for size in sizes:\n",
    "    # floating point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    x_quant = torch.quantize_per_tensor(x, scale=x.abs().max()/127, zero_point=0, dtype=torch.qint8)\n",
    "    qlinear = nnq.Linear(size, size, dtype=torch.qint8)\n",
    "    qlinear.set_weight_bias(x_quant, None)\n",
    "    int8_times.append(benchmark(x_quant, qlinear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, float_times, marker='o', linestyle='-', color='r', label='float precision')\n",
    "plt.plot(sizes, int8_times, marker='o', linestyle='-', color='b', label='int8 precision')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: Naive vs NEON SIMD vs NEON I8MM Matrix Multiplication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **OpenELM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to Download and Use Llama-3.2-1B from Hugging Face\n",
    "\n",
    "1. **Log In or Sign Up**  \n",
    "   If you don’t already have a Hugging Face account, create one. Otherwise, log in to your existing account.\n",
    "\n",
    "2. **Visit the Model Page**  \n",
    "   Navigate to the [apple/OpenELM](https://huggingface.co/apple/OpenELM) page on Hugging Face.\n",
    "\n",
    "3. **Request Access**  \n",
    "   On the model's page, click the **\"Access repository\"** button. You’ll be prompted to review and agree to the terms of use.\n",
    "\n",
    "4. **Visit the Lamma Model Page**\n",
    "   Navigate to the [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) page on Hugging Face. You need to do this, as the OpenELM model uses the Lamma Tokenizer. \n",
    "\n",
    "5. **Wait for Approval**  \n",
    "   After agreeing to the terms, access will be granted. This may take a few hours where you should receive a notificatino via email.\n",
    "\n",
    "5. **Login via the Command Line**\n",
    "\n",
    "To download the model, you may need to authenticate your Hugging Face account on your local machine. Run the following command in your terminal and follow the prompts to log in:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "After this authentication, you can download and use the model with the following script.\n",
    "\n",
    "**NOTE** This script may take up to half an hour to run the first time it is executed. This is because the Hugging Face package needs to download the model weights, which include all 1 billion parameters, amounting to roughly 2.5 GB. After the initial execution, reloading the model should be faster (45s on RasberryPi 5) since Hugging Face caches the weights locally.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3e1a161ab04da58cde5971c453d8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c0830569814a93a2220ce0a9f67504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_openelm.py:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:\n",
      "- configuration_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389f605c5f954c8ea868c17053689ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_openelm.py:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:\n",
      "- modeling_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9482f74b03e04dbd886d0e13e25bcb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/914M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5081b2f5e4030843095be1c8744d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (proj_2): Linear(in_features=768, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=2048, bias=False)\n",
       "          (proj_2): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=2560, bias=False)\n",
       "          (proj_2): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "          (proj_2): Linear(in_features=2304, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=7680, bias=False)\n",
       "          (proj_2): Linear(in_features=3840, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (proj_2): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (16): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=10752, bias=False)\n",
       "          (proj_2): Linear(in_features=5376, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (17): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=11264, bias=False)\n",
       "          (proj_2): Linear(in_features=5632, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (18): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=11776, bias=False)\n",
       "          (proj_2): Linear(in_features=5888, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "      (19): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=24, key_heads=6, value_heads=6\n",
       "          (qkv_proj): Linear(in_features=1536, out_features=2304, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1536, out_features=12288, bias=False)\n",
       "          (proj_2): Linear(in_features=6144, out_features=1536, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1536, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M-Instruct\", trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets run a prompt through the model and see what it generates**\n",
    "LLamma3.2-1B is a small base model trained to autoregressively generate a probability distribution over tokens based on the given input tokens. In this form, it functions as an autocomplete engine: given an initial prompt, the predicted next token can be appended to the original input and recursively passed through the model, creating a continuous sentence. However, this process requires the model to be forward-passed multiple times, increasing computational cost with longer sequences. To manage this, we'll set a maximum sequence length of 30 tokens and observe what the model generates as a completion to the prompt: ***ARM is a company that designs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM is a company that designs and sells microcontrollers, processors, and other\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ARM is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=20)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets measure the models static memory consumption** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total static memory usage: 1748.00244140625 MB\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(p.numel() * p.element_size() for p in model.buffers())\n",
    "print(f\"Total static memory usage: {(total_size + buffer_size) / (1024 ** 2)} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a significant amount of memory consumption—nearly 2GB! Many small embedded devices may struggle to handle this. Therefore, it's crucial to explore strategies for reducing the model's memory footprint, alongside improving inference speed. Doing so will expand the range of devices capable of running the model and, since memory access is energy-intensive, it will also reduce energy consumption and extend battery life. Next however, lets look at the latency bottlenecks and dynamic memory usage during inference with `torch.profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      model_inference         3.04%     102.400ms       100.00%        3.368s        3.368s      19.99 Mb    -277.23 Mb             1  \n",
      "                                         aten::linear         0.04%       1.397ms        84.72%        2.853s      35.228ms      73.23 Mb           0 b            81  \n",
      "                                         aten::matmul         0.22%       7.312ms        84.57%        2.848s      35.166ms      73.23 Mb           0 b            81  \n",
      "                                             aten::mm        84.26%        2.838s        84.26%        2.838s      35.040ms      73.23 Mb      73.23 Mb            81  \n",
      "                                            aten::mul         3.25%     109.364ms         3.26%     109.940ms     416.441us     101.71 Mb     101.71 Mb           264  \n",
      "                                            aten::add         1.40%      46.999ms         1.44%      48.336ms     300.221us      25.01 Mb      25.01 Mb           161  \n",
      "                                          aten::copy_         1.40%      46.988ms         1.40%      46.988ms     223.750us           0 b           0 b           210  \n",
      "                                            aten::pow         1.29%      43.434ms         1.30%      43.688ms     539.363us      25.29 Mb      25.29 Mb            81  \n",
      "                                           aten::mean         0.10%       3.447ms         1.20%      40.324ms     497.827us     139.50 Kb     139.49 Kb            81  \n",
      "                   aten::scaled_dot_product_attention         0.02%     776.981us         1.07%      35.999ms       1.800ms       6.40 Mb    -102.38 Kb            20  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.368s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "prompt = \"ARM is a company that designs and develops microprocessors and other microchips. It is also known for its ARM Cortex processor and the ARM Mali GPU. ARM has been a leader in the mobile computing industry since the early 2000s, and is now a major player in the AI and \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Profile the model\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "# Print a summary\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like this \n",
    "\n",
    "| **Name**                                      | Self CPU % | Self CPU   | CPU total % | CPU total   | CPU time avg  | CPU Mem      | Self CPU Mem | # of Calls |\n",
    "|-----------------------------------------------|------------|------------|-------------|-------------|---------------|--------------|--------------|------------|\n",
    "| model_inference                               | 3.04%      | 102.400ms  | 100.00%     | 3.368s      | 3.368s        | 19.99 Mb     | -277.23 Mb   | 1          |\n",
    "| aten::linear                                  | 0.04%      | 1.397ms    | 84.72%      | 2.853s      | 35.228ms      | 73.23 Mb     | 0 b          | 81         |\n",
    "| aten::matmul                                  | 0.22%      | 7.312ms    | 84.57%      | 2.848s      | 35.166ms      | 73.23 Mb     | 0 b          | 81         |\n",
    "| aten::mm                                      | 84.26%     | 2.838s     | 84.26%      | 2.838s      | 35.040ms      | 73.23 Mb     | 73.23 Mb     | 81         |\n",
    "| aten::mul                                     | 3.25%      | 109.364ms  | 3.26%       | 109.940ms   | 416.441us     | 101.71 Mb    | 101.71 Mb    | 264        |\n",
    "| aten::add                                     | 1.40%      | 46.999ms   | 1.44%       | 48.336ms    | 300.221us     | 25.01 Mb     | 25.01 Mb     | 161        |\n",
    "| aten::copy_                                   | 1.40%      | 46.988ms   | 1.40%       | 46.988ms    | 223.750us     | 0 b          | 0 b          | 210        |\n",
    "| aten::pow                                     | 1.29%      | 43.434ms   | 1.30%       | 43.688ms    | 539.363us     | 25.29 Mb     | 25.29 Mb     | 81         |\n",
    "| aten::mean                                    | 0.10%      | 3.447ms    | 1.20%       | 40.324ms    | 497.827us     | 139.50 Kb    | 139.49 Kb    | 81         |\n",
    "| aten::scaled_dot_product_attention            | 0.02%      | 776.981us  | 1.07%       | 35.999ms    | 1.800ms       | 6.40 Mb      | -102.38 Kb   | 20         |\n",
    "\n",
    "**Self CPU time total:** 3.368s\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways from Profiling\n",
    "\n",
    "- **Matrix Multiplications (aten::mm)**: The dominant computational operation, accounting for 84.26% of the total CPU computation time. Dense layers (e.g., `aten::linear`) rely heavily on these operations.\n",
    "- **Element-wise Operations**: `aten::mul` and `aten::add` are relatively minor contributors to overall performance, consuming ~4.7% of the computation time combined.\n",
    "- **Memory Usage**: Matrix multiplications (`aten::mm`) are the most memory-intensive operation, allocating 73.23 MB. Dense layers similarly dominate memory usage.\n",
    "- **Scaled Dot Product Attention**: This is relatively efficient, using only 6.40 MB and less than 1.1% of the total computation time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization Focus**\n",
    "\n",
    "**Matrix multiplications in dense layers are the most critical operations to optimize.** These dominate both computation and memory usage, making them the primary target for performance improvements. Techniques such as vectorization and quantization, covered in earlier sections, can significantly enhance both efficiency and memory usage. Let's start by lowering the precision of the feedforward linear layers that rely heavily on matrix multiplications to the int8 datatype and see how this can speed up computation. This is a technique known as quantization. Specifically, we will start with a method called **Integer Symmetric Weight-Only Quantization**.\n",
    " \n",
    "\n",
    "\n",
    "#### **Integer Per-TensorSymmetric Weight-Only Quantization**\n",
    "\n",
    "Integer symmetric weight-only quantization reduces a neural network's weight precision from floating-point to integer values while keeping activations in floating-point format during inference. While the underlying matrix multiplication must still occur in floating point e.g. `torch.mm(W.dequantize(), x)`, the weight-only quantization can decrease the static memory consumption of the model by up to 4 times. Additionally in the case where the inference speed is limited by the memory bandwidth, reducing the weight precision can also reduce the memory read latencies, providing inference speedups. \n",
    "\n",
    "Given that the equation of the linear layer is\n",
    "\n",
    "$y = XW + b$\n",
    "\n",
    "To apply this technique, floating-point weights $ W $ are quantized to integer values $ Q_w $ using a scale factor $ S_w $. The scale factor can be computed based on the maximum absolute weight value:\n",
    "\n",
    "$\n",
    "S_w = \\frac{\\max(|W|)}{2^{b-1} - 1}\n",
    "$\n",
    "\n",
    "where $ b $ is the bit-width of the integer representation (e.g., $ b = 8 $ for 8-bit integers).\n",
    "\n",
    "The quantization and dequantization processes are defined by:\n",
    "\n",
    "- **Quantization**:\n",
    "  $\n",
    "  Q_w = \\text{round}\\left( \\frac{W}{S_w} \\right)\n",
    "  $\n",
    "\n",
    "- **Dequantization**:\n",
    "  $\n",
    "  \\hat{W} = Q_w \\cdot S_w\n",
    "  $\n",
    "\n",
    "During inference, the quantized weights $ Q_w $ are used with floating-point activations $ x $ to compute the output $ y $:\n",
    "\n",
    "$\n",
    "y = X \\hat{W}  + b = X (Q_w \\cdot S_w) + b\n",
    "$\n",
    "\n",
    "**Lets collect a weight matrix and activation tensor from one of the feed forward layers in the OpenELM Model.** (we can do this using a functionality of pytorch called forward hooks)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected weight matrix has shape 8192x2048 this is the number of (input_channels x output_channels)\n",
      "The collected activation tensor has shape 1x61x8192 this is the number of (batch_size x sequence_length x token_dim)\n"
     ]
    }
   ],
   "source": [
    "# Lets grab a weight tensor and activation one of the feed forward layers in the Lamma3.2-1B model. \n",
    "# pick whatever transformer layer you like \n",
    "\n",
    "layer_idx = 7 # alter this to select the transformer block of your choosing. (the model has 16 transformer blocks)\n",
    "\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output\n",
    "    return hook\n",
    "\n",
    "# register a forward hook to collect the activations of the gate_proj layer in the transformer block. \n",
    "layer = model.model.layers[layer_idx].mlp.gate_proj\n",
    "hook_handle = layer.register_forward_hook(get_activation('gate_proj')) \n",
    "\n",
    "# run the model forward pass \n",
    "with torch.no_grad(): \n",
    "    model(**inputs) \n",
    "\n",
    "# extract the weight matrix and activation tensor from the hook. \n",
    "W = layer.weight.data.clone() \n",
    "X = activations['gate_proj']\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"The collected weight matrix has shape {W.shape[0]}x{W.shape[1]} this is the number of (input_channels x output_channels)\")\n",
    "print(f\"The collected activation tensor has shape {X.shape[0]}x{X.shape[1]}x{X.shape[2]} this is the number of (batch_size x sequence_length x token_dim)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!, now we have collected a weight matrix and activation tensor from the inside of the Lamma3.2-1B feed forward layers. We can now use these to test the accuracy of the quantization technique. Lets start by implementing the per-tensor quantization technque described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization to compare against our quantized version. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization (int8 quantization)\n",
    "S = W.abs().max() / (2**(b-1) - 1)  # Scale Factor Quantization\n",
    "Q = torch.round(W / S).clamp(-2**(b-1), 2**(b-1) - 1)  # Quantizing the weights (clamp is used to prevent overflow)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat = torch.matmul(X, (Q * S)) # Notice the Dequantzation step here (Q * S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quantization error to see if we have correctly approximated the full precision linear layer. This can be defined as the absolute difference between the full precision linear layer and our quantized version. Using our notation, where $y$ is the full precision output, $\\hat{y}$ is the quantized output, $Q$ is the quantized tensor, and $S$ is the scale factor, the quantization error can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Quantization Error} = \\left| y - \\hat{y} \\right| = \\left| y - W \\cdot (Q \\cdot S) \\right|\n",
    "$$\n",
    "\n",
    "Additionally, the relative quantization error as a percentage can be calculated to understand the error in relation to the magnitude of the full precision output:\n",
    "\n",
    "$$\n",
    "\\text{Relative Quantization Error (\\%)} = \\left( \\frac{\\left| y - \\hat{y} \\right|}{\\left| y \\right|} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "This relative error provides a normalized measure of the quantization error, making it easier to compare across different scales of output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 64646.39 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 5.89 (%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantization error. This is the difference between the full precision linear layer and our quantized version. \n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel.mean():.2f} (%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the mean and min relative quantization error are around 6% and 0% (for layer 8 of the Lamma3.2-1B model). The Max relative quantization error may however be huge. This is caused by outliers in the weight matrix distribution that skew the quantization error. There are anumber of techniques to mitigate this issue. We will not cover them here but you can learn more about them in the [LLM.int8() paper](https://arxiv.org/abs/2208.07339). \n",
    "\n",
    "This is a good start but we can quantize the weights, to give roughly a 5% relative quantization error. We however can do better. To do so lets try and understand what the distribution of the data in the weight matrix looks like. Below we will plot a boxplot of the 4 largest and 4 smallest channels in your chosen layer (set by the layer_idx variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the L2 norm of each channel along axis 1\n",
    "channel_magnitudes = np.linalg.norm(W.clone().numpy(), axis=1)\n",
    "\n",
    "num_channels_to_select = 5\n",
    "largest_channels_indices = np.argsort(channel_magnitudes)[-num_channels_to_select:]\n",
    "smallest_channels_indices = np.argsort(channel_magnitudes)[:num_channels_to_select]\n",
    "\n",
    "# Select the largest and smallest channels along axis 1\n",
    "largest_channels = W.numpy()[largest_channels_indices, :]\n",
    "smallest_channels = W.numpy()[smallest_channels_indices, :]\n",
    "\n",
    "# Combine the data for boxplot\n",
    "combined_data = np.concatenate((largest_channels, smallest_channels), axis=0)\n",
    "labels = ['Largest'] * num_channels_to_select + ['Smallest'] * num_channels_to_select\n",
    "\n",
    "combined_data = combined_data.T\n",
    "print(combined_data.shape)\n",
    "# Plot boxplot for the selected channels\n",
    "def plot_combined_boxplot(combined_data, labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=combined_data, palette=\"Set2\")\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels * (combined_data.shape[1] // len(labels)))\n",
    "    plt.title(f'BoxPlot Distributions of Largest and Smallest Channels in Lamma3.2-1B FeedForward Layer {layer_idx}')\n",
    "    plt.xlabel('Channel Type')\n",
    "    plt.ylabel('Weight Value')\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_boxplot(combined_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Per-Tensor Symmetric Weight-Only Quantization\n",
    "\n",
    "Looking at this plot, what you can see is that the larger channels have very wide distributions and large outliers. Additionally, the small channels have very tight distributions centered around 0. If we used per-tensor quantization with just a single scale value $S_w$ for the entire tensor, all the values in the small channels would be mapped to zero, causing large quantization errors.\n",
    "\n",
    "To overcome this, we can use a technique called **Per-Channel Weight-Only Quantization**. Instead of a single scale factor for the entire tensor, we compute a separate scale factor for each channel of the weight matrix. This allows us to more accurately quantize the weights in each channel, reducing the quantization error and preserving the accuracy of the model.\n",
    "\n",
    "The process for doing this is as follows:\n",
    "\n",
    "#### 1. Channel-Wise Scale Factor\n",
    "For each channel $i$ of $\\mathbf{W}$ (i.e., row $\\mathbf{W}_i$), compute a channel-specific scale factor:\n",
    "\n",
    "$\n",
    "S_{w_i} = \\frac{\\max(|\\mathbf{W}_i|)}{2^{b-1} - 1}.\n",
    "$\n",
    "\n",
    "#### 2. Channel-Wise Quantization and Dequantization\n",
    "1. **Quantization**: Convert the weights of each channel to integer values:\n",
    "   $\n",
    "   Q_{w_i} = \\text{round}\\left(\\frac{\\mathbf{W}_i}{S_{w_i}}\\right), \\quad Q_{w_i} \\in \\{-2^{b-1}, \\ldots, 2^{b-1} - 1\\}.\n",
    "   $\n",
    "\n",
    "2. **Dequantization**: Recover the approximate floating-point weights of each channel:\n",
    "   $\n",
    "   \\hat{\\mathbf{W}}_i = Q_{w_i} \\cdot S_{w_i}.\n",
    "   $\n",
    "\n",
    "The final quantized weight matrix $\\hat{\\mathbf{W}}$ is reconstructed by concatenating the channel-wise dequantized weights.\n",
    "\n",
    "#### 3. Inference\n",
    "During inference, the computation of $\\mathbf{y}$ remains similar but uses the per-channel dequantized weights:\n",
    "\n",
    "$\n",
    "\\mathbf{y} = \\mathbf{x} \\cdot \\hat{\\mathbf{W}} = \\mathbf{x} \\cdot \\left(\\sum_{i=1}^m Q_{w_i} \\cdot S_{w_i}\\right).\n",
    "$\n",
    "\n",
    "This approach ensures that both small and large channels are appropriately quantized, reducing overall quantization error. Lets implement this below. Your task is to complete the missing line in the code cell below that computs the per-channel scale factor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S_per_channel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# Bit-width for quantization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ------ Complete the missing line here ------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# you need to compute a different scale factor for each channel in the weight matrix. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# S = ... \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(W \u001b[38;5;241m/\u001b[39m \u001b[43mS_per_channel\u001b[49m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Q <- quantize(W)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Weight-only quantized linear layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m y_hat_per_channel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(X, (Q \u001b[38;5;241m*\u001b[39m S_per_channel)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'S_per_channel' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# you need to compute a different scale factor for each channel in the weight matrix. \n",
    "# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "# S_per_channel = ... \n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X, (Q * S_per_channel)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can run this test below to check your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative per channel quantization error: 6028.29 (%), per-tensor quantization error: 64646.39 (%)\n",
      "Min relative per channel quantization error: 0.00 (%), per-tensor rel quantization error: 0.00 (%)\n",
      "Mean relative per channel quantization error: 1.13 (%), per-tensor mean rel quantization error: 5.89 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: 1.13% and Per-Tensor quantization error 5.89%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Calculate quantization error\n",
    "residuals_per_channel = (y - y_hat_per_channel).abs()\n",
    "residuals_rel_per_channel = (residuals_per_channel / y.abs()) * 100\n",
    "print(f\"Max relative per channel quantization error: {residuals_rel_per_channel.max():.2f} (%), per-tensor quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative per channel quantization error: {residuals_rel_per_channel.min():.2f} (%), per-tensor rel quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative per channel quantization error: {residuals_rel_per_channel.mean():.2f} (%), per-tensor mean rel quantization error: {residuals_rel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel_per_channel.mean() < residuals_rel.mean(): \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: {residuals_rel_per_channel.mean():.2f}% and Per-Tensor quantization error {residuals_rel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel_per_channel.mean():.2f}%. Please try again.</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# you need to compute a different scale factor for each channel in the weight matrix. \n",
    "# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "S_per_channel = W.abs().max(dim=1, keepdim=True)[0] / (2**(b-1) - 1)  # Scale factor\n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X, (Q * S_per_channel)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! you should see that the per-channel weight only quantizaton technqiue has a significantly lower quantization error than per-tensor quantization. Lets now use this approach to quantize Lamma3.2-1B! Specifically we will apply the to the feedfoward layers of Lamma3.2-1B transformer blocks as they quantize the best. Lets build a class to achieve this. **Below is a simple implementation of the Per-ChannelWeight-Only Integer Quantization technique that can be applied to the pytorch linear layer. It however is not completed, you will therefore need to fill in the missing line in the `quantize` member function, aswell as the linear multiplication in the `forward` function**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(out_features), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to compute a variable called `scale` that is used to quantize the `tensor` Look in the above exmple if you are unsure how to do this. \n",
    "        # scale = ...\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # you can use the `torch.nn.functional.linear` function to perform the linear layer operation. (Remember to dequantize the weights first)\n",
    "        # y = ...\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can test your implementation by running the following cell. The output relative error should be similar to the above example.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 8579.00 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 1.70 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: 1.70%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# setup the full precision linear layer\n",
    "linear = nn.Linear(W.shape[0], W.shape[1], bias=False)\n",
    "linear.weight.data = W.t()\n",
    "\n",
    "# setup the quantized linear layer\n",
    "qlinear = WeightOnlyInt8Linear(W.shape[0], W.shape[1], bias=False)\n",
    "qlinear.quantize(W.t()) \n",
    "\n",
    "# run the forward pass through the linear layer and compare the output to the full precision linear layer. \n",
    "y = linear(X)\n",
    "y_hat = qlinear(X)\n",
    "\n",
    "# Calculate relative quantization error\n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel.mean() < 8: \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: {residuals_rel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel.mean():.2f}%. Please try again.</div>\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SOLUTION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(1, dtype=torch.float32), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to compute a variable called `scale` that is used to quantize the `tensor` Look in the above exmple if you are unsure how to do this. \n",
    "        scale = torch.Tensor((tensor.abs().max(dim=1, keepdims=True)[0] / qmax))\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        quantized_weights = quantized_weights.type(torch.int8)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        assert self.weight.dtype == torch.int8\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # you can use the `torch.nn.functional.linear` function to perform the linear layer operation. \n",
    "        y = F.linear(x, self.weight * self.scale, self.bias)\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets now use this to quantize the feedforward layers of Lamma3.2-1B.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): WeightOnlyInt8Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): WeightOnlyInt8Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): WeightOnlyInt8Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model.eval()\n",
    "\n",
    "def quantize_model(model): \n",
    "    for block in model.model.layers: \n",
    "        for name, module in block.mlp.named_modules(): \n",
    "            if isinstance(module, nn.Linear): \n",
    "                qlinear = WeightOnlyInt8Linear(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "                if module.bias is not None: \n",
    "                    qlinear.bias.data = module.bias.data \n",
    "                qlinear.quantize(module.weight.data.clone())\n",
    "                # Use setattr on the parent module to replace the linear layer\n",
    "                parent_module = block.mlp\n",
    "                setattr(parent_module, name, qlinear)\n",
    "    return model\n",
    "\n",
    "model = quantize_model(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets run a prompt through the model and make sure we haven't broken anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM is a company that designs and manufactures microprocessors and other microelectronic products. ARM chips are used in many devices such as smartphones, tablets, smartwatches, and more. ARM chips are widely used because they are fast, efficient,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ARM is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have confirmed that. Lets check the static memory consumption, to see what benefits weight only quantization has brought us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total static memory usage: 2410.2600708007812 MB\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(p.numel() * p.element_size() for p in model.model.parameters())\n",
    "buffer_size = sum(p.numel() * p.element_size() for p in model.model.buffers())\n",
    "print(f\"Total static memory usage: {(total_size + buffer_size) / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So we have reduced the static memory consumption from 4.7Gb to 2.3Gb. This is a significant reduction, and will make the model more portable to smaller devices. We can however do better. Lets explore graph optimization techniques to improve the inference speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of `torch.compile` for ARM CPUs in PyTorch\n",
    "\n",
    "`torch.compile` is a feature in PyTorch that optimizes model execution by compiling models into more efficient forms, specifically tailored for ARM CPUs. This process reduces the overhead of PyTorch's dynamic execution, leading to faster execution times and lower resource usage on ARM architectures. By analyzing the model's computation graph, `torch.compile` applies optimizations such as operator fusion and loop unrolling, which are particularly beneficial for ARM's architecture.\n",
    "\n",
    "ARM CPUs are known for their energy efficiency and performance, making them ideal for deploying AI models in diverse environments. `torch.compile` leverages ARM's specific capabilities, including its support for SIMD (Single Instruction, Multiple Data) instructions like NEON, to enhance model performance. This allows for parallel processing of multiple data points, significantly accelerating computationally intensive tasks.\n",
    "\n",
    "The feature is user-friendly, requiring minimal changes to existing code. Users can compile their models with `torch.compile` and immediately benefit from performance improvements on ARM CPUs. This makes it particularly useful for complex models or those running on ARM-based devices, offering significant speedups and efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency for Eager Execution: 519.14 ms\n"
     ]
    }
   ],
   "source": [
    "def benchmark_latency(inputs, model, num_runs=10):\n",
    "    import time\n",
    "    import torch\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Warm-up runs (to stabilize any initial overhead)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            model(**inputs)\n",
    "\n",
    "    # Measure latency\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            model(**inputs)\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "\n",
    "    # Calculate average latency\n",
    "    avg_latency = sum(times) / len(times)\n",
    "    return avg_latency\n",
    "\n",
    "\n",
    "lat = benchmark_latency(inputs, model)\n",
    "print(f\"Average latency for Eager Execution: {lat:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "compiled_model = torch.compile(model)\n",
    "lat = benchmark_latency(inputs, compiled_model)\n",
    "print(f\"Average latency for Torch Compile: {lat:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model for embedded inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.shape)\n",
    "print(inputs.attention_mask.shape)\n",
    "print(inputs.input_ids.dtype)\n",
    "print(inputs.attention_mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:97: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of BaseModelOutputWithPast(last_hidden_state=tensor([[[ 1.5326, -1.3593,  2.4385,  ..., -1.0574,  0.4010, -1.1256],\n         [-1.1032,  3.1969,  3.7620,  ..., -5.2792, -4.0093,  2.8944],\n         [ 1.5108,  1.3452, -0.2563,  ..., -5.3894, -6.9593, -0.1786],\n         ...,\n         [ 1.4442,  1.9402,  1.1227,  ..., -3.1643, -6.0105, -2.4108],\n         [ 3.6687,  4.9128,  1.4363,  ..., -3.1012, -4.0978, -0.2268],\n         [-0.4561,  4.2190,  2.2034,  ..., -2.9271, -3.0164, -0.9648]]],\n       grad_fn=<MulBackward0>), past_key_values=((tensor([[[[ 9.5610e-02,  1.7827e-01,  3.6755e-02,  ..., -1.7652e+00,\n            1.7963e+00,  7.9892e-03],\n          [-1.0441e+00, -1.9604e+00, -1.3867e+00,  ...,  6.3562e-01,\n           -2.3245e+00, -2.2302e+00],\n          [-4.0632e+00, -8.6298e-01, -3.0124e+00,  ...,  2.7366e+00,\n           -1.4887e+00, -2.8741e-01],\n          ...,\n          [-1.1385e+00, -3.3855e+00, -2.5555e+00,  ...,  1.7879e+00,\n           -1.7340e+00, -2.0997e+00],\n          [ 3.8586e+00, -3.0442e+00, -1.6802e+00,  ...,  2.6446e+00,\n           -1.3799e+00, -2.4386e+00],\n          [ 6.2070e+00, -9.6388e-02, -1.1381e+00,  ...,  1.5129e+00,\n           -1.4761e+00, -1.9522e+00]],\n\n         [[ 1.3574e-03, -6.1648e-03,  1.7492e-02,  ...,  1.3465e+00,\n           -3.6406e-01,  2.4415e-01],\n          [ 4.1891e+00, -2.6343e+00,  2.6864e+00,  ...,  1.2204e+00,\n            2.0671e+00, -2.0118e+00],\n          [ 4.8540e-01, -4.4151e-01,  4.9244e-01,  ..., -2.1734e-01,\n            1.3597e+00, -7.4817e-01],\n          ...,\n          [-2.6706e+00,  6.8963e-01,  2.3554e+00,  ...,  3.0610e-01,\n            1.7619e+00, -1.2003e+00],\n          [-2.0480e+00,  3.3310e-01,  1.2774e+00,  ..., -3.3540e-01,\n            2.1050e+00, -2.6796e+00],\n          [-1.8765e+00,  2.3173e+00,  6.0927e-01,  ..., -4.6700e-01,\n            1.8176e+00, -2.1536e+00]],\n\n         [[ 9.4520e-03, -1.5944e-02, -2.4548e-02,  ...,  1.3063e+00,\n            2.7019e+00, -1.7912e+00],\n          [ 8.7218e-01, -1.0578e+00, -1.5827e+00,  ...,  3.3310e-02,\n           -3.7639e-01,  4.6344e-01],\n          [-1.4165e-01, -3.0495e-01,  3.4495e-02,  ..., -3.5958e-01,\n           -1.0394e+00,  6.4655e-01],\n          ...,\n          [-5.6807e-01,  8.5252e-01, -7.0232e-01,  ..., -1.0918e-01,\n           -2.6005e+00,  7.6607e-01],\n          [-8.6511e-02,  3.2448e-01, -1.0111e+00,  ..., -1.8479e+00,\n            6.0253e-01,  1.4022e+00],\n          [ 1.2307e+00, -1.5465e-02,  7.3055e-01,  ..., -4.3984e-01,\n           -1.7431e+00, -2.3357e+00]],\n\n         ...,\n\n         [[ 1.8422e-01,  1.7773e-01,  2.1953e-02,  ...,  1.0377e+00,\n           -1.6470e+00,  1.0830e+00],\n          [-2.4002e+00,  1.6295e+00, -1.3280e+00,  ...,  4.8186e-02,\n            1.6535e+00, -4.3970e-01],\n          [ 3.0804e-01, -3.1984e-01, -3.4331e-01,  ...,  1.0190e+00,\n           -1.2367e+00, -1.4422e+00],\n          ...,\n          [ 5.7487e-01, -2.2988e-01, -5.5945e-02,  ..., -1.1135e+00,\n            4.8710e-01, -1.2589e+00],\n          [-9.0519e-01,  5.3804e-01,  8.3072e-01,  ..., -1.3570e+00,\n           -1.9944e-02, -1.3020e+00],\n          [-7.4401e-02, -2.7901e-01,  8.7861e-02,  ..., -1.0235e+00,\n            2.4862e-01, -9.2794e-01]],\n\n         [[-2.6273e-03,  2.6221e-03, -3.0507e-02,  ...,  1.1223e+00,\n            1.1003e+00,  1.8951e-01],\n          [ 4.7903e+00,  3.9686e+00,  1.0754e-01,  ...,  2.1310e-01,\n           -2.8108e-02, -8.7820e-01],\n          [ 1.0246e+00,  1.3096e+00,  1.4995e+00,  ..., -2.0850e+00,\n           -1.6414e-02, -1.2134e-01],\n          ...,\n          [-6.4768e+00, -1.2898e+00, -1.7894e+00,  ..., -3.3122e+00,\n           -6.5641e-01, -2.1036e+00],\n          [-8.0486e-01, -8.0386e-01, -6.5649e-01,  ..., -1.9635e+00,\n            3.5160e-02, -2.1034e-01],\n          [-3.8587e-01, -4.6915e+00, -1.9725e+00,  ..., -4.0251e+00,\n           -1.0439e+00, -2.0460e+00]],\n\n         [[-1.6762e-02,  9.5977e-02, -5.1725e-02,  ...,  1.3930e+00,\n            7.2637e-01, -1.7956e+00],\n          [-9.7841e-01,  3.2913e+00, -1.2614e+00,  ..., -1.5560e+00,\n           -1.1715e+00,  1.6520e+00],\n          [ 1.6378e-01, -3.6365e-01, -1.2429e+00,  ...,  3.4448e-01,\n            1.0371e+00, -4.1897e-01],\n          ...,\n          [-1.8854e-01, -5.7922e-01,  4.0452e-01,  ..., -7.6429e-01,\n           -6.2004e-01,  8.1454e-01],\n          [ 3.4380e-01, -1.2955e+00, -1.9310e+00,  ...,  2.8621e-02,\n            1.0520e+00,  5.6551e-01],\n          [ 1.8523e+00, -2.8961e+00,  1.2598e+00,  ..., -4.3885e-01,\n           -8.9879e-01,  7.6234e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.5795e-03,  1.3599e-02, -4.5031e-02,  ...,  3.4113e-04,\n           -2.8627e-03,  1.1434e-01],\n          [-9.9041e-02, -6.0634e-02, -2.1137e-02,  ...,  3.0084e-01,\n            2.7410e-02, -2.8304e-01],\n          [ 5.0372e-02, -7.0389e-03, -5.9483e-03,  ...,  1.2424e-02,\n            9.2066e-02, -1.9506e-02],\n          ...,\n          [ 9.5086e-02, -2.9229e-04,  5.0430e-03,  ...,  3.8130e-02,\n           -3.3565e-02, -8.2930e-03],\n          [ 2.4362e-02, -2.1318e-02, -3.0878e-02,  ..., -6.8150e-02,\n            8.7928e-02,  7.7262e-02],\n          [ 3.4411e-02, -1.3991e-03, -2.1157e-02,  ..., -3.8938e-02,\n            6.3289e-02, -5.3524e-03]],\n\n         [[ 4.5735e-04,  2.5294e-04, -4.7072e-05,  ..., -4.1792e-04,\n            5.8174e-05, -1.2571e-03],\n          [-6.8077e-02,  4.3200e-02, -1.6258e-01,  ...,  1.6611e-01,\n            1.8847e-01,  1.0549e-01],\n          [-1.6937e-03, -3.3735e-03, -7.3070e-03,  ...,  7.0302e-03,\n           -5.8456e-03,  4.7597e-03],\n          ...,\n          [ 2.0076e-01, -6.8954e-02,  1.4551e-02,  ..., -4.2534e-02,\n            1.5192e-01,  3.5590e-01],\n          [-6.8439e-02, -1.4327e-01,  2.9145e-03,  ...,  2.8026e-02,\n            8.6677e-02, -1.3608e-01],\n          [-1.7211e-01,  4.3724e-02, -4.5069e-02,  ..., -1.0670e-02,\n            4.1900e-02, -3.6702e-02]],\n\n         [[-4.7917e-03, -3.6565e-03,  3.6446e-04,  ..., -3.5328e-04,\n           -6.5906e-04, -9.0832e-04],\n          [-9.4917e-02, -2.8329e-02,  4.3898e-02,  ...,  3.1837e-02,\n           -3.1751e-02, -6.8372e-02],\n          [-1.9278e-02, -3.1923e-02, -7.9513e-03,  ..., -4.0204e-02,\n            1.9414e-03, -1.5249e-02],\n          ...,\n          [ 2.0179e-01, -1.4612e-02,  3.0915e-02,  ...,  7.6138e-03,\n           -8.9133e-02,  4.7802e-02],\n          [-5.1400e-02,  5.7345e-02,  7.4082e-02,  ..., -2.9667e-02,\n           -1.5746e-02,  4.2932e-02],\n          [ 1.4780e-01,  4.6155e-03,  4.9988e-02,  ...,  6.1138e-02,\n            9.1901e-03,  1.6400e-02]],\n\n         ...,\n\n         [[-3.7790e-03,  6.0383e-04, -3.2364e-03,  ..., -5.6062e-04,\n           -1.4380e-04,  4.3967e-04],\n          [ 1.7510e-02, -3.3539e-02, -3.3570e-02,  ..., -1.7791e-02,\n           -4.1592e-03,  1.0626e-02],\n          [-1.4188e-01, -2.0370e-02, -3.2847e-02,  ...,  1.2244e-02,\n            6.5973e-03,  2.4519e-02],\n          ...,\n          [ 2.2899e-02, -7.2733e-02,  5.6826e-02,  ...,  2.8298e-02,\n            2.9018e-02, -1.1067e-03],\n          [ 3.3121e-03,  2.2296e-02, -5.5816e-02,  ...,  2.6916e-02,\n           -3.7854e-02, -2.1149e-02],\n          [ 6.6373e-02, -1.4837e-02, -1.9332e-03,  ..., -4.1016e-02,\n           -2.2104e-02,  1.5020e-02]],\n\n         [[-5.6312e-04, -1.6511e-04,  4.0350e-03,  ...,  8.4011e-05,\n           -1.4644e-03, -8.6666e-04],\n          [ 1.0925e-01,  5.0212e-02,  1.4440e-01,  ...,  6.7587e-03,\n           -1.4874e-02, -1.4358e-01],\n          [-7.5719e-03,  5.1073e-03,  1.8936e-02,  ...,  1.7475e-03,\n           -3.7511e-03,  1.0725e-02],\n          ...,\n          [-6.6414e-02,  5.5474e-02, -1.0623e-01,  ..., -7.9432e-02,\n            4.9748e-02, -1.3237e-01],\n          [ 5.7485e-03,  4.3640e-03, -2.1794e-02,  ...,  6.7570e-03,\n            3.4369e-03,  7.0808e-03],\n          [ 1.9519e-01,  4.5070e-02,  1.5320e-02,  ...,  3.4810e-02,\n           -5.0381e-02, -1.9583e-01]],\n\n         [[ 5.6741e-03,  2.7614e-03,  1.6247e-03,  ...,  2.1552e-05,\n            5.5974e-04, -1.0744e-03],\n          [ 3.1769e-02, -1.8088e-01,  7.9264e-03,  ...,  3.3389e-02,\n           -7.1930e-02,  6.4582e-02],\n          [ 7.2849e-03,  2.0841e-03, -1.0420e-03,  ...,  1.5843e-02,\n           -1.1953e-02,  9.6781e-03],\n          ...,\n          [-1.5549e-01,  1.2683e-01, -1.9559e-02,  ..., -5.6759e-02,\n            3.0501e-04,  7.7877e-02],\n          [ 1.7493e-03,  7.1972e-02,  4.0724e-02,  ..., -1.5790e-01,\n           -8.9693e-02, -8.0710e-02],\n          [-1.0907e-02, -1.9946e-01, -1.1967e-01,  ..., -5.7081e-02,\n           -4.5260e-02,  4.0982e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.7967e-02, -1.5538e-01, -3.5137e-02,  ..., -4.5287e-01,\n           -9.7894e-02,  9.9902e-01],\n          [ 2.1435e+00, -1.1052e+00,  2.9003e-01,  ...,  3.4048e-01,\n           -1.8103e-01, -4.0469e+00],\n          [-1.1528e+00,  8.0551e-02, -3.3165e+00,  ..., -4.6863e-01,\n            1.6778e-02, -3.6705e+00],\n          ...,\n          [-2.0375e+00,  2.7392e+00, -9.9266e-01,  ...,  3.6336e-01,\n            3.0317e-01, -3.8284e+00],\n          [-1.0166e+00,  2.8167e+00, -1.3171e+00,  ..., -1.0825e-01,\n           -2.5986e-01, -3.9744e+00],\n          [ 2.1676e+00,  1.0250e+00,  1.3725e-01,  ..., -1.1101e+00,\n            4.4459e-01, -4.0472e+00]],\n\n         [[ 2.0526e-01, -1.0120e-01,  2.8459e-02,  ...,  8.7780e-01,\n           -3.6544e-01, -9.6916e-01],\n          [ 1.4555e+00, -5.0022e+00, -2.4639e+00,  ..., -1.7220e+00,\n            1.2669e+00,  2.8843e+00],\n          [-1.7542e+00, -3.8467e+00, -3.0021e+00,  ..., -1.2341e+00,\n           -2.3595e+00,  3.0494e+00],\n          ...,\n          [-2.5242e+00, -5.1299e-01, -2.8127e+00,  ..., -3.1357e+00,\n            6.3081e-01,  2.5451e+00],\n          [ 3.9007e+00,  1.7564e+00, -1.6855e+00,  ..., -1.6215e+00,\n            1.2152e-01,  3.5846e+00],\n          [ 3.4854e+00,  3.3406e+00, -2.1783e+00,  ..., -2.6598e+00,\n           -1.6483e+00,  2.4277e+00]],\n\n         [[-2.4163e-02, -1.3793e-02, -1.5605e-02,  ..., -3.3829e-01,\n           -2.1568e-01, -2.7395e-01],\n          [-1.6232e+00, -9.9561e-01, -2.0859e+00,  ...,  7.0393e-01,\n           -1.0576e+00, -9.0824e-01],\n          [-7.1267e-02, -2.0515e+00, -1.4744e+00,  ...,  5.6114e-01,\n            5.6179e-01,  4.5942e-01],\n          ...,\n          [ 2.2636e+00, -3.5167e-03, -1.3527e+00,  ...,  6.2748e-01,\n           -4.1023e-01,  6.6814e-01],\n          [ 7.4718e-01,  8.5385e-01, -5.1241e-01,  ...,  6.5449e-01,\n           -2.8959e-01,  1.1189e+00],\n          [-5.5142e-01,  2.2874e+00,  1.1139e-03,  ..., -6.2647e-01,\n           -1.2708e+00,  1.7367e-01]],\n\n         ...,\n\n         [[ 1.8175e-01, -6.8501e-02,  9.8065e-02,  ...,  5.6903e-01,\n            6.9947e-01,  3.4793e-01],\n          [ 1.2064e+00, -1.8463e-01,  2.7027e+00,  ..., -5.7690e+00,\n           -3.5259e+00, -2.6167e+00],\n          [-4.0773e+00,  1.5611e+00,  1.7085e+00,  ..., -1.9952e+00,\n           -4.0612e+00, -4.2319e+00],\n          ...,\n          [-2.2839e+00,  3.9910e+00, -5.4121e-01,  ..., -4.6933e+00,\n           -5.7405e+00, -2.9257e+00],\n          [ 2.5067e+00,  1.7773e+00, -1.2392e+00,  ..., -6.0192e+00,\n           -5.4825e+00, -4.3812e+00],\n          [ 5.1151e+00, -6.7553e-02, -2.5948e+00,  ..., -3.0317e+00,\n           -4.5868e+00,  2.0757e+00]],\n\n         [[ 2.4641e-01, -7.5120e-04,  1.4931e-01,  ...,  4.2755e-02,\n            2.9609e-01, -5.9467e-01],\n          [ 2.2968e+00, -1.0072e+00,  2.1115e+00,  ..., -2.2388e+00,\n            1.3980e+00,  2.3438e+00],\n          [-2.3381e+00, -3.5296e+00,  1.1245e+00,  ...,  1.0777e+00,\n            8.0039e-02,  3.0540e+00],\n          ...,\n          [-4.5341e+00, -1.4650e+00, -1.5961e+00,  ...,  1.7733e-01,\n           -1.4686e+00,  4.1432e+00],\n          [ 7.0741e-01,  1.2569e+00, -2.8351e+00,  ..., -1.0399e+00,\n           -7.8148e-01,  3.2712e+00],\n          [ 6.3685e+00,  2.0896e+00, -4.8673e+00,  ...,  7.3676e-01,\n           -1.1852e+00,  4.6863e+00]],\n\n         [[ 6.5294e-02,  5.4152e-03,  9.5116e-02,  ..., -2.6992e-01,\n            1.3247e-01, -1.0223e+00],\n          [ 1.0522e+00,  1.3931e+00,  1.9404e+00,  ..., -1.5272e+00,\n            1.4662e+00,  1.2632e+00],\n          [ 1.4162e-01,  8.8900e-01,  3.2562e-01,  ...,  4.7265e-01,\n            1.1191e+00,  1.5841e+00],\n          ...,\n          [-9.9284e-01,  5.0924e-01, -1.1408e+00,  ...,  2.1557e+00,\n           -1.8703e-01,  7.6806e-01],\n          [ 1.0320e-01,  1.1855e-01, -1.0493e+00,  ...,  2.0507e+00,\n            1.0512e+00,  7.7381e-01],\n          [ 1.9356e+00, -2.4486e+00, -6.1142e-01,  ..., -7.0353e-01,\n           -8.7412e-01,  8.5476e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-3.2377e-03,  3.6782e-03, -2.0835e-03,  ...,  2.1884e-03,\n           -1.7065e-03, -1.4043e-02],\n          [-6.8249e-02, -9.0891e-02,  7.4523e-02,  ...,  4.0324e-02,\n            1.1355e-01,  8.2261e-02],\n          [-1.2549e-01,  8.1891e-02, -1.0486e-01,  ..., -2.0578e-02,\n           -8.8899e-02,  1.4754e-01],\n          ...,\n          [ 3.4477e-01,  2.5028e-02, -1.5197e-02,  ..., -1.4269e-01,\n            2.6218e-01,  6.2984e-02],\n          [-1.9109e-01, -1.9266e-02,  1.1804e-01,  ..., -3.0260e-01,\n            2.0018e-01,  3.0816e-01],\n          [-1.1212e-01, -8.4712e-02, -8.4004e-03,  ..., -1.1759e-01,\n            6.7740e-02,  6.6850e-02]],\n\n         [[ 4.1326e-04,  1.7973e-03, -3.2491e-03,  ..., -1.8416e-03,\n           -2.1346e-03, -5.6098e-03],\n          [ 7.9583e-02, -1.7815e-01, -5.7443e-03,  ...,  2.3471e-01,\n            1.9975e-01,  7.2629e-02],\n          [ 2.4855e-02,  1.8370e-01, -4.9052e-02,  ..., -9.4083e-02,\n           -9.4262e-03, -1.7727e-02],\n          ...,\n          [ 2.2283e-01,  5.1064e-02,  8.6225e-02,  ..., -5.5915e-02,\n            3.2762e-01,  1.3147e-01],\n          [-6.8345e-02,  6.0235e-02,  1.0391e-01,  ...,  4.2214e-02,\n            2.4259e-02, -2.2577e-01],\n          [-1.5042e-02,  8.3515e-02, -3.7921e-02,  ..., -2.3938e-01,\n            1.6055e-01, -5.0694e-02]],\n\n         [[ 7.5014e-03, -3.1590e-03,  1.1861e-02,  ...,  9.1303e-03,\n            8.1564e-03, -1.5911e-01],\n          [ 6.1164e-02, -7.1458e-03, -6.6609e-02,  ...,  7.1957e-02,\n           -2.3087e-02,  5.9988e-01],\n          [-2.5318e-01, -1.0410e-01, -5.7920e-02,  ...,  2.6127e-01,\n            3.5043e-02,  8.3340e-01],\n          ...,\n          [-3.3453e-02,  1.8735e-01,  6.7241e-02,  ...,  2.0223e-01,\n            1.4901e-01,  5.9556e-01],\n          [ 1.0063e-01,  1.1752e-01,  4.5007e-01,  ...,  6.4598e-02,\n            1.2316e-01,  3.3600e-01],\n          [ 7.8331e-02, -7.1940e-02, -2.5699e-01,  ...,  1.2051e-02,\n            2.1061e-01,  4.6554e-01]],\n\n         ...,\n\n         [[-8.6541e-03, -3.8981e-03, -2.6316e-02,  ...,  1.9426e-02,\n            1.4738e-03,  3.2282e-03],\n          [-5.9135e-02, -9.9246e-02, -3.6900e-01,  ...,  1.0190e-01,\n            2.3477e-01, -3.7322e-03],\n          [ 6.5881e-02, -9.7573e-02, -9.5627e-02,  ...,  9.0878e-02,\n           -1.9417e-02, -1.0951e-02],\n          ...,\n          [-1.9661e-01,  1.1670e-01, -2.6479e-02,  ...,  1.9701e-02,\n            2.5399e-02, -9.7587e-02],\n          [-1.2173e-01, -1.6699e-01,  9.9858e-02,  ...,  5.7304e-02,\n           -4.9886e-02, -4.7641e-02],\n          [ 1.9774e-01,  1.5135e-02, -5.3215e-02,  ...,  8.8475e-02,\n            1.5693e-01, -9.6357e-02]],\n\n         [[ 5.1806e-03, -5.2374e-03, -2.2343e-03,  ..., -1.5930e-03,\n            3.5463e-03, -4.3412e-03],\n          [-5.9287e-02,  7.1173e-02, -2.3751e-02,  ..., -2.0564e-01,\n            2.2858e-01,  3.2094e-01],\n          [ 8.5236e-02,  6.9737e-02,  6.1274e-02,  ..., -1.8012e-01,\n           -3.3260e-02,  2.3315e-02],\n          ...,\n          [ 1.1825e-02, -5.9067e-02, -1.0706e-01,  ..., -1.6356e-01,\n           -3.4370e-01,  2.4248e-02],\n          [ 4.9969e-02,  1.4826e-01, -3.9919e-02,  ..., -4.0458e-02,\n            6.6567e-02, -8.3417e-02],\n          [ 1.0430e-01, -3.2007e-02, -1.4930e-01,  ...,  3.6746e-01,\n            1.4962e-01,  1.6755e-01]],\n\n         [[-5.6988e-04, -8.3401e-03, -8.2436e-03,  ..., -1.5176e-02,\n            7.5245e-03, -3.7639e-03],\n          [ 1.8856e-01,  2.7176e-01, -1.5545e-01,  ..., -1.4014e-02,\n           -3.9068e-02,  1.3135e-01],\n          [-5.3303e-02,  1.8317e-02,  3.4295e-02,  ..., -9.3917e-02,\n            6.7584e-02,  3.9990e-02],\n          ...,\n          [-1.3570e-01,  1.3095e-01,  1.1496e-01,  ..., -5.9814e-01,\n            9.0253e-02,  1.7751e-01],\n          [ 3.8223e-02,  3.8875e-02, -1.8843e-01,  ..., -9.1675e-02,\n            5.0060e-02, -2.1880e-02],\n          [-4.1986e-01,  2.1757e-01, -1.4327e-01,  ...,  7.7314e-02,\n            9.4747e-02,  8.5389e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7701e-01,  1.2814e-01, -5.6786e-02,  ...,  1.3507e-01,\n           -1.4585e+00, -6.9457e-01],\n          [ 2.6764e-01,  2.9066e+00, -3.2170e+00,  ...,  8.7905e-02,\n            4.8198e+00,  2.8236e+00],\n          [ 9.2695e-01,  1.4156e+00, -7.8957e-01,  ...,  1.7659e-01,\n            3.0773e+00,  9.0030e-01],\n          ...,\n          [-2.9926e-01,  3.5457e-02, -1.4828e+00,  ..., -1.5730e-01,\n            4.5621e+00,  9.8181e-01],\n          [-6.7563e-01, -2.2247e+00, -1.5221e+00,  ...,  5.3594e-01,\n            4.2234e+00,  1.0064e+00],\n          [-2.3657e+00, -1.6536e+00, -1.2066e+00,  ..., -4.5254e-01,\n            5.0909e+00,  1.6524e+00]],\n\n         [[-6.3640e-02, -1.6707e-02,  1.4471e-02,  ...,  4.7435e-01,\n           -3.9084e-01, -2.5429e-01],\n          [ 1.1823e+00,  5.1502e-01, -4.1835e-01,  ...,  1.1009e-01,\n           -2.9313e-01, -8.9718e-01],\n          [ 1.3956e+00,  1.6913e+00, -8.9101e-01,  ...,  8.5906e-02,\n            7.4851e-01, -8.5068e-01],\n          ...,\n          [-1.0720e+00,  1.5869e+00, -1.4292e+00,  ...,  4.5084e-01,\n           -3.9096e-01, -2.0741e+00],\n          [-1.3015e+00, -2.1746e-01, -1.5711e+00,  ...,  8.3230e-01,\n            9.6939e-01, -7.3736e-01],\n          [-1.5033e+00, -8.2268e-01, -7.3206e-01,  ...,  1.1917e+00,\n           -1.2171e+00, -1.7185e+00]],\n\n         [[ 8.8415e-02, -7.7824e-02,  5.8160e-02,  ...,  3.3606e-01,\n           -1.3101e+00, -1.1014e+00],\n          [-2.3669e+00, -3.6616e+00, -1.2765e+00,  ...,  2.0766e+00,\n            3.3702e+00,  5.0247e+00],\n          [-3.1251e+00, -3.1575e+00, -7.4505e-01,  ...,  4.9128e-01,\n            1.8747e+00,  4.4647e+00],\n          ...,\n          [ 2.0000e+00, -6.1031e-01, -1.6045e+00,  ...,  7.2175e-01,\n            3.4087e+00,  3.7380e+00],\n          [ 3.2028e+00,  2.2939e+00, -4.2439e+00,  ...,  4.8598e-01,\n            2.9832e+00,  3.1867e+00],\n          [ 2.3772e+00,  2.5177e+00, -1.3139e+00,  ...,  1.4361e+00,\n            3.7860e+00,  4.9523e+00]],\n\n         ...,\n\n         [[-2.5395e-01, -1.4475e-01,  1.7906e-01,  ..., -1.9359e-01,\n           -3.9308e-01,  8.8821e-01],\n          [-2.6796e+00, -2.0157e+00,  3.1024e+00,  ...,  4.2189e-01,\n           -1.3306e+00, -4.9016e+00],\n          [ 1.6348e+00, -5.3055e-02,  1.9740e+00,  ..., -1.4344e+00,\n           -2.4934e-01, -3.7893e+00],\n          ...,\n          [ 2.8807e+00,  1.4479e+00, -1.4614e-03,  ..., -1.0776e+00,\n            2.9119e-01, -3.3255e+00],\n          [-1.1927e+00,  1.2245e+00, -1.0029e+00,  ...,  5.8190e-01,\n            1.8443e+00, -3.6302e+00],\n          [-3.7693e+00,  2.5642e+00, -1.8331e+00,  ..., -1.2635e+00,\n           -5.5469e-02, -3.2170e+00]],\n\n         [[ 1.0696e-02, -8.8757e-02, -1.3791e-01,  ..., -5.1921e-01,\n            2.0680e-01, -9.1169e-02],\n          [ 6.0183e-01, -8.7852e-01, -2.0071e+00,  ...,  3.2347e-01,\n            6.0076e-01, -6.7956e-01],\n          [ 1.1107e+00, -1.4100e-01, -1.3417e+00,  ...,  2.6395e+00,\n            1.8479e+00,  1.0224e+00],\n          ...,\n          [-1.5660e+00,  8.7282e-01,  9.3701e-01,  ...,  1.8857e+00,\n            8.2834e-01,  2.4639e-01],\n          [-1.4213e+00,  1.9002e+00,  7.0173e-01,  ...,  1.7888e+00,\n            1.6171e+00, -8.0769e-01],\n          [-1.2590e+00,  1.6183e+00,  1.4074e+00,  ...,  8.6553e-01,\n           -4.7060e-01,  5.8349e-01]],\n\n         [[ 7.1884e-02,  1.4028e-01,  6.8085e-02,  ...,  2.4312e-01,\n           -1.7492e-01,  2.5649e-01],\n          [ 5.1536e+00,  3.8639e+00,  2.2726e+00,  ...,  4.6500e-01,\n           -2.1876e+00, -8.3482e-01],\n          [ 2.9126e+00,  3.1976e+00,  3.2291e+00,  ...,  7.0219e-01,\n           -1.1357e-01,  1.1334e+00],\n          ...,\n          [-4.1209e+00,  2.7656e-01,  2.6617e+00,  ..., -3.8955e-01,\n           -2.3895e-01,  3.2122e+00],\n          [-2.8110e+00,  3.8436e-01,  1.9234e+00,  ..., -1.6735e+00,\n            7.6658e-01,  1.5494e+00],\n          [-1.6525e+00, -1.5763e+00,  6.6924e-01,  ..., -1.4195e+00,\n           -1.4014e+00,  4.7321e-02]]]], grad_fn=<AddBackward0>), tensor([[[[-6.1820e-03,  5.4064e-03,  8.8012e-03,  ...,  1.4157e-02,\n            5.4133e-03, -9.3104e-03],\n          [ 2.9941e-01,  6.7903e-01, -1.6327e-01,  ...,  8.3898e-02,\n            2.1157e-01, -7.8838e-02],\n          [ 3.1184e-01,  7.6078e-02,  3.5302e-01,  ..., -2.8107e-01,\n           -1.5467e-01,  4.1138e-02],\n          ...,\n          [-9.6306e-02,  6.9173e-02,  6.5667e-02,  ...,  2.6577e-01,\n           -6.4739e-02, -3.4857e-02],\n          [-2.4713e-01, -3.5561e-01, -1.4759e-01,  ...,  1.7052e-01,\n            7.5728e-02, -1.5410e-01],\n          [-2.3465e-01,  1.5675e-01,  2.8819e-01,  ...,  1.2776e-01,\n            6.8719e-02, -2.5834e-01]],\n\n         [[-1.2713e-02,  1.2328e-01,  2.5158e-03,  ..., -2.7015e-03,\n            9.6137e-03,  1.6953e-03],\n          [ 7.1951e-01, -1.6914e-01, -1.1247e-01,  ..., -4.8129e-01,\n            1.4451e-01, -6.2263e-02],\n          [-2.4158e-01, -5.0414e-02,  1.1081e-01,  ...,  4.6947e-02,\n           -1.9938e-01,  7.7349e-02],\n          ...,\n          [-1.9011e-01, -1.5957e-01,  1.9725e-01,  ...,  2.3459e-01,\n            1.6987e-01,  1.8029e-01],\n          [-3.1155e-01, -3.1968e-01, -2.1609e-01,  ...,  2.5294e-01,\n            6.0543e-02,  1.5184e-01],\n          [ 1.5025e-01,  1.5711e-01, -1.4395e-02,  ..., -1.5603e-01,\n            6.3068e-02,  3.9409e-01]],\n\n         [[ 7.1219e-03,  3.1417e-03, -1.0653e-02,  ...,  1.6981e-02,\n           -7.8259e-03,  4.3196e-03],\n          [-1.9960e-01,  2.7524e-01,  1.9772e-02,  ..., -1.6835e-01,\n            8.5904e-02, -2.8022e-01],\n          [-1.0467e-01,  3.5048e-02,  1.1350e-02,  ...,  5.4866e-02,\n           -1.1864e-01,  2.1198e-01],\n          ...,\n          [-1.8213e-01,  9.0273e-03, -1.1572e-01,  ..., -9.9041e-02,\n            8.4770e-02,  4.1519e-01],\n          [-1.2092e-01, -3.8780e-01, -3.6987e-01,  ..., -3.8170e-01,\n           -2.6693e-02,  3.6690e-01],\n          [-5.4785e-02, -3.9241e-01, -1.1562e-01,  ...,  4.1272e-01,\n            3.9609e-01,  1.8474e-01]],\n\n         ...,\n\n         [[ 3.3089e-02,  1.1034e-03, -6.1658e-03,  ..., -2.2012e-01,\n            2.6411e-03, -9.0748e-04],\n          [ 2.0938e-01, -6.8443e-01, -3.5323e-01,  ...,  1.0073e+00,\n            2.5970e-01,  2.7422e-01],\n          [-9.7218e-01, -6.1478e-01,  2.2379e-01,  ...,  1.2794e+00,\n            1.3615e-01, -8.1561e-01],\n          ...,\n          [-3.2407e-01, -5.6607e-01,  1.0611e-01,  ...,  1.0359e+00,\n           -1.1281e-01, -1.8238e-01],\n          [-2.6283e-01,  2.3867e-02, -5.1711e-01,  ...,  1.1837e+00,\n            1.3125e-01,  1.7411e-01],\n          [-3.9711e-01, -2.7209e-01, -1.6776e-01,  ...,  8.4565e-01,\n           -7.8791e-02,  3.2866e-02]],\n\n         [[ 5.5222e-03,  5.5187e-03, -2.4976e-03,  ..., -1.4439e-02,\n            2.8718e-03, -1.2763e-02],\n          [-9.6075e-02, -8.6414e-02,  7.9327e-02,  ...,  3.3177e-02,\n            5.1969e-02, -1.4187e-01],\n          [-7.6938e-02, -2.0906e-01,  1.4066e-01,  ..., -1.1291e-01,\n            1.5014e-01, -7.1458e-02],\n          ...,\n          [ 1.4898e-02, -6.8294e-02, -7.0356e-02,  ...,  2.5958e-02,\n           -4.2350e-02,  7.3146e-02],\n          [-1.7195e-01, -3.7946e-01, -6.4187e-02,  ...,  1.7663e-02,\n           -1.0420e-02,  1.2907e-01],\n          [-1.8034e-01,  2.7801e-03,  4.4255e-01,  ..., -2.3100e-01,\n           -4.6818e-02,  2.5140e-01]],\n\n         [[ 9.3314e-03, -1.0290e-02, -8.7502e-03,  ...,  1.6052e-03,\n            4.6391e-03, -5.5440e-04],\n          [ 3.4664e-01, -8.4540e-01,  1.8082e-01,  ..., -1.9415e-01,\n            1.3139e-01,  3.4814e-02],\n          [ 6.2430e-01,  1.2965e-01,  3.7513e-02,  ..., -2.3402e-02,\n            5.5743e-02,  9.9857e-02],\n          ...,\n          [ 1.9298e-01, -3.6481e-01,  2.9809e-02,  ...,  2.0951e-02,\n            6.1376e-02, -8.9263e-02],\n          [-1.6985e-01,  3.1712e-01, -2.7905e-01,  ..., -2.4076e-01,\n            5.8419e-02, -1.1336e-01],\n          [-1.5146e-01, -2.7222e-01,  1.6510e-03,  ..., -6.8705e-03,\n           -4.0594e-01,  1.8751e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.2282e-02, -1.0165e-01, -1.1627e-01,  ..., -1.1198e+00,\n            1.2367e+00,  9.2278e-01],\n          [ 2.6947e+00, -1.1263e+00, -2.5378e+00,  ...,  3.8508e-01,\n            7.6976e-01, -9.4456e-01],\n          [ 1.3896e+00, -5.4602e-01, -1.3699e+00,  ..., -3.5502e-01,\n            9.4520e-01, -1.5680e+00],\n          ...,\n          [-1.7908e+00,  9.5140e-01, -1.4063e+00,  ..., -2.5337e-01,\n            6.5877e-02, -5.4208e-01],\n          [-7.0496e-01,  1.6231e-01, -5.4211e-01,  ..., -6.9233e-01,\n            1.3365e+00, -2.8431e+00],\n          [-2.0937e-01, -1.1077e-01, -5.4180e-01,  ..., -4.4429e-01,\n            6.7587e-01, -9.7499e-01]],\n\n         [[ 2.0206e-01,  1.7876e-01,  2.1073e-01,  ..., -1.2444e-01,\n            6.5449e-01,  1.8805e-01],\n          [ 5.5494e+00,  2.7431e-01,  2.8697e+00,  ...,  7.7296e-01,\n           -4.0696e-01,  8.3838e-01],\n          [ 1.1737e+00,  3.4204e-03,  2.3668e+00,  ...,  3.9985e-01,\n            9.8103e-01, -7.5665e-01],\n          ...,\n          [-3.2539e+00, -3.5224e+00,  2.2123e+00,  ..., -1.0414e+00,\n           -5.9234e-01,  1.4053e+00],\n          [-8.2012e-01, -4.5371e+00,  3.9510e-01,  ...,  1.8237e+00,\n            1.3257e+00,  1.9354e+00],\n          [ 1.8927e+00, -2.3753e+00, -4.3274e-01,  ..., -7.4800e-01,\n           -4.4143e-01,  1.1267e+00]],\n\n         [[ 2.1973e-02,  7.3497e-02, -5.5480e-02,  ..., -1.8062e-01,\n            1.7539e+00, -1.5784e+00],\n          [ 3.3127e+00,  2.5819e+00, -2.4716e+00,  ...,  1.6049e+00,\n           -5.4206e+00,  6.3410e+00],\n          [ 2.7562e+00,  2.5899e+00, -3.3643e+00,  ..., -1.7875e-01,\n           -4.6014e+00,  6.0351e+00],\n          ...,\n          [-2.7383e+00,  8.6641e-01, -4.0272e+00,  ...,  6.0972e-01,\n           -5.1756e+00,  5.1306e+00],\n          [-2.2211e+00,  1.1909e+00, -2.4088e+00,  ...,  3.1355e-01,\n           -5.1743e+00,  4.1928e+00],\n          [-1.0697e+00, -1.9966e+00, -1.2744e+00,  ..., -1.0002e-01,\n           -5.1887e+00,  5.8612e+00]],\n\n         ...,\n\n         [[ 1.1266e-01, -1.1705e-01,  7.7527e-02,  ..., -5.3134e-01,\n           -1.2289e+00,  2.8861e-01],\n          [ 3.4821e-01, -2.3385e+00,  1.6136e+00,  ..., -1.4544e+00,\n            4.0061e-01, -3.1225e-01],\n          [-1.1731e+00, -4.6898e-02,  1.0132e+00,  ..., -1.2839e+00,\n           -1.2630e-01,  9.5137e-01],\n          ...,\n          [-2.0203e-01,  1.2111e+00, -1.6005e-02,  ..., -5.1681e-01,\n           -3.3450e-01, -2.1627e-01],\n          [ 1.1841e+00, -2.7654e-01,  1.8011e-01,  ..., -5.9574e-01,\n           -6.3698e-01,  3.0269e-01],\n          [ 1.0746e+00,  7.7070e-01, -9.6412e-01,  ..., -1.0072e+00,\n           -7.1167e-02, -1.6784e-01]],\n\n         [[-2.3820e-01, -1.1915e-01, -1.3883e-01,  ...,  1.2986e+00,\n           -1.2263e+00,  1.8063e+00],\n          [-1.6037e+00, -1.0995e+00, -2.6342e+00,  ..., -1.7757e+00,\n           -1.0154e+00, -2.1865e+00],\n          [ 1.9105e+00,  8.1207e-01, -7.8228e-01,  ..., -1.4002e+00,\n           -1.6378e+00, -2.3000e+00],\n          ...,\n          [ 9.5449e-01,  1.6893e+00,  1.3387e+00,  ..., -9.9793e-01,\n            3.8434e-01, -8.7498e-01],\n          [-1.4753e+00,  2.2700e+00,  1.4921e+00,  ...,  2.1434e+00,\n            1.2297e+00, -9.2150e-01],\n          [-2.7791e+00,  3.6354e-01,  2.2254e+00,  ..., -2.0619e+00,\n           -1.6319e+00, -2.4281e+00]],\n\n         [[ 1.3673e-01,  1.1688e-01,  1.1799e-01,  ..., -7.8946e-01,\n           -2.1710e+00,  1.2962e+00],\n          [ 1.4964e+00,  3.5646e+00,  6.2662e-01,  ..., -6.0104e-01,\n            4.9614e+00, -1.0345e-01],\n          [-9.5273e-01,  8.1729e-01,  6.4827e-01,  ...,  9.7054e-02,\n            7.1365e+00,  5.5422e-01],\n          ...,\n          [-7.5994e-01, -3.1559e-01,  3.6393e-01,  ..., -2.4653e-01,\n            5.8504e+00, -1.6054e-01],\n          [ 6.1265e-01, -4.1892e-01,  2.2466e-01,  ..., -3.9562e-01,\n            6.2402e+00,  2.3792e-01],\n          [ 1.0237e+00, -1.5125e+00, -1.0386e-02,  ...,  2.2863e-01,\n            5.2406e+00, -2.2171e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.4321e-03, -8.7532e-03, -4.7245e-03,  ...,  8.1068e-03,\n            8.7612e-04,  6.9124e-03],\n          [-1.0467e-01, -1.9611e-02,  2.4791e-02,  ..., -4.3568e-02,\n           -5.7890e-02,  2.3274e-01],\n          [-9.9035e-02, -3.5960e-01,  1.4438e-01,  ..., -2.2842e-02,\n            1.6327e-01,  3.1524e-01],\n          ...,\n          [-3.0338e-01, -7.6661e-03,  4.5093e-01,  ...,  1.9181e-01,\n            2.8696e-01, -9.1073e-02],\n          [-3.0866e-02, -6.2382e-02,  2.9084e-02,  ..., -1.5676e-01,\n           -1.7092e-01,  1.1898e-01],\n          [-2.6783e-01, -1.7185e-01,  1.7530e-01,  ...,  1.3997e-01,\n            9.0545e-02, -3.8462e-01]],\n\n         [[ 7.0619e-04, -3.2205e-03,  8.7327e-03,  ...,  1.4436e-03,\n            2.0123e-02,  9.0113e-04],\n          [ 1.2734e-02, -4.0825e-02,  4.3852e-03,  ..., -1.2760e-02,\n           -5.4517e-01, -2.6769e-01],\n          [-1.6712e-01, -1.6344e-01,  9.6333e-02,  ..., -6.8344e-03,\n           -2.1898e-02, -3.1479e-01],\n          ...,\n          [ 3.0523e-01,  2.7330e-01, -6.8375e-03,  ..., -1.5098e-01,\n           -1.0993e-01,  3.0583e-01],\n          [ 3.4018e-01, -7.7130e-02, -1.4879e-01,  ..., -3.0749e-02,\n            5.6093e-01,  1.9593e-01],\n          [ 1.0211e-01,  1.3157e-01, -1.3349e-01,  ...,  1.2502e-01,\n           -6.5941e-02, -1.1809e-01]],\n\n         [[-2.7866e-03,  1.0993e-02,  6.1155e-03,  ...,  1.5371e-02,\n            2.8720e-03, -2.3152e-02],\n          [-6.4073e-02,  1.5430e-02,  2.6343e-01,  ...,  8.0253e-02,\n            1.7867e-01,  1.2398e+00],\n          [ 5.1941e-02,  1.3583e-01, -1.6573e-01,  ..., -8.3535e-02,\n            1.3651e-01,  1.5384e-01],\n          ...,\n          [-1.2322e-01,  1.8373e-01, -8.2243e-02,  ...,  1.0742e-01,\n            3.1176e-01,  3.3107e-01],\n          [-1.7566e-02,  1.9896e-01, -1.6854e-02,  ...,  7.8815e-02,\n            2.5707e-01,  1.6920e-02],\n          [ 7.9629e-03,  1.1664e-02, -1.9343e-01,  ..., -2.2412e-02,\n            2.4916e-01,  3.5274e-01]],\n\n         ...,\n\n         [[-1.4118e-02, -3.8184e-03,  1.3744e-02,  ..., -4.0826e-04,\n           -4.0499e-03,  8.0336e-03],\n          [ 5.4029e-02, -1.0008e-01, -3.6924e-01,  ..., -5.0542e-01,\n           -1.2258e-01, -1.0312e-01],\n          [ 2.2759e-02, -7.9112e-02, -9.1752e-02,  ..., -3.1552e-01,\n           -1.4367e-01,  7.4229e-02],\n          ...,\n          [-2.3685e-01, -5.3028e-01, -7.6086e-02,  ..., -9.8673e-02,\n           -1.3463e-02,  2.3224e-01],\n          [-4.5401e-01,  1.1683e-01, -2.2798e-02,  ..., -6.0475e-01,\n           -3.5990e-01, -1.3324e-01],\n          [-1.8999e-01,  7.5736e-02,  2.8255e-02,  ..., -1.2506e-01,\n           -1.9876e-01,  2.6780e-01]],\n\n         [[-3.5600e-03,  2.4092e-04, -9.2962e-03,  ..., -1.4673e-02,\n            4.7458e-03, -1.1192e-02],\n          [-1.5482e-02,  1.1980e-01,  2.1448e-01,  ...,  6.4581e-02,\n           -1.7597e-01,  1.1077e-01],\n          [ 2.6037e-02,  7.4305e-02, -1.0224e-01,  ..., -6.9476e-02,\n           -9.4906e-02, -2.6536e-01],\n          ...,\n          [ 1.0273e-01,  8.0308e-02,  1.1856e-01,  ...,  1.3968e-01,\n            1.5410e-01, -1.1429e-01],\n          [ 1.1800e-01,  5.7852e-02,  1.4395e-01,  ...,  1.1984e-01,\n            1.3060e-01, -9.2328e-02],\n          [-1.5998e-01, -8.2837e-02,  8.2714e-02,  ..., -3.5956e-02,\n           -9.6886e-02, -8.8186e-02]],\n\n         [[ 5.8911e-03, -8.2265e-03,  5.3291e-03,  ...,  8.7417e-04,\n           -1.3503e-02, -1.0993e-02],\n          [ 2.8653e-03,  4.0489e-03, -2.4093e-01,  ..., -1.3937e-01,\n            2.8739e-01,  5.5181e-01],\n          [ 1.2024e-01, -1.2629e-02,  2.7387e-02,  ...,  1.7477e-01,\n            1.2676e-01,  1.6036e-01],\n          ...,\n          [ 5.3377e-02,  2.0845e-01,  4.0209e-03,  ...,  7.6788e-02,\n            5.8192e-01,  2.6301e-01],\n          [ 3.5664e-01, -8.8840e-02, -1.1054e-01,  ...,  7.6085e-03,\n            6.6291e-01, -1.2942e-01],\n          [ 2.7074e-02,  1.2920e-01, -1.4235e-01,  ...,  3.5492e-01,\n            7.7556e-02,  6.1752e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0673, -0.0380,  0.0587,  ..., -0.2401,  1.2458, -0.5890],\n          [-2.6230, -1.4096,  1.0336,  ..., -1.0997,  0.7870, -2.2670],\n          [ 0.0684, -0.8596, -0.4222,  ...,  0.0686,  1.8364, -1.7942],\n          ...,\n          [ 0.4360,  0.0993, -0.3778,  ...,  0.2330,  0.4170, -1.8798],\n          [-0.4057,  0.0715, -0.6293,  ...,  0.2380,  1.6907, -3.2166],\n          [-0.4575,  0.9622, -0.8712,  ...,  0.7507,  0.7713, -1.9774]],\n\n         [[-0.1482,  0.1215, -0.1029,  ..., -1.0353, -1.0146,  0.8210],\n          [-2.7964,  1.4028, -2.8288,  ..., -0.8176, -0.8423,  1.3311],\n          [ 1.2582,  0.1732, -1.6765,  ..., -0.2601, -0.9366,  1.1228],\n          ...,\n          [ 1.3964, -1.2859,  0.7055,  ...,  0.5220, -1.2482, -0.1308],\n          [-1.7408, -1.5983,  0.9078,  ...,  0.0471, -0.5481,  0.2924],\n          [-1.7288, -1.0642,  1.4368,  ...,  0.5570, -1.4549, -0.8904]],\n\n         [[ 0.0888, -0.0451, -0.0820,  ...,  1.2527, -1.6093,  0.8153],\n          [ 1.1792, -1.2164, -3.3174,  ...,  1.8395,  2.3515, -0.6136],\n          [-1.5866,  0.1771, -1.9733,  ...,  1.0520,  2.0242, -0.3927],\n          ...,\n          [-1.0689,  1.1365, -0.1952,  ...,  0.5337,  1.3976, -0.5168],\n          [ 0.3324,  1.2364, -0.5633,  ...,  0.5687,  2.1237, -0.3122],\n          [ 2.0921,  0.1781,  0.9043,  ...,  1.3126,  1.3001,  0.6655]],\n\n         ...,\n\n         [[ 0.1052, -0.0565,  0.0683,  ..., -0.1388,  0.3983,  1.4133],\n          [ 3.4661, -1.0068,  1.4672,  ..., -0.7612, -0.6297, -3.9924],\n          [ 0.2750,  0.7824,  0.9173,  ..., -0.4139,  0.6396, -3.5821],\n          ...,\n          [-1.0323,  0.2570, -0.7744,  ..., -0.0266,  0.3943, -1.8256],\n          [ 0.6397,  0.2168, -0.2871,  ..., -0.5229,  0.9866, -2.3851],\n          [ 0.4325,  0.0755, -0.7504,  ..., -0.3195,  1.3672, -1.1089]],\n\n         [[ 0.1472,  0.0827,  0.0479,  ...,  0.3101,  0.0898, -0.3013],\n          [ 1.7290,  1.5935,  0.6793,  ...,  0.7582,  1.3635,  0.5977],\n          [-0.9900,  0.1284,  1.2406,  ...,  0.2236, -0.4528, -1.5582],\n          ...,\n          [-0.7661, -0.5840,  2.1097,  ..., -0.0846,  0.3760,  0.1993],\n          [ 0.5308, -1.5368,  1.5667,  ..., -0.7289, -0.2877, -1.1430],\n          [ 1.6712, -0.8101,  1.7596,  ...,  1.8957,  0.6416,  0.1783]],\n\n         [[-0.1321,  0.1387, -0.0576,  ..., -0.8415,  1.0974,  0.2400],\n          [ 0.5177,  3.2533, -1.6478,  ..., -0.1231, -0.1318,  1.0980],\n          [ 2.1462,  0.3267, -2.0895,  ..., -0.5722,  0.1243,  0.8788],\n          ...,\n          [ 0.0263, -0.7484, -1.8913,  ..., -1.8921, -0.5946,  1.4830],\n          [-1.3986, -0.6276, -1.3345,  ..., -1.0882,  1.3606,  1.5931],\n          [-1.7844, -1.3445, -0.5183,  ..., -0.7938, -0.0828,  1.6731]]]],\n       grad_fn=<AddBackward0>), tensor([[[[ 7.0184e-03, -2.6607e-01,  8.9918e-03,  ...,  1.0008e-02,\n            9.8950e-03,  2.1153e-02],\n          [-1.7189e-02,  7.4637e-01,  7.3324e-02,  ...,  1.7711e-01,\n            1.8271e-01, -1.9652e-01],\n          [ 4.3036e-01,  4.6241e-01, -2.1777e-02,  ..., -5.7014e-02,\n           -4.1061e-01, -3.6297e-01],\n          ...,\n          [ 1.1786e-01,  3.7584e-01, -1.6500e-01,  ...,  4.3060e-01,\n            1.1172e-01, -3.4991e-01],\n          [ 1.5505e-01,  6.0234e-01, -2.4335e-03,  ..., -1.4892e-01,\n            1.3472e-01, -1.7865e-01],\n          [ 8.8841e-02,  5.1718e-01, -3.3933e-01,  ...,  2.1001e-01,\n           -2.9975e-02, -1.9431e-01]],\n\n         [[-5.5726e-04, -7.7484e-03, -5.9076e-03,  ..., -2.0097e-02,\n           -3.3195e-03, -1.2772e-02],\n          [-1.1337e-01,  2.4335e-02,  1.2576e-01,  ..., -2.8663e-02,\n            7.0448e-01,  1.6985e-01],\n          [-4.6211e-01,  5.2771e-02,  1.5753e-01,  ..., -2.2525e-01,\n            1.1754e-01, -1.0439e-01],\n          ...,\n          [-5.2622e-01,  3.3897e-01, -3.6210e-03,  ..., -1.6833e-01,\n            8.1798e-03, -3.5989e-02],\n          [-1.7898e-01,  1.4902e-01,  4.3760e-02,  ..., -6.7576e-02,\n            9.4174e-02, -1.0657e-01],\n          [ 1.6964e-01, -3.3308e-01, -1.3923e-01,  ..., -3.0010e-01,\n           -2.0117e-01,  1.6435e-02]],\n\n         [[-3.5693e-02, -7.9178e-03,  1.0561e-02,  ...,  7.9003e-03,\n            1.1250e-02,  1.1793e-02],\n          [-2.1157e-01, -9.3278e-01,  1.2789e-01,  ..., -2.2399e-02,\n           -3.4019e-01, -3.9702e-01],\n          [ 1.5232e-01, -6.8308e-01, -1.2667e-02,  ..., -4.4224e-02,\n            1.6209e-01, -4.1417e-01],\n          ...,\n          [ 1.5063e-01, -2.0814e-01,  2.1461e-01,  ..., -9.8562e-02,\n           -3.6636e-02, -3.1627e-01],\n          [ 3.9176e-01, -1.8605e-01,  2.8391e-02,  ..., -9.8264e-02,\n           -9.5677e-03, -3.8438e-01],\n          [-5.0047e-02, -4.1200e-01,  2.9480e-01,  ...,  3.6376e-01,\n           -8.1028e-02, -4.4138e-01]],\n\n         ...,\n\n         [[-1.7020e-02,  2.0514e-02, -1.3219e-02,  ..., -2.7396e-03,\n           -2.3856e-03,  1.1714e-02],\n          [-4.4627e-02, -9.1744e-02, -9.9211e-02,  ...,  7.2449e-02,\n            9.8154e-02, -3.2808e-02],\n          [ 7.0124e-02,  1.2697e-01,  1.8704e-01,  ..., -1.5831e-01,\n           -1.7343e-02,  1.6386e-01],\n          ...,\n          [-2.0358e-01, -1.5230e-01,  3.7804e-02,  ...,  7.0350e-02,\n           -4.1890e-01,  5.0514e-02],\n          [-1.2607e-02,  2.3928e-01, -3.0951e-02,  ...,  1.8131e-01,\n           -3.2832e-01, -2.9254e-01],\n          [-7.5559e-02, -2.4791e-02, -7.3714e-02,  ...,  2.6189e-01,\n           -1.7322e-01, -8.1548e-02]],\n\n         [[ 2.9759e-03,  9.9851e-03,  1.2614e-02,  ...,  4.7422e-02,\n            1.3567e-02, -3.1465e-03],\n          [ 1.0880e-01,  1.3510e+00,  1.3634e-01,  ...,  2.3499e-02,\n           -1.7103e-01,  1.1104e-01],\n          [ 1.5093e-01,  5.5053e-01,  1.4742e-01,  ..., -5.6128e-02,\n            8.8363e-02,  1.1552e-01],\n          ...,\n          [-8.5937e-02,  1.9849e-01, -5.3613e-02,  ..., -2.0514e-01,\n           -2.6807e-02, -6.9927e-03],\n          [-3.7672e-01, -3.1404e-01, -1.6301e-01,  ..., -1.0568e-01,\n           -2.0208e-01,  3.4364e-01],\n          [ 1.1627e-01,  4.4837e-01, -6.7039e-01,  ...,  2.4221e-01,\n           -4.9293e-01,  2.4366e-01]],\n\n         [[-9.0115e-03,  5.8156e-03,  5.7625e-03,  ...,  1.5618e-03,\n           -8.1958e-03, -1.0273e-02],\n          [ 3.4790e-01, -3.6076e-01,  2.4894e-01,  ..., -3.2158e-01,\n           -5.9973e-02, -1.2560e-01],\n          [ 2.7315e-01, -8.5368e-02,  1.3310e-01,  ...,  1.5860e-01,\n           -1.6372e-01, -8.2068e-03],\n          ...,\n          [-1.0695e-01, -2.5185e-03,  1.4617e-01,  ..., -4.5404e-02,\n           -1.9578e-01, -2.9609e-02],\n          [ 1.1120e-01,  9.2414e-02,  3.8157e-02,  ..., -4.1317e-02,\n           -1.5420e-01, -2.1623e-01],\n          [-1.6257e-01, -1.2010e-02,  6.4325e-02,  ...,  2.4202e-01,\n           -1.9851e-01, -1.4417e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.6317e-02,  1.3466e-01,  5.4830e-02,  ..., -9.1992e-01,\n           -1.4019e+00,  1.4818e+00],\n          [ 1.2326e+00,  3.1613e+00,  1.6568e+00,  ...,  1.8509e-01,\n           -1.3722e+00, -6.6491e+00],\n          [ 1.9328e+00,  1.8872e+00,  2.0568e+00,  ...,  3.9007e-01,\n           -7.1264e-01, -6.8318e+00],\n          ...,\n          [-1.2359e+00, -1.2981e+00,  2.6715e+00,  ..., -5.8398e-01,\n            1.1117e-01, -7.2999e+00],\n          [-1.6475e+00, -1.7938e+00,  9.2559e-02,  ..., -3.8479e-02,\n           -1.5146e-01, -6.6211e+00],\n          [-1.8923e+00, -2.7682e+00,  4.9398e-01,  ..., -9.5186e-01,\n           -5.1755e-01, -7.0788e+00]],\n\n         [[-5.3628e-02,  8.0139e-02, -8.0991e-02,  ..., -1.3485e+00,\n           -1.5041e+00,  6.0794e-01],\n          [-1.0587e+00,  1.2573e+00, -1.7250e+00,  ...,  2.2708e-01,\n            5.9032e-01,  5.7291e-01],\n          [-2.1097e-01,  4.3254e-01, -1.0558e+00,  ...,  1.4978e+00,\n            1.1671e+00,  1.4344e+00],\n          ...,\n          [ 1.2665e+00, -7.3746e-01, -6.9133e-01,  ...,  5.8056e+00,\n            1.6770e+00,  1.7636e+00],\n          [-6.8971e-01, -3.6889e-01,  6.9248e-01,  ...,  6.6919e+00,\n            4.2297e+00,  1.0080e+00],\n          [-4.3376e-01, -1.0968e+00,  5.8477e-01,  ...,  5.3802e+00,\n            6.0880e-01,  1.7648e+00]],\n\n         [[-5.5008e-05, -5.0495e-02, -5.0035e-02,  ..., -3.3727e-01,\n            2.7300e-01, -4.0619e-01],\n          [ 6.1092e-01, -4.9943e-01, -5.3615e-01,  ...,  4.8645e-01,\n           -1.3054e+00,  7.6184e-01],\n          [ 8.0350e-01, -2.8751e-01, -1.0037e+00,  ...,  5.0757e+00,\n           -3.8508e+00,  2.2675e+00],\n          ...,\n          [-3.5077e-01,  3.9110e-01, -7.7770e-02,  ...,  6.1296e+00,\n           -2.7130e+00,  1.4375e+00],\n          [-2.9203e-01,  4.3968e-01, -1.1092e-01,  ...,  5.5444e+00,\n           -1.5172e+00,  3.0202e+00],\n          [-2.7358e-02, -3.0619e-02,  3.5029e-01,  ...,  5.5981e+00,\n           -5.3268e-01,  3.4868e+00]],\n\n         ...,\n\n         [[ 1.0437e-01, -3.3548e-02,  1.2506e-01,  ...,  1.6803e+00,\n           -1.1270e+00, -1.2944e-01],\n          [ 1.7417e+00, -2.8875e-01,  2.3643e+00,  ..., -2.9597e+00,\n           -1.9080e+00,  1.2661e+00],\n          [ 2.9395e-02,  7.7660e-01,  4.5813e-01,  ..., -5.1198e+00,\n            2.3659e-02, -2.0262e-01],\n          ...,\n          [-1.4781e+00,  1.3819e+00, -1.3117e+00,  ..., -3.8288e+00,\n           -1.2726e+00, -1.0869e+00],\n          [ 3.4382e-01,  7.7357e-01, -2.1937e+00,  ..., -3.9219e+00,\n            3.8463e-01, -1.7025e+00],\n          [ 5.3140e-01, -1.3983e-01, -1.8373e+00,  ..., -4.2518e+00,\n           -1.2880e-02, -1.2317e+00]],\n\n         [[-6.8644e-02, -8.1956e-02, -4.0383e-02,  ...,  8.9328e-01,\n            1.8409e+00,  1.4446e+00],\n          [-1.1454e+00, -1.9503e+00, -9.7478e-01,  ...,  3.1692e-01,\n           -3.3628e+00, -1.6152e-01],\n          [ 4.7400e-01, -8.7426e-01, -1.1007e+00,  ...,  3.3010e-01,\n           -3.5354e+00, -2.1006e-01],\n          ...,\n          [ 1.1901e+00,  1.3551e+00, -4.0182e-01,  ..., -9.0979e-02,\n           -3.8868e+00,  6.3973e-01],\n          [ 1.3428e-01,  7.1527e-01, -3.9368e-03,  ...,  7.7908e-02,\n           -3.9934e+00,  3.0507e-01],\n          [-1.0753e+00,  9.4585e-01,  1.7405e-01,  ...,  1.5428e-01,\n           -4.0549e+00, -2.9338e-01]],\n\n         [[-4.6730e-02,  3.6169e-02,  1.8060e-01,  ..., -1.3124e+00,\n            3.0565e-01, -9.7563e-01],\n          [-4.1502e+00,  1.2285e+00,  3.0599e+00,  ..., -1.2249e+00,\n            1.0287e+00, -3.6803e-01],\n          [-1.2745e+00,  9.7856e-01,  1.4054e+00,  ..., -1.7665e+00,\n            7.9501e-01, -9.4531e-01],\n          ...,\n          [ 2.1988e+00,  2.1710e-01, -1.2996e+00,  ..., -7.9220e-01,\n           -2.9255e-01, -7.1018e-01],\n          [ 1.0463e-01, -9.0516e-01, -6.1460e-01,  ..., -1.5183e+00,\n           -2.8626e-01, -1.4258e-01],\n          [ 3.0811e-01, -2.4144e+00, -2.5698e+00,  ...,  7.1495e-01,\n           -7.5689e-01,  3.9869e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5741e-02,  1.2704e-02,  1.0975e-02,  ..., -1.9629e-02,\n           -4.1710e-02,  1.3925e-02],\n          [ 1.0290e-01, -8.6253e-02, -7.6577e-02,  ..., -3.2430e-02,\n            4.8261e-01, -1.7361e-01],\n          [ 7.6148e-02,  1.4310e-01, -2.5367e-01,  ..., -1.9399e-01,\n            5.5266e-01, -2.5191e-01],\n          ...,\n          [-1.1625e-01,  1.5059e-01, -1.2364e-02,  ...,  3.2351e-02,\n            5.4142e-01,  1.3567e-01],\n          [-3.2112e-01,  1.6964e-01, -1.5878e-02,  ..., -8.1791e-02,\n            4.7819e-01,  5.1879e-02],\n          [ 1.2166e-01, -2.2661e-02,  1.4029e-01,  ...,  2.2474e-01,\n            6.1302e-01,  7.6070e-02]],\n\n         [[-7.9166e-04,  2.3437e-03,  8.0485e-03,  ...,  7.7399e-03,\n            9.5045e-03,  4.6631e-03],\n          [ 4.2380e-02, -8.7425e-02, -1.3761e-01,  ...,  7.3599e-02,\n           -2.6021e-01, -2.5217e-01],\n          [-1.7206e-01,  1.6248e-01, -2.3086e-01,  ..., -3.4205e-01,\n           -2.5810e-01, -3.5532e-01],\n          ...,\n          [-9.6249e-02,  2.2249e-01, -6.6095e-01,  ..., -2.5532e-01,\n           -2.2615e-01, -2.3028e-01],\n          [ 9.3356e-02,  2.3495e-01, -5.7557e-01,  ..., -4.1006e-01,\n           -3.8410e-01,  2.0350e-02],\n          [ 1.2238e-01,  2.7590e-01, -1.5102e-01,  ..., -1.0727e-01,\n           -3.4931e-01, -3.5034e-01]],\n\n         [[ 3.9210e-02,  1.9393e-04,  3.3262e-03,  ...,  8.1781e-03,\n           -9.4381e-03, -7.9167e-03],\n          [-9.4547e-01,  1.1973e-01, -2.8968e-01,  ..., -1.8359e-01,\n            1.4042e-01,  8.3514e-03],\n          [-8.3745e-01,  2.7255e-01, -3.6366e-01,  ...,  3.1175e-01,\n           -9.0469e-01,  3.0255e-02],\n          ...,\n          [-8.4997e-01,  2.1602e-01,  7.3421e-02,  ...,  1.1707e-02,\n           -1.1145e-01, -2.7753e-02],\n          [-8.5703e-01, -2.1940e-02, -5.1277e-02,  ..., -1.4274e-01,\n            1.5582e-01,  1.0579e-01],\n          [-8.0754e-01, -1.9184e-02,  1.6539e-01,  ...,  6.7397e-02,\n            3.5809e-02, -1.3506e-01]],\n\n         ...,\n\n         [[ 1.9551e-02,  1.0297e-02,  1.5773e-03,  ..., -1.1819e-02,\n           -1.2796e-03, -1.1804e-02],\n          [ 1.8765e-01, -2.8786e-02, -2.9936e-02,  ..., -3.0006e-02,\n           -2.2696e-01, -1.0051e-01],\n          [-6.9160e-02,  2.4853e-01, -1.6608e-01,  ..., -4.4749e-01,\n            2.3618e-02,  1.7608e-01],\n          ...,\n          [ 1.6023e-01,  2.2103e-01, -3.2907e-01,  ..., -1.1427e-01,\n            1.2678e-01,  1.8312e-02],\n          [-2.5502e-02,  4.3437e-02, -8.4752e-02,  ..., -1.3916e-01,\n            1.1505e-02, -9.1755e-02],\n          [ 9.1606e-02,  1.1718e-01, -2.1248e-01,  ...,  1.8931e-01,\n            1.1512e-01, -1.6510e-01]],\n\n         [[ 2.2793e-02,  2.3541e-02, -2.8061e-02,  ..., -1.2769e-01,\n           -7.2716e-03,  6.0188e-04],\n          [-3.0313e-01,  2.7208e-02, -1.7947e-01,  ...,  3.1877e-01,\n            8.2309e-03, -1.9664e-01],\n          [-1.6683e-01,  2.7815e-01,  1.8993e-02,  ...,  1.8573e-01,\n            4.8218e-02, -3.9488e-02],\n          ...,\n          [-2.5782e-02, -2.7187e-01,  9.3562e-02,  ...,  4.0017e-02,\n           -1.2506e-01, -2.9782e-02],\n          [ 1.4955e-01,  1.9041e-01,  2.7136e-01,  ...,  4.8256e-02,\n           -3.2006e-01,  1.2765e-01],\n          [-7.3341e-02, -8.2576e-02,  1.6007e-02,  ...,  4.9811e-02,\n           -5.3329e-02, -6.1567e-02]],\n\n         [[-2.4003e-02,  1.4413e-03, -9.7189e-03,  ..., -1.0038e-02,\n           -4.0050e-03, -7.8792e-05],\n          [ 4.3207e-02, -1.1919e-01, -5.4457e-01,  ..., -1.4651e-01,\n            1.0372e-01,  2.7977e-01],\n          [-1.0743e-01, -2.7909e-01, -5.8523e-01,  ..., -1.0493e-01,\n           -1.3021e-01, -2.6057e-01],\n          ...,\n          [ 2.4492e-01,  4.2667e-01, -2.2949e-01,  ...,  1.7911e-01,\n            1.6487e-01,  9.0538e-02],\n          [-1.0448e-01,  1.6701e-01, -4.7813e-01,  ..., -1.7286e-02,\n            1.5522e-01,  9.7537e-02],\n          [-1.7028e-02,  8.0929e-01,  9.4702e-01,  ..., -3.3726e-02,\n           -1.0608e-01,  4.0858e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.7858e-02,  1.7457e-01, -2.2763e-02,  ..., -1.1239e+00,\n            9.3052e-01,  9.3458e-01],\n          [ 4.1589e+00,  3.1935e+00, -2.4707e+00,  ..., -1.0910e+00,\n            2.0046e+00,  2.3571e+00],\n          [ 4.1812e+00,  7.1243e-01, -4.3240e+00,  ..., -5.7747e-01,\n            1.6257e+00,  1.6119e+00],\n          ...,\n          [-2.7209e+00, -4.7509e+00, -4.3103e+00,  ..., -2.4002e+00,\n            1.1299e-01,  1.4929e+00],\n          [-3.3909e+00, -3.7879e+00, -2.5160e+00,  ..., -1.4558e+00,\n            6.1843e-01,  2.3745e+00],\n          [-2.5503e+00, -2.7173e+00, -1.4323e+00,  ..., -1.3430e+00,\n           -6.9395e-01,  1.7852e+00]],\n\n         [[ 5.7449e-02, -3.3637e-03, -1.6319e-02,  ..., -4.2206e-01,\n            1.2233e+00,  3.4318e-01],\n          [ 3.2573e+00, -1.4257e+00, -1.1359e+00,  ..., -6.8138e-01,\n            1.1662e+00, -1.3537e-01],\n          [-4.8836e-01, -1.0289e+00, -1.0035e-03,  ..., -5.0336e-01,\n            4.7493e-01,  2.7646e-01],\n          ...,\n          [-1.5530e+00, -1.6113e+00, -3.6268e-01,  ..., -4.6333e-01,\n           -7.9367e-01,  8.5537e-01],\n          [ 9.4621e-01,  1.2660e+00, -6.1222e-01,  ...,  4.1511e-01,\n           -2.5061e-01,  1.0104e+00],\n          [ 2.2199e+00,  2.2013e+00, -1.1727e+00,  ...,  5.7435e-01,\n           -7.1837e-01,  1.9721e-01]],\n\n         [[ 8.8649e-05, -1.0856e-02,  6.7287e-03,  ..., -1.1209e+00,\n           -8.4819e-01,  1.1060e+00],\n          [ 3.0503e-02,  3.2406e-01,  3.5938e-01,  ...,  9.3989e+00,\n            4.5584e+00, -9.7403e+00],\n          [ 6.1180e-03, -4.0424e-02,  1.3981e-01,  ...,  8.2844e+00,\n            9.3161e-01, -9.1253e+00],\n          ...,\n          [-3.2833e-01, -3.9624e-03,  3.8422e-02,  ...,  1.5234e+00,\n            3.0882e+00, -8.6050e+00],\n          [-7.1255e-01, -1.5154e-01, -4.9607e-02,  ...,  5.0184e-01,\n            3.0396e+00, -6.9793e+00],\n          [-4.2383e-02, -2.2849e-01, -5.1579e-01,  ..., -1.3567e+00,\n            8.6881e+00, -4.9598e+00]],\n\n         ...,\n\n         [[ 4.7751e-03,  2.4765e-02,  6.5427e-02,  ...,  1.1498e+00,\n           -5.5660e-01, -2.3739e+00],\n          [-2.7048e-01,  1.9240e+00,  9.2797e-01,  ...,  6.1590e-01,\n           -5.3501e-02,  3.5105e+00],\n          [-1.4208e+00,  7.9953e-01,  7.2649e-01,  ...,  1.4090e+00,\n           -1.0943e-01,  4.6916e+00],\n          ...,\n          [ 1.4458e+00, -1.2390e+00, -1.6505e-01,  ...,  1.6354e+00,\n           -5.8472e-01,  4.2420e+00],\n          [ 1.2140e+00, -1.6530e+00, -7.6998e-01,  ...,  1.3969e+00,\n           -6.5632e-01,  4.7695e+00],\n          [ 4.2530e-01, -1.0632e+00, -1.5249e+00,  ...,  9.5883e-01,\n           -1.4171e+00,  4.5755e+00]],\n\n         [[ 4.9551e-03,  4.8713e-03, -6.7961e-02,  ..., -9.5603e-01,\n           -2.0773e+00, -1.4528e+00],\n          [-2.4987e+00,  4.8178e-01, -8.5864e-01,  ..., -1.1032e+00,\n           -4.5302e-01, -2.4382e+00],\n          [-1.3630e+00,  1.4044e+00, -7.8732e-01,  ..., -3.2960e-01,\n           -6.6378e-01, -1.6802e+00],\n          ...,\n          [ 1.0112e+00,  6.0710e-01,  3.2059e-01,  ..., -1.3903e-01,\n           -1.1601e+00, -1.1141e+00],\n          [ 5.1350e-01, -1.1856e+00, -1.4531e-01,  ...,  3.1771e-01,\n           -4.8929e-02, -3.4188e-01],\n          [ 5.9443e-01, -1.3259e+00,  2.0242e-02,  ..., -1.4919e+00,\n           -5.6953e-01, -7.6237e-01]],\n\n         [[-7.9350e-02, -5.4308e-02, -9.5349e-03,  ...,  9.4083e-01,\n            2.5170e+00, -3.2786e+00],\n          [-2.1497e+00, -1.7756e+00,  8.0995e-01,  ..., -3.3403e+00,\n           -1.8374e-01,  4.8019e+00],\n          [ 1.6129e+00,  5.4331e-01,  6.1357e-01,  ..., -3.7763e+00,\n            1.2750e+00,  7.3814e+00],\n          ...,\n          [ 9.7731e-01,  1.4124e+00,  3.8733e-01,  ..., -2.2185e+00,\n            1.4152e+00,  8.3236e+00],\n          [-1.5950e+00, -1.5879e-01,  4.6176e-01,  ..., -1.2807e+00,\n            1.1276e+00,  8.4439e+00],\n          [-7.3651e-01, -4.5895e-03, -7.2717e-02,  ..., -3.6994e+00,\n           -1.4185e+00,  6.0466e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 9.3590e-04,  2.8529e-02,  5.6978e-03,  ...,  1.2661e-02,\n            8.9264e-03, -8.7175e-03],\n          [ 3.0078e-01,  2.6767e-01,  1.9271e-01,  ...,  1.4708e-01,\n            7.1249e-02,  1.2822e-01],\n          [-8.4698e-02,  5.7607e-01, -3.0367e-01,  ...,  1.0784e-01,\n           -2.7994e-02,  4.0537e-01],\n          ...,\n          [ 1.2736e-01, -7.2686e-02,  3.0685e-01,  ..., -1.6118e-02,\n           -5.6757e-01, -3.7781e-01],\n          [ 1.8854e-01,  4.3236e-01,  2.4099e-01,  ..., -4.3005e-02,\n           -9.0187e-01,  1.1908e-01],\n          [ 1.0048e-01, -7.0765e-03,  4.2359e-02,  ..., -6.1907e-01,\n           -1.0317e-02,  2.9996e-01]],\n\n         [[ 4.2125e-02, -1.4949e-02,  6.8945e-03,  ...,  6.5633e-03,\n           -1.9918e-03, -1.2263e-02],\n          [-8.6123e-01,  2.1418e-02, -4.6317e-02,  ..., -4.7484e-01,\n           -2.6559e-01, -2.0109e-01],\n          [-3.3328e-01,  3.3497e-02, -1.6863e-01,  ..., -2.9473e-01,\n           -3.0380e-01,  8.4597e-02],\n          ...,\n          [-3.1169e-01,  1.6245e-01, -5.2212e-02,  ..., -2.4400e-01,\n           -3.6954e-01, -2.5663e-02],\n          [-9.2145e-02,  4.4051e-01,  4.6001e-02,  ..., -1.2436e-03,\n           -9.6444e-02, -4.9024e-01],\n          [ 2.0358e-01,  1.3464e-01, -2.1163e-01,  ..., -1.3686e-01,\n           -1.3435e-01,  1.6847e-01]],\n\n         [[ 4.8812e-03,  6.3428e-03, -1.5907e-02,  ..., -7.9164e-03,\n            1.9445e-03,  9.0020e-03],\n          [-9.8095e-02, -8.6381e-02, -3.6844e-01,  ..., -5.7589e-02,\n            2.1959e-01, -7.0982e-02],\n          [ 3.0170e-01, -1.0434e-01, -3.8194e-03,  ...,  6.2394e-02,\n            1.2737e-01, -1.1202e-01],\n          ...,\n          [ 2.2344e-01,  9.3603e-03, -2.2775e-01,  ..., -7.9702e-03,\n            2.8950e-02,  1.9388e-01],\n          [ 3.4498e-01,  8.0839e-02, -7.5499e-02,  ...,  5.3918e-02,\n           -1.1760e-01,  1.5815e-01],\n          [ 1.9497e-01, -1.1156e-01, -2.7446e-01,  ..., -6.2212e-02,\n            1.6979e-01,  5.9713e-02]],\n\n         ...,\n\n         [[-2.1932e-03,  6.7271e-03,  1.6275e-02,  ..., -1.2165e-02,\n            6.1163e-03, -3.1671e-02],\n          [ 9.7741e-02,  9.4036e-02,  2.4281e-02,  ...,  1.0980e-01,\n           -1.0405e-01,  1.0067e-01],\n          [-3.2719e-02,  5.3270e-02, -6.5072e-02,  ...,  1.3246e-01,\n           -4.2380e-01,  1.8715e-01],\n          ...,\n          [ 4.3528e-01,  1.0122e-01,  1.3358e-01,  ...,  4.0793e-01,\n           -2.2985e-01, -1.7101e-01],\n          [ 3.1636e-02,  1.8576e-02,  2.6358e-01,  ...,  3.2357e-01,\n           -3.8288e-01, -1.5802e-01],\n          [ 1.3856e-01, -7.9769e-02,  8.5927e-02,  ...,  4.6440e-01,\n           -3.4063e-02, -3.4477e-01]],\n\n         [[ 1.3362e-02,  9.6465e-03, -3.3372e-03,  ..., -2.1057e-02,\n           -1.5878e-02,  1.7561e-02],\n          [-1.7414e-01,  1.1742e-01,  1.4036e-01,  ...,  2.4521e-01,\n            9.3921e-02,  1.9606e-01],\n          [ 8.1271e-02,  4.4230e-01,  2.0403e-02,  ..., -1.2735e-01,\n           -1.8625e-01,  1.1084e-01],\n          ...,\n          [-2.4271e-01,  1.0439e-01, -1.1056e-01,  ...,  3.6153e-01,\n           -3.2894e-01,  2.8341e-02],\n          [ 5.9896e-02,  1.3758e-01, -2.8612e-01,  ...,  2.2576e-01,\n           -4.9372e-01,  5.5741e-02],\n          [-3.2227e-01,  4.0608e-01, -5.1128e-01,  ...,  4.2920e-01,\n           -1.1371e-01,  7.4510e-01]],\n\n         [[ 1.2613e-02,  4.4390e-03, -2.8977e-02,  ...,  1.5352e-02,\n           -3.1253e-03,  1.0260e-02],\n          [ 2.6141e-02,  4.4875e-01,  4.2975e-01,  ...,  3.2282e-01,\n           -2.1937e-01, -2.1450e-01],\n          [-1.0085e-01,  1.5843e-01,  9.9052e-01,  ...,  1.2787e-01,\n           -2.0459e-01, -5.0992e-01],\n          ...,\n          [-1.1244e-01,  6.0204e-02,  4.3369e-02,  ...,  1.0944e-01,\n            3.6751e-03, -9.8643e-02],\n          [-6.7898e-02, -1.1371e-01,  2.6787e-01,  ..., -1.7534e-01,\n            2.3881e-01, -4.7611e-02],\n          [-1.0737e-01, -1.8074e-01,  1.1761e-01,  ...,  5.3748e-03,\n           -3.4666e-01, -4.4969e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.2986e-02, -5.6758e-02,  4.2181e-02,  ...,  6.8194e-01,\n           -2.4365e-01, -2.6245e-01],\n          [-2.0277e+00,  3.6989e-01,  1.0358e+00,  ...,  1.5145e+00,\n           -2.0044e+00,  8.4235e-01],\n          [-1.3090e+00,  8.2125e-01,  5.5625e-01,  ..., -1.8567e-01,\n           -1.1091e+00,  5.4527e-01],\n          ...,\n          [ 1.5006e+00,  4.5542e-01,  4.3176e-01,  ...,  7.1436e-01,\n           -8.7396e-01, -4.8069e-01],\n          [ 1.0901e+00,  3.9606e-01, -4.7152e-01,  ...,  3.2983e-01,\n           -5.0065e-01,  5.0166e-01],\n          [ 1.2840e-01, -8.3067e-01, -8.1293e-01,  ..., -4.0872e-01,\n           -1.9316e+00, -1.9433e+00]],\n\n         [[-6.8734e-02,  8.0266e-02, -9.0677e-03,  ...,  7.3990e-01,\n            8.9014e-02, -5.7725e-01],\n          [-2.7317e+00,  2.1390e+00,  1.1494e+00,  ..., -3.4212e+00,\n           -3.1490e-01, -1.6709e+00],\n          [-2.9074e-01, -5.3336e-02, -5.1275e-01,  ..., -2.8581e+00,\n            4.7676e-02, -2.8024e-01],\n          ...,\n          [ 1.6968e+00, -1.9554e+00,  1.7006e+00,  ..., -4.3257e+00,\n           -2.2115e-01,  6.2416e-01],\n          [-9.2230e-01, -9.7474e-01,  3.9166e-01,  ..., -4.3167e+00,\n           -2.2765e-01, -4.8433e-01],\n          [-6.0461e-01, -1.0264e+00,  3.7290e-01,  ..., -3.7289e+00,\n           -6.2502e-02,  5.3949e-01]],\n\n         [[ 1.7127e-03, -3.6500e-02, -9.9987e-03,  ...,  3.8029e-01,\n            7.0632e-01, -9.5575e-02],\n          [-1.7413e-01, -2.3737e-01, -9.7911e-01,  ...,  1.6357e+00,\n           -9.5725e-02,  5.3678e-01],\n          [-8.8745e-01,  3.5649e-02, -1.8132e-01,  ...,  3.8635e-01,\n           -3.3454e-01,  3.4820e-01],\n          ...,\n          [ 4.0956e-01,  9.8702e-01, -1.6676e+00,  ..., -6.2349e-02,\n           -3.2785e-01,  3.2371e-01],\n          [ 1.3126e+00, -1.0274e-01, -5.2804e-01,  ...,  3.1959e-01,\n           -1.4227e-01,  1.1870e-01],\n          [ 7.6431e-01,  1.3596e-01, -2.6380e-01,  ...,  5.5026e-02,\n            4.8367e-02,  1.2360e+00]],\n\n         ...,\n\n         [[ 4.9501e-02, -3.4285e-02,  4.6124e-02,  ..., -3.8699e-01,\n            1.4646e+00, -1.7441e-01],\n          [ 1.2642e+00, -1.4507e+00,  1.6048e+00,  ...,  2.4330e+00,\n           -4.2889e+00, -5.5782e-01],\n          [-1.4796e-01,  3.3218e-01,  3.1144e-01,  ...,  2.9231e+00,\n           -3.6052e+00, -1.9999e+00],\n          ...,\n          [-1.2134e+00,  1.0892e+00, -3.7682e-01,  ...,  1.4461e+00,\n           -4.1793e+00, -2.2762e+00],\n          [ 7.4900e-01,  1.1938e-01, -4.1763e-01,  ...,  1.6771e+00,\n           -4.0144e+00, -2.6294e+00],\n          [ 4.3123e-01,  6.7885e-01, -1.1292e+00,  ...,  1.6564e+00,\n           -2.8780e+00, -1.3151e+00]],\n\n         [[-4.2510e-02,  3.7889e-02,  6.0747e-02,  ..., -5.7372e-02,\n           -1.2709e+00, -7.8387e-01],\n          [-5.0359e-01,  8.3469e-01,  1.2501e+00,  ...,  2.3276e-02,\n           -1.6576e+00, -4.7826e+00],\n          [ 3.5352e-01, -6.4913e-02,  5.3792e-01,  ..., -1.1122e-01,\n           -1.0168e+00, -5.8695e+00],\n          ...,\n          [ 5.3220e-02, -7.1725e-01,  4.9906e-01,  ..., -9.5418e-01,\n           -3.0370e+00, -2.7932e+00],\n          [-4.3403e-03, -3.6534e-01, -2.3779e-02,  ..., -1.4896e+00,\n           -2.6524e+00, -4.4556e+00],\n          [-3.7900e-01, -5.6974e-01, -3.5380e-02,  ..., -2.3949e+00,\n           -1.5284e+00, -4.1054e+00]],\n\n         [[ 5.9208e-02, -7.4041e-02, -3.8141e-02,  ...,  4.1372e-01,\n            1.9973e+00,  1.8169e+00],\n          [-3.5135e-01, -1.5177e+00, -1.0858e+00,  ...,  2.8560e-01,\n           -6.1455e-01, -2.1975e+00],\n          [-1.7711e+00, -5.5017e-01, -8.7693e-01,  ...,  1.6907e-01,\n           -5.6715e-02, -2.5102e+00],\n          ...,\n          [ 1.6425e-01,  2.4681e+00, -1.4838e+00,  ...,  4.7841e-01,\n           -1.6130e+00, -4.9014e+00],\n          [ 1.0952e+00,  1.1468e+00,  6.9954e-01,  ..., -7.0833e-01,\n           -1.6066e+00, -4.8652e+00],\n          [ 1.0568e+00,  6.1719e-01,  4.0996e-01,  ..., -2.6305e-02,\n           -3.2821e+00, -4.5679e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.5075e-03, -4.3818e-02,  3.2011e-02,  ...,  3.3098e-02,\n           -1.9107e-02, -3.6052e-02],\n          [-2.1269e-03,  1.6216e-01, -1.5261e-01,  ..., -2.4489e-01,\n           -1.2112e-01,  1.9746e-01],\n          [-2.4059e-01,  3.1051e-01,  5.7210e-02,  ..., -2.6637e-01,\n           -6.1936e-02,  2.2837e-01],\n          ...,\n          [-1.1204e-01,  2.9352e-01,  2.7793e-01,  ..., -1.0572e-01,\n            2.2897e-02,  5.1392e-01],\n          [-1.3869e-01,  1.0130e-01, -6.4869e-02,  ...,  4.4783e-02,\n            4.8176e-03,  2.2737e-01],\n          [-4.1347e-03,  2.8174e-02, -1.6069e-01,  ...,  2.7191e-01,\n            2.8911e-04,  4.5144e-01]],\n\n         [[-1.2707e-02,  7.7928e-03, -2.4912e-02,  ...,  4.3719e-04,\n            1.3313e-02,  3.7711e-03],\n          [ 1.4313e-01, -3.8084e-01,  4.3599e-01,  ..., -1.1904e-01,\n            2.5198e-01,  2.1550e-01],\n          [-5.0879e-03, -3.7911e-02,  2.5485e-01,  ..., -2.7624e-01,\n           -1.2211e-01,  2.4520e-01],\n          ...,\n          [-1.3643e-01, -3.5685e-01,  4.4867e-01,  ...,  1.2322e-01,\n            6.0183e-01,  2.4804e-01],\n          [-2.0965e-01, -3.1830e-01,  3.1960e-01,  ...,  3.6249e-02,\n            3.7764e-01,  6.0210e-03],\n          [-8.3376e-02, -2.1156e-01,  1.5272e-01,  ..., -6.9034e-02,\n            2.5154e-01, -1.3800e-01]],\n\n         [[ 4.9766e-03,  1.6422e-02, -9.3895e-04,  ..., -1.0992e-02,\n            2.7222e-02, -3.3118e-03],\n          [ 7.0952e-02,  1.4994e-01, -1.4358e-01,  ...,  1.3321e-01,\n           -9.9199e-01, -5.0039e-02],\n          [ 2.2315e-03,  1.3689e-02,  5.4795e-01,  ...,  4.3925e-01,\n           -3.8316e-01,  1.2275e-01],\n          ...,\n          [ 1.4568e-01,  3.6804e-01,  1.8879e-01,  ...,  6.3231e-01,\n           -6.8411e-01,  8.1384e-02],\n          [ 1.6347e-01,  5.0384e-02,  1.6249e-01,  ...,  2.5391e-01,\n            4.1089e-02,  8.6733e-02],\n          [ 1.0754e-01,  2.0364e-03,  3.1858e-02,  ...,  1.2100e-01,\n           -6.8371e-01, -2.3201e-02]],\n\n         ...,\n\n         [[-6.8232e-03, -3.4541e-03,  3.0257e-03,  ..., -1.1610e-02,\n            1.4196e-02, -4.2651e-03],\n          [-1.9102e-01, -9.6374e-02, -3.8347e-01,  ..., -6.8594e-02,\n            1.5969e-01, -2.9198e-01],\n          [-1.1796e-01, -3.7198e-01, -1.7730e-01,  ...,  3.0197e-01,\n            4.4423e-01, -3.5287e-01],\n          ...,\n          [ 2.5382e-01,  1.8584e-02, -1.4926e-01,  ...,  1.1287e-01,\n            1.5608e-01,  1.8151e-01],\n          [ 5.1760e-02,  1.7294e-02, -1.8609e-01,  ..., -1.3589e-01,\n            6.0654e-02, -8.9075e-02],\n          [-3.4706e-01, -2.0907e-01, -1.7131e-01,  ..., -2.4358e-01,\n           -1.2957e-01, -2.4586e-01]],\n\n         [[ 1.8800e-02, -2.2288e-02, -1.8583e-02,  ...,  2.8067e-02,\n            2.0518e-02,  7.2470e-03],\n          [-2.2676e-01,  2.8546e-01,  1.0846e-01,  ...,  7.3832e-02,\n           -2.8026e-02,  2.3904e-06],\n          [-2.1163e-02,  7.5956e-01,  1.8756e-01,  ...,  3.1446e-01,\n            5.6013e-01, -2.0784e-01],\n          ...,\n          [ 1.9506e-01,  1.2693e-01, -2.0888e-01,  ...,  1.7235e-01,\n            6.0297e-02, -8.2514e-02],\n          [ 4.7009e-01,  4.0924e-01, -2.4973e-01,  ..., -1.2039e-01,\n            1.1012e-01,  5.1201e-02],\n          [ 2.7584e-01,  3.3358e-01, -1.8232e-01,  ...,  2.0032e-01,\n            2.2118e-01,  5.0746e-02]],\n\n         [[ 1.1655e-02, -1.1191e-02, -2.4126e-02,  ...,  1.7493e-02,\n           -1.3610e-02, -2.2237e-02],\n          [-1.6543e-01, -2.5003e-01,  8.6125e-02,  ...,  5.6394e-02,\n            3.4471e-02,  9.6381e-02],\n          [-1.6387e-01, -1.1931e-01,  5.5219e-01,  ...,  5.0510e-01,\n           -1.5565e-01,  8.6988e-02],\n          ...,\n          [-3.0309e-01, -2.2586e-02,  1.5457e-01,  ...,  1.6099e-01,\n           -5.5603e-01, -2.0773e-01],\n          [-4.4897e-01, -2.6678e-02, -3.1037e-01,  ...,  1.3030e-01,\n           -9.0136e-01,  5.7729e-02],\n          [ 2.0204e-01,  3.6599e-02,  1.9856e-01,  ...,  7.2303e-02,\n           -2.2883e-01, -1.2853e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4815e-02,  3.8855e-03, -6.5161e-02,  ...,  3.6477e-01,\n           -7.6282e-01,  3.9310e-01],\n          [ 1.7472e-01,  8.0178e-01, -5.7563e-01,  ..., -1.6855e+00,\n           -3.8904e+00, -1.9497e+00],\n          [-9.5987e-01,  5.8723e-01, -7.1325e-01,  ..., -1.8820e+00,\n           -1.9519e+00, -3.5142e+00],\n          ...,\n          [ 2.9018e-01, -2.2332e-01, -2.2718e-01,  ..., -1.3363e+00,\n           -1.4313e+00, -2.5188e+00],\n          [ 3.7244e-01, -1.9810e-01, -1.9881e-01,  ..., -6.5588e-01,\n           -1.1756e+00, -4.1595e+00],\n          [ 6.5614e-01, -1.0736e+00,  8.7181e-01,  ..., -9.0678e-01,\n           -7.1075e-01, -3.9002e+00]],\n\n         [[-5.2089e-03,  1.4205e-02, -1.2689e-03,  ..., -5.0026e-01,\n           -5.8205e-01,  5.3615e-01],\n          [-1.5271e+00,  8.8098e-01, -5.7725e-02,  ...,  4.1800e+00,\n           -1.7192e+00,  1.1539e+00],\n          [ 3.9878e-01, -2.5377e-01, -2.3752e-01,  ...,  4.6732e+00,\n           -8.8458e-01, -5.0242e-01],\n          ...,\n          [ 7.0503e-01, -1.1254e+00,  2.2898e-01,  ...,  3.6139e+00,\n           -7.5675e-02,  9.0863e-01],\n          [-3.5906e-01, -1.2928e-01,  1.5099e-01,  ...,  2.8251e+00,\n           -3.7178e-01, -2.4456e+00],\n          [-7.6586e-01, -1.8641e-01,  8.9007e-01,  ...,  7.6406e-01,\n           -8.1659e-01, -3.2165e-01]],\n\n         [[-4.6029e-02, -7.7140e-02,  7.5468e-02,  ..., -7.1893e-02,\n            1.2510e+00, -1.8972e-01],\n          [-1.7925e+00, -1.6732e+00,  2.4657e+00,  ..., -3.3280e-01,\n            1.4177e+00, -1.9065e-01],\n          [-1.3771e-01, -1.2481e-01,  1.1394e+00,  ...,  2.8269e-03,\n            1.3836e+00,  4.2271e-01],\n          ...,\n          [ 1.1215e+00,  1.7611e+00, -2.1272e+00,  ...,  4.1990e-02,\n            1.2248e+00,  4.4655e-01],\n          [-4.5170e-03,  1.8393e+00, -5.3411e-01,  ..., -2.2830e-05,\n            5.9196e-01,  8.4567e-01],\n          [-1.4081e+00,  2.5823e-01, -2.0935e+00,  ...,  8.9313e-02,\n           -9.1095e-03, -7.3424e-01]],\n\n         ...,\n\n         [[ 3.8313e-02,  3.1387e-02, -4.9776e-02,  ...,  1.4955e+00,\n            1.4901e+00,  3.7326e-01],\n          [ 4.7177e-01, -1.9497e-01, -9.5550e-01,  ..., -1.4786e+00,\n           -5.3615e+00, -4.5147e+00],\n          [-2.1454e-01, -3.4196e-01,  1.6850e-01,  ..., -2.6179e+00,\n           -6.5332e+00, -5.7047e+00],\n          ...,\n          [-5.5921e-01,  7.6688e-01,  5.1687e-01,  ..., -2.3091e+00,\n           -6.1471e+00, -5.2471e+00],\n          [ 5.3142e-01, -1.4104e-02,  2.5696e-01,  ..., -2.2951e+00,\n           -6.7571e+00, -5.0329e+00],\n          [ 6.1794e-01, -4.1871e-01,  8.8736e-01,  ..., -9.6804e-01,\n           -6.1409e+00, -5.1368e+00]],\n\n         [[ 1.6163e-01,  7.1201e-02,  7.7224e-02,  ..., -9.5703e-01,\n            5.5020e-01,  6.5097e-01],\n          [ 6.2034e-01,  1.2192e+00,  2.8485e+00,  ..., -4.1860e+00,\n            2.0176e+00, -1.7881e+00],\n          [-3.9736e+00, -1.6081e+00,  2.0517e+00,  ..., -4.9265e+00,\n            2.1788e+00,  1.6002e-01],\n          ...,\n          [-1.7394e+00, -3.7851e+00, -5.4124e-01,  ..., -4.0162e+00,\n            2.0071e+00, -5.5163e-01],\n          [ 3.5902e+00, -2.1066e+00, -1.7955e+00,  ..., -2.4751e+00,\n            3.4249e+00, -7.4834e-03],\n          [ 5.7654e+00, -5.9770e-01, -2.7081e+00,  ..., -4.5666e+00,\n            3.4483e+00, -4.5146e-01]],\n\n         [[ 9.2309e-03, -1.3726e-02, -6.7525e-02,  ...,  5.8759e-01,\n            1.1254e+00,  8.7347e-01],\n          [ 6.7081e-01,  1.3083e+00, -2.8793e-01,  ..., -1.8503e+00,\n            1.0474e+00, -1.1659e+00],\n          [-1.9691e+00,  6.1067e-02, -4.9139e-02,  ..., -2.0803e+00,\n            1.0640e+00, -1.6873e+00],\n          ...,\n          [-1.9309e-01, -8.1366e-01, -1.0745e+00,  ..., -1.1412e+00,\n            1.2140e+00, -2.7790e+00],\n          [ 1.2697e+00, -4.0457e-01,  5.4169e-01,  ..., -2.0708e+00,\n           -3.0930e-01, -3.3942e+00],\n          [ 1.1935e+00, -3.6645e-01,  1.2198e+00,  ..., -2.5969e+00,\n            3.4379e-02, -4.6407e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3331e-02, -4.6928e-03, -7.3019e-03,  ..., -1.9747e-02,\n           -3.0523e-03,  2.9128e-02],\n          [ 1.2601e-01, -5.6824e-02,  1.2379e-01,  ...,  1.2374e-01,\n           -3.3303e-02, -3.7515e-01],\n          [ 1.9364e-01, -1.2125e-01,  2.8124e-01,  ..., -1.0768e-01,\n            6.3876e-02, -6.0966e-01],\n          ...,\n          [-4.2872e-02, -1.5920e-01,  3.1075e-01,  ...,  7.3740e-02,\n           -1.0305e-01, -3.5542e-01],\n          [ 9.1024e-02, -1.0297e-01,  4.4406e-01,  ...,  3.4327e-01,\n           -1.5741e-01, -6.0071e-01],\n          [ 1.5528e-01, -2.7940e-01,  3.7453e-01,  ..., -7.5135e-02,\n           -3.9484e-02, -1.8948e-01]],\n\n         [[-4.6705e-03,  2.1753e-03,  2.7855e-02,  ..., -1.3684e-03,\n            6.2462e-02,  2.0781e-03],\n          [-1.2567e-03, -1.0440e-01, -1.3493e-02,  ..., -1.8711e-01,\n           -9.1693e-02,  6.7549e-02],\n          [ 2.9302e-01,  2.4074e-01, -4.7760e-02,  ..., -3.8108e-01,\n            2.2722e-02,  2.2241e-01],\n          ...,\n          [ 6.7676e-02,  3.6794e-01,  2.8198e-01,  ..., -1.3390e-01,\n            5.1727e-01,  2.9591e-01],\n          [ 5.2915e-02,  3.9826e-01,  1.7824e-01,  ..., -8.1494e-03,\n            1.1654e-01,  4.1005e-01],\n          [ 6.6410e-02,  2.2054e-01,  2.0727e-01,  ...,  6.0972e-02,\n            2.6168e-01,  1.0454e-01]],\n\n         [[-6.5995e-03, -3.0940e-02,  3.0484e-02,  ...,  5.9859e-04,\n            1.1084e-02, -1.7138e-01],\n          [ 1.1578e-01,  2.1733e-03, -2.2893e-01,  ..., -1.7371e-01,\n           -2.2726e-03, -5.9387e-02],\n          [ 2.0877e-01,  2.2600e-01, -1.2645e-01,  ..., -3.3830e-01,\n            6.7676e-02,  1.6034e-01],\n          ...,\n          [ 2.2702e-01,  3.6602e-02, -2.1516e-01,  ..., -9.3808e-01,\n            2.2783e-01, -1.7914e-01],\n          [ 4.6318e-01,  1.4795e-02,  1.8363e-02,  ..., -5.6283e-01,\n            1.2641e-01,  4.9621e-01],\n          [-1.6959e-01, -2.2225e-03,  2.8095e-01,  ..., -4.9871e-01,\n            1.7634e-01,  7.5611e-01]],\n\n         ...,\n\n         [[ 4.8023e-03,  4.0914e-01, -1.4497e-02,  ...,  7.0290e-03,\n            1.2445e-02, -2.1458e-02],\n          [ 1.3051e-02, -9.3685e-01,  1.1798e-01,  ..., -1.4003e-01,\n           -1.2717e-01,  8.9570e-02],\n          [-1.6168e-01, -1.6029e+00,  2.3556e-02,  ...,  4.6295e-02,\n            6.0343e-02,  1.0125e-01],\n          ...,\n          [-1.6702e-01, -1.0712e+00,  2.1793e-01,  ...,  1.0436e-01,\n           -4.7951e-02, -1.9403e-01],\n          [-1.1241e-01, -1.2524e+00,  2.6625e-01,  ..., -2.1663e-02,\n           -7.1329e-02, -2.4839e-01],\n          [-1.3651e-01, -6.7578e-01, -2.5005e-02,  ...,  6.8521e-02,\n           -2.7406e-01, -2.5056e-01]],\n\n         [[-1.6400e-02, -9.9874e-03, -1.3735e-02,  ..., -2.2474e-02,\n            2.0281e-02,  2.2781e-01],\n          [ 4.7074e-01, -4.9741e-01, -3.5659e-01,  ...,  9.1268e-02,\n            1.1694e-01, -2.1805e-01],\n          [ 2.0684e-01,  4.0959e-01, -5.0544e-02,  ...,  9.8892e-02,\n            2.3886e-01,  1.7650e-01],\n          ...,\n          [ 2.1865e-01,  2.6039e-01, -1.8954e-02,  ..., -1.6628e-01,\n           -2.1735e-02, -5.7966e-01],\n          [ 1.3507e-01,  1.4209e-01,  4.9038e-02,  ...,  1.2472e-01,\n           -8.2507e-02,  2.5974e-01],\n          [ 2.4348e-01, -3.2005e-02, -2.9278e-01,  ..., -3.8628e-02,\n            1.9323e-01, -4.1019e-01]],\n\n         [[-5.9746e-03,  1.0048e-02, -4.2604e-03,  ..., -1.0930e-01,\n            3.4326e-02,  8.4342e-02],\n          [ 1.0278e-01, -3.1082e-02,  5.8721e-02,  ...,  6.1986e-01,\n           -2.0531e-01, -6.2644e-01],\n          [ 1.1943e-01, -5.8391e-02,  5.6865e-02,  ...,  7.2164e-01,\n           -1.2859e-01, -6.6454e-01],\n          ...,\n          [-3.3965e-01, -1.6304e-01,  5.2154e-01,  ...,  7.4580e-01,\n           -5.2450e-02, -6.4519e-01],\n          [-2.0817e-01,  2.0681e-01,  3.1963e-01,  ...,  7.1918e-01,\n           -3.6935e-02, -6.8810e-01],\n          [-3.5905e-01,  3.3189e-01,  2.7067e-01,  ...,  2.4084e-01,\n           -5.4943e-02, -6.5760e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7947e-02,  7.9241e-03,  5.4580e-02,  ...,  7.3166e-01,\n            7.5432e-01, -5.3334e-01],\n          [-1.4227e+00, -3.1328e-01,  7.6773e-01,  ..., -1.4749e+00,\n            1.3679e+00,  1.1102e+00],\n          [ 2.1317e-01, -3.5648e-01, -4.1964e-01,  ...,  1.3892e-01,\n            8.3991e-01, -9.4518e-01],\n          ...,\n          [ 2.0481e-01,  6.3052e-01,  5.3492e-01,  ...,  1.6286e+00,\n            7.7440e-01, -2.1364e+00],\n          [-9.4441e-01,  1.3181e+00, -9.8881e-01,  ...,  9.1665e-01,\n            8.6446e-01, -2.0505e+00],\n          [-9.4097e-01,  1.0915e+00, -3.3495e-02,  ..., -2.5536e-01,\n            1.0748e+00, -1.3074e+00]],\n\n         [[-2.9458e-02,  1.3561e-02, -1.8575e-02,  ..., -1.3401e+00,\n           -1.5849e+00,  8.7977e-01],\n          [-2.5556e+00,  6.0933e-01, -1.5862e+00,  ..., -2.3096e+00,\n           -2.1301e+00,  1.4235e-01],\n          [-1.0071e+00,  4.9559e-02, -6.3062e-01,  ..., -3.2193e+00,\n           -1.8662e+00, -1.0737e-02],\n          ...,\n          [ 6.9421e-01, -4.1986e-01, -9.7129e-01,  ..., -2.5432e+00,\n           -1.3412e+00,  3.5665e-01],\n          [ 1.0528e+00, -1.8289e+00, -3.9715e-01,  ..., -2.5711e+00,\n           -1.0718e+00, -2.1864e-01],\n          [ 1.1092e+00, -1.4605e+00, -1.4248e+00,  ..., -1.9361e+00,\n            7.9981e-01,  4.2452e-01]],\n\n         [[ 4.9120e-02,  7.5759e-02,  2.0578e-02,  ...,  5.4245e-01,\n            3.2780e-01, -1.0823e+00],\n          [ 1.9434e+00,  1.4119e+00,  8.9912e-01,  ..., -2.4708e+00,\n           -5.8531e-01,  1.9458e+00],\n          [ 1.3223e+00,  6.1247e-01,  1.3897e+00,  ..., -2.6598e+00,\n           -2.1375e+00,  9.3404e-01],\n          ...,\n          [-1.0583e+00, -1.9213e+00,  1.4193e+00,  ..., -1.9858e+00,\n           -2.4528e+00,  1.8812e+00],\n          [-1.6972e+00, -1.7027e+00,  1.3032e-01,  ..., -1.7238e+00,\n           -3.0209e+00,  1.1120e+00],\n          [ 6.4561e-02, -9.5799e-01,  4.1644e-01,  ..., -2.1372e+00,\n           -2.3745e+00,  6.9682e-01]],\n\n         ...,\n\n         [[ 3.2195e-02,  5.6718e-02,  3.6552e-02,  ..., -1.1915e+00,\n            2.4083e+00, -8.5296e-01],\n          [ 1.9427e+00,  8.3545e-01,  1.8341e+00,  ..., -2.2661e+00,\n           -6.1698e+00, -2.3955e+00],\n          [ 1.3729e+00, -1.2718e+00,  9.0652e-01,  ..., -2.7209e+00,\n           -4.1600e+00, -2.1662e+00],\n          ...,\n          [-1.8673e+00, -1.3913e+00, -1.8973e+00,  ..., -2.7562e+00,\n           -3.9062e+00, -3.6888e+00],\n          [-1.5251e+00, -1.6881e+00, -7.6363e-01,  ..., -1.6459e+00,\n           -3.8735e+00, -1.6431e+00],\n          [-1.5214e-01, -6.2781e-01, -2.1714e+00,  ..., -1.4013e+00,\n           -4.1542e+00, -2.9377e+00]],\n\n         [[-3.5472e-02, -2.0856e-02,  8.1324e-03,  ..., -5.8972e-02,\n           -1.6879e+00, -5.6755e-01],\n          [-9.1058e-01, -1.8759e+00,  1.3397e+00,  ..., -6.5463e-01,\n            2.4758e+00,  1.8893e+00],\n          [ 7.3099e-01, -1.5181e+00,  3.8525e-01,  ...,  9.1481e-02,\n            2.7941e+00,  7.3755e-01],\n          ...,\n          [ 5.8331e-01, -2.1469e-01,  6.6975e-01,  ...,  9.8887e-03,\n            3.0421e+00,  9.7136e-01],\n          [-9.5213e-01,  1.7801e+00, -4.6554e-01,  ..., -1.5791e+00,\n            3.7799e+00,  1.1569e+00],\n          [-2.1433e+00,  2.5012e+00, -1.9069e-01,  ..., -8.2747e-01,\n            3.1538e+00,  4.1107e-01]],\n\n         [[ 6.5458e-03,  4.6341e-03,  1.3568e-01,  ...,  6.9160e-01,\n            6.4990e-01,  5.2006e-01],\n          [ 4.8618e+00,  4.0616e+00,  3.7445e+00,  ...,  1.9428e+00,\n            1.0039e+00,  1.3118e+00],\n          [ 3.8549e+00,  1.3905e+00,  2.3375e+00,  ...,  7.9430e-01,\n           -8.6311e-01,  2.3621e+00],\n          ...,\n          [-3.1003e+00,  8.0585e-01, -1.8532e+00,  ..., -3.3204e-01,\n           -1.2169e+00,  1.0019e+00],\n          [-3.9758e+00,  5.6328e-01, -3.1805e+00,  ..., -5.5188e-01,\n           -1.2724e+00,  3.1984e+00],\n          [-6.5815e-01, -5.3763e-01, -4.4679e+00,  ...,  1.2719e+00,\n           -1.3666e+00,  7.4437e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.1082e-04,  3.5931e-03,  1.1365e-02,  ...,  2.7242e-02,\n           -1.2164e-02,  4.3938e-03],\n          [ 7.1937e-01, -3.0655e-01, -7.0021e-02,  ...,  7.0112e-02,\n            6.7781e-02, -5.4168e-01],\n          [ 7.3877e-01,  2.1703e-02, -1.0473e-01,  ..., -2.1154e-02,\n            2.0039e-01, -5.4422e-01],\n          ...,\n          [ 6.1225e-01, -1.2619e-01, -3.8412e-01,  ..., -1.9074e-01,\n            3.5200e-01, -5.5218e-01],\n          [ 5.8258e-01,  1.7563e-02, -1.8405e-01,  ..., -3.2325e-02,\n            1.4457e-01, -4.9312e-01],\n          [ 6.1921e-01, -1.5764e-01, -2.0414e-01,  ...,  8.6097e-02,\n           -6.4259e-02, -8.1969e-01]],\n\n         [[-1.2056e-02, -5.5574e-03,  3.8054e-02,  ..., -1.7010e-01,\n            3.2242e-02, -1.3090e-02],\n          [-1.2494e-01,  1.3325e-01,  6.3530e-02,  ...,  3.1447e-02,\n            9.3956e-02,  1.3487e-01],\n          [-2.0624e-01,  2.0004e-01,  1.4182e-01,  ..., -3.0684e-02,\n           -2.0348e-01,  2.2170e-01],\n          ...,\n          [ 8.2619e-02, -4.3028e-02,  5.5799e-02,  ..., -3.4660e-01,\n           -3.2581e-01,  5.0147e-01],\n          [ 8.1035e-02,  2.4790e-01,  1.1859e-01,  ..., -1.9436e-02,\n           -1.4303e-01,  1.4264e-01],\n          [-1.4075e-01,  2.3101e-01, -2.6854e-01,  ...,  1.1051e-01,\n           -3.3257e-01,  5.5553e-02]],\n\n         [[ 3.2441e-02,  7.3343e-03, -7.6267e-03,  ..., -6.4834e-03,\n           -1.7868e-02, -1.3072e-02],\n          [ 3.0421e-01,  8.5115e-02, -7.7033e-02,  ..., -1.5478e-01,\n            1.5215e-01, -6.6995e-02],\n          [ 3.4015e-01,  3.5630e-02, -1.5668e-02,  ..., -1.7520e-01,\n            2.5132e-01,  1.6824e-01],\n          ...,\n          [ 1.7575e-01, -3.0521e-01,  4.1171e-01,  ...,  7.2286e-02,\n           -2.5222e-01, -1.4530e-01],\n          [ 1.4619e-01,  1.0188e-01, -2.4380e-01,  ..., -1.6767e-01,\n           -3.9960e-01,  1.6171e-01],\n          [-2.3076e-01,  2.3107e-02,  4.7495e-03,  ...,  7.6523e-02,\n            3.8540e-02, -1.1297e-01]],\n\n         ...,\n\n         [[-7.2353e-03,  1.2753e-02,  5.2595e-03,  ..., -1.2895e-02,\n           -5.8279e-03, -7.6916e-03],\n          [ 2.4093e-01, -6.4406e-02,  1.2542e-01,  ...,  1.0988e-01,\n            3.0498e-01,  1.3181e-01],\n          [-1.0176e-01,  1.6784e-02, -1.7389e-01,  ..., -1.2551e-01,\n            1.2583e-01, -1.7929e-02],\n          ...,\n          [-5.6606e-02,  4.1440e-02, -2.0578e-01,  ...,  3.2717e-01,\n            1.3850e-03, -8.0089e-02],\n          [ 1.3123e-01, -6.5005e-02, -1.2321e-02,  ...,  1.7689e-02,\n            1.9605e-01, -9.2188e-02],\n          [-3.0912e-02, -2.2582e-01,  2.3516e-01,  ...,  1.8610e-01,\n            3.1048e-01, -4.0471e-02]],\n\n         [[ 2.8551e-03,  4.7691e-02, -3.8349e-02,  ...,  4.4290e-02,\n           -2.6177e-04, -3.4950e-01],\n          [ 4.8099e-01, -3.4549e-01,  1.2902e-01,  ...,  7.0070e-01,\n            3.1902e-01,  1.7167e-01],\n          [ 2.5321e-01, -3.6839e-02, -1.0152e-01,  ...,  3.4972e-01,\n            2.1394e-01,  1.3799e-01],\n          ...,\n          [ 1.2337e-01,  1.2475e-01, -2.7210e-01,  ...,  3.3952e-01,\n           -2.4348e-01, -2.5896e-01],\n          [ 1.2148e-01, -5.1883e-02, -2.5086e-01,  ...,  5.8157e-01,\n            3.1913e-03, -1.3361e-02],\n          [ 5.2966e-01, -4.1191e-02, -3.2714e-01,  ...,  2.6910e-01,\n           -2.4594e-01,  1.3633e-01]],\n\n         [[-8.1304e-03,  1.4941e-02, -1.0689e-02,  ...,  7.0810e-03,\n            5.8582e-03, -1.3757e-02],\n          [-2.2038e-01,  2.3056e-01,  2.9953e-02,  ..., -4.9608e-02,\n           -1.3103e-01, -1.4165e-01],\n          [-5.8053e-01,  9.6231e-03,  2.1723e-01,  ...,  2.3857e-01,\n            1.9165e-01,  2.1279e-02],\n          ...,\n          [-2.3823e-02,  1.5250e-01, -4.2803e-01,  ..., -1.1787e-01,\n            4.9306e-01, -3.7298e-01],\n          [-1.1632e-03,  2.6019e-03, -1.5353e-01,  ..., -2.3680e-01,\n            2.4624e-01, -7.7515e-02],\n          [-3.3636e-01, -1.6598e-01,  2.2549e-01,  ..., -7.8078e-02,\n            2.5051e-01,  2.2947e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2312e-01,  8.0175e-04,  6.5801e-02,  ...,  3.4184e-01,\n            7.8137e-01, -1.7893e-01],\n          [-2.0273e+00,  2.5088e-01,  2.2136e+00,  ...,  2.1602e+00,\n           -3.6832e+00,  5.5925e-02],\n          [ 5.8843e-01, -2.2986e+00,  2.9580e+00,  ...,  2.8248e+00,\n           -4.8020e+00, -7.1777e-01],\n          ...,\n          [ 3.2741e+00, -1.0181e+00,  8.7457e-01,  ...,  2.4720e+00,\n           -6.6660e+00, -1.2930e+00],\n          [ 3.3239e-01,  1.3234e+00,  1.0918e+00,  ...,  3.1143e+00,\n           -6.8487e+00, -1.3001e+00],\n          [-1.6789e+00,  1.6649e+00,  6.4973e-01,  ...,  4.9136e+00,\n           -5.9759e+00, -1.3086e+00]],\n\n         [[-4.0276e-02, -6.4417e-02, -5.3369e-02,  ...,  4.9013e-01,\n            9.7800e-02, -3.2408e-01],\n          [-2.2293e+00, -1.7687e+00, -1.3600e+00,  ..., -1.2043e+00,\n           -3.0898e-01, -8.6754e-01],\n          [-1.8947e+00, -4.6684e-01, -4.1457e-01,  ..., -2.0658e+00,\n           -5.5696e-02,  2.4413e-01],\n          ...,\n          [ 7.9429e-01,  2.2565e+00,  9.3412e-01,  ..., -8.6943e-01,\n           -6.8542e-01, -3.4692e-01],\n          [ 1.1933e+00,  7.4079e-01,  1.6663e+00,  ..., -9.9109e-01,\n           -1.0008e+00,  5.6319e-01],\n          [ 5.9951e-01,  1.5651e+00,  2.5191e+00,  ..., -3.8812e-02,\n           -6.5149e-01, -1.8521e-02]],\n\n         [[ 3.3561e-02,  8.4039e-02, -2.8396e-02,  ..., -4.7250e-01,\n            1.9230e+00, -2.1323e-01],\n          [ 5.7200e-01,  2.2279e+00,  1.5472e-01,  ..., -6.2765e-01,\n            8.7105e-01, -2.3120e-01],\n          [-1.0779e+00,  2.0015e-01,  1.5254e+00,  ...,  1.1903e+00,\n           -1.7934e-01,  2.8011e-02],\n          ...,\n          [-5.1735e-01, -1.3757e+00,  1.9088e+00,  ...,  4.7389e+00,\n           -1.3282e+00, -6.7751e-01],\n          [ 8.7289e-01, -1.1860e+00,  1.6457e+00,  ...,  1.5667e+00,\n           -1.8226e+00,  1.0986e-01],\n          [ 1.0428e+00, -1.9213e+00,  2.9362e-01,  ..., -5.2215e-01,\n           -1.1983e+00,  2.4624e-01]],\n\n         ...,\n\n         [[ 3.5742e-03, -1.2710e-02, -1.4841e-02,  ...,  9.3606e-03,\n           -1.9389e-01,  7.3719e-02],\n          [-4.5218e-01,  6.1590e-02,  2.1453e-01,  ...,  1.5978e+00,\n            4.0628e+00, -3.0448e-01],\n          [-2.9151e-03,  2.6232e-01,  3.1400e-01,  ...,  4.0447e+00,\n            5.4559e+00, -3.1808e+00],\n          ...,\n          [ 4.8316e-01,  1.1920e-02,  8.1735e-01,  ...,  4.6892e+00,\n            4.6682e+00, -1.2101e+00],\n          [ 6.1053e-01, -5.4636e-01,  1.2391e+00,  ...,  4.1324e+00,\n            6.1381e+00, -2.8616e+00],\n          [ 7.9651e-01,  2.1781e-01,  8.0535e-01,  ...,  4.9993e+00,\n            4.2333e+00, -2.6715e+00]],\n\n         [[ 3.2944e-02, -4.3002e-03,  7.3976e-03,  ..., -7.6260e-01,\n            7.9404e-01, -7.5428e-01],\n          [-5.1165e-01,  3.3352e-01,  1.6246e-01,  ...,  1.1631e+00,\n            1.7283e+00,  3.4625e-01],\n          [ 1.5882e-01, -2.4296e-02, -2.3312e-01,  ...,  1.3329e+00,\n            8.8855e-01, -4.3772e-02],\n          ...,\n          [ 3.1424e-01, -2.7989e-01, -6.4704e-01,  ...,  2.0611e+00,\n            8.6480e-01,  1.1318e+00],\n          [ 1.9290e-02,  2.0101e-01, -5.4484e-01,  ...,  1.2569e+00,\n            5.4770e-01,  2.0927e+00],\n          [ 1.4652e-01,  7.5938e-01, -9.8990e-02,  ...,  1.3441e+00,\n            1.2334e+00,  3.1849e+00]],\n\n         [[-2.4856e-02,  5.0377e-02, -2.0058e-02,  ..., -7.2910e-02,\n            1.7153e+00,  6.1020e-01],\n          [ 1.2633e+00,  1.7550e+00, -1.0946e+00,  ..., -4.0205e-01,\n           -3.2863e+00,  1.8942e+00],\n          [ 1.4678e+00,  9.7763e-01, -9.2193e-01,  ..., -1.7589e-01,\n           -3.5836e+00,  3.3629e+00],\n          ...,\n          [-4.3405e-01,  4.0004e-01, -1.2932e+00,  ..., -8.0985e-01,\n           -2.9375e+00,  3.4383e+00],\n          [-1.2732e+00,  6.0986e-02, -6.0903e-01,  ..., -6.1697e-01,\n           -3.2768e+00,  3.0371e+00],\n          [-6.4617e-01, -2.6350e-01, -7.1758e-01,  ..., -1.4704e+00,\n           -2.5376e+00,  4.9321e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.8676e-03,  1.1584e-03, -2.5280e-03,  ...,  1.0427e-03,\n           -1.0717e-02, -7.4770e-03],\n          [-7.7805e-01, -9.1382e-02, -8.1707e-02,  ..., -4.8083e-01,\n           -7.2615e-01,  4.8550e-01],\n          [-1.7969e-01,  5.2268e-03, -3.1901e-01,  ..., -2.2387e-01,\n           -2.4940e-01,  4.2651e-02],\n          ...,\n          [ 4.2639e-01, -4.6097e-01,  9.1851e-02,  ...,  2.3704e-01,\n           -1.4049e-01,  4.5012e-01],\n          [ 5.4303e-02,  2.1139e-01, -2.1290e-01,  ..., -2.3935e-01,\n            1.9690e-01, -1.4556e-01],\n          [ 6.2408e-01,  1.3162e-01,  5.5379e-02,  ..., -1.1796e-01,\n            1.4404e-01, -4.0336e-01]],\n\n         [[ 2.7190e-01,  1.4988e-02, -2.0839e-03,  ...,  3.9396e-03,\n           -1.3605e-02, -1.2758e-03],\n          [-2.7506e-01,  9.9242e-02,  5.8154e-03,  ..., -7.6369e-02,\n            3.0214e-01,  5.2639e-02],\n          [-7.1955e-01,  1.5563e-01, -2.2284e-01,  ...,  2.7428e-02,\n           -1.4784e-02,  1.6643e-01],\n          ...,\n          [-8.5632e-01, -1.5496e-01, -6.9391e-03,  ..., -3.6154e-01,\n           -1.1608e-01,  7.1647e-02],\n          [-4.2170e-01,  4.5486e-02,  1.2847e-01,  ..., -1.3755e-01,\n           -4.6646e-01,  1.7543e-01],\n          [-9.4145e-01, -2.0402e-02, -8.6336e-02,  ..., -6.6149e-02,\n           -3.6288e-01,  3.5880e-01]],\n\n         [[ 3.8737e-02,  1.7042e-02, -4.3715e-03,  ...,  3.2801e-02,\n            3.5758e-03, -2.1997e-01],\n          [ 6.4525e-02, -1.3032e-01, -7.0709e-02,  ..., -1.1451e-01,\n           -1.4236e-01, -5.1637e-01],\n          [ 1.2331e-01,  7.8746e-02, -9.9049e-03,  ..., -1.1019e-01,\n           -2.0571e-01, -1.6622e-01],\n          ...,\n          [-5.6203e-02, -3.0834e-02,  8.1040e-02,  ...,  2.3187e-01,\n           -3.1130e-01, -1.1976e-01],\n          [ 5.7486e-02, -1.6409e-01,  9.7397e-02,  ...,  8.6678e-02,\n           -1.8303e-01, -2.1524e-01],\n          [ 9.5699e-02, -9.4756e-02,  1.3789e-01,  ..., -1.6573e-01,\n           -5.1339e-02, -5.8899e-01]],\n\n         ...,\n\n         [[ 1.0134e-02,  8.3906e-03, -1.4678e-02,  ...,  2.0061e-02,\n            8.9872e-03, -4.0096e-03],\n          [-6.4184e-01,  2.4045e-01,  9.8672e-02,  ...,  4.4117e-02,\n            9.3468e-02, -7.7275e-01],\n          [-7.0650e-01, -4.5859e-01, -1.3923e-01,  ..., -4.0585e-01,\n           -2.4234e-02,  5.8095e-01],\n          ...,\n          [-7.0595e-01,  4.2824e-02, -1.0454e-01,  ..., -1.7282e-01,\n            3.1412e-03,  4.0021e-02],\n          [-1.8547e-01, -5.4997e-01,  5.4040e-01,  ...,  8.5589e-01,\n           -6.9312e-01, -6.8143e-02],\n          [-4.1477e-02, -3.4422e-01, -1.2055e-01,  ..., -5.7615e-01,\n           -6.1381e-01,  3.2405e-01]],\n\n         [[ 4.6773e-03, -6.1752e-03, -1.3456e-02,  ...,  2.2459e-03,\n           -1.4624e-02,  1.7275e-02],\n          [ 1.0933e-01, -2.4646e-01,  4.2456e-01,  ..., -5.0560e-02,\n           -5.1515e-01,  7.0203e-02],\n          [-5.8753e-01, -2.8683e-01,  5.6241e-01,  ...,  8.7894e-02,\n           -5.3783e-01, -1.3549e-01],\n          ...,\n          [-3.6709e-01,  1.0368e-02, -2.6492e-01,  ...,  3.0226e-01,\n            1.7847e-01, -3.9443e-01],\n          [-1.8174e-01, -1.5944e-01, -1.2444e-01,  ...,  1.7185e-01,\n           -1.4663e-01, -3.7696e-01],\n          [ 8.7055e-02, -4.1232e-02,  1.1925e-02,  ..., -4.3146e-02,\n           -3.0805e-01, -1.8966e-01]],\n\n         [[ 9.5875e-03,  1.9877e-03,  5.5560e-04,  ...,  1.6537e-02,\n            1.5532e-03,  1.2090e-02],\n          [-2.8249e-02,  3.7383e-02,  3.8696e-01,  ..., -5.9451e-02,\n            1.7590e-01, -3.6832e-02],\n          [-2.0332e-03, -1.6847e-01,  1.1968e-02,  ...,  6.5603e-02,\n           -1.3205e-01,  2.7796e-01],\n          ...,\n          [-5.3895e-01, -9.4709e-02, -2.4603e-01,  ..., -9.8091e-02,\n            1.9656e-01, -9.3505e-02],\n          [-1.3160e-01,  2.0801e-01, -1.3776e-01,  ..., -5.2161e-02,\n           -6.2677e-02,  3.4716e-02],\n          [-1.6502e-01,  1.7395e-01,  9.3074e-02,  ..., -6.8535e-02,\n           -1.7071e-01,  6.4562e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.5351e-02, -8.4757e-02, -6.0248e-02,  ...,  6.7227e-01,\n            1.3881e+00, -7.4223e-01],\n          [ 1.2616e+00, -1.2370e+00, -7.4736e-01,  ...,  1.2731e+00,\n           -1.0633e+00, -4.8718e-01],\n          [ 1.1614e-01,  1.6054e-01, -4.4539e-01,  ...,  4.4137e-01,\n           -1.3666e+00,  1.7992e-01],\n          ...,\n          [-5.4168e-01, -2.6959e-01,  6.0514e-01,  ..., -2.5940e-01,\n           -5.2639e-01, -1.0426e+00],\n          [-5.8534e-01, -1.8146e-01,  1.1400e+00,  ..., -2.3092e-01,\n           -1.5982e+00, -1.6913e-01],\n          [ 7.7766e-01,  1.6303e+00,  2.2796e+00,  ..., -5.4150e-01,\n           -1.6284e+00, -1.7295e+00]],\n\n         [[-1.8946e-02, -2.2157e-02,  2.3029e-02,  ...,  3.3776e-01,\n            1.0489e-01, -1.6274e+00],\n          [-2.2000e-01, -1.2279e-01,  4.6447e-01,  ...,  4.0435e-01,\n           -1.0897e+00,  5.0793e+00],\n          [ 3.8183e-01, -1.3501e-01,  9.4150e-01,  ...,  3.5868e+00,\n           -2.5323e+00,  3.0625e+00],\n          ...,\n          [ 7.1628e-01,  2.1930e-01,  6.7505e-01,  ...,  2.6556e+00,\n           -5.0718e-01,  4.5530e+00],\n          [-5.0165e-01, -7.0535e-01, -3.4468e-01,  ...,  3.8036e+00,\n           -1.9527e+00,  3.5220e+00],\n          [-6.0048e-01, -9.2246e-01, -7.6930e-01,  ...,  4.0905e+00,\n           -1.6349e+00,  3.0920e+00]],\n\n         [[ 2.1701e-02, -9.1714e-03,  9.7642e-02,  ...,  9.4590e-01,\n           -2.8308e-01, -5.0261e-01],\n          [ 6.7626e+00, -2.7791e+00,  3.9439e+00,  ...,  2.1430e+00,\n           -6.6616e-01,  9.2521e-02],\n          [ 5.1229e+00, -4.7000e+00,  2.7205e+00,  ...,  1.3188e+00,\n            1.8473e+00,  2.0944e+00],\n          ...,\n          [-6.0245e+00, -1.9102e+00,  4.0358e-01,  ...,  2.3604e+00,\n            1.3408e+00,  1.3355e+00],\n          [-5.4936e+00,  2.1390e+00, -1.3403e-01,  ...,  2.7181e+00,\n            2.1750e+00,  1.3737e+00],\n          [-1.2934e+00,  4.0860e+00, -2.1220e+00,  ...,  2.4340e+00,\n            3.3896e-01,  1.1140e+00]],\n\n         ...,\n\n         [[ 5.4727e-03,  1.4969e-02, -4.3817e-02,  ..., -3.0039e-01,\n            1.1738e-01,  2.3453e+00],\n          [-9.3585e-01,  1.6610e+00, -1.2045e+00,  ..., -1.0590e+00,\n            6.9103e-01, -2.5385e+00],\n          [-8.8610e-01,  2.2702e+00,  1.1747e+00,  ..., -1.7107e+00,\n           -1.5240e+00, -3.1733e+00],\n          ...,\n          [ 9.4467e-01, -1.8372e-01, -2.0256e-02,  ...,  2.1084e-01,\n           -2.7350e+00, -4.0278e+00],\n          [ 1.1101e+00, -2.2151e+00,  1.6296e+00,  ..., -1.1708e+00,\n           -1.5052e+00, -4.0883e+00],\n          [ 8.3699e-01, -2.0846e+00,  2.7970e-01,  ...,  8.6965e-01,\n           -2.0266e+00, -3.0972e+00]],\n\n         [[ 2.7228e-02,  2.5961e-02, -3.5073e-02,  ...,  5.4443e-01,\n            4.4475e-01,  8.4028e-01],\n          [ 2.2954e-01,  1.0210e+00, -1.6859e+00,  ...,  3.5193e+00,\n            6.4974e-01, -5.3879e-01],\n          [ 1.8964e-01,  9.0448e-02,  8.7046e-01,  ...,  2.1939e+00,\n           -3.3477e-01,  5.3138e-01],\n          ...,\n          [ 1.9603e-01, -2.4385e-01, -6.1381e-01,  ...,  2.2383e+00,\n           -1.2850e+00, -4.3273e-01],\n          [-1.3237e+00, -1.1143e+00, -2.3366e-01,  ...,  1.7414e+00,\n           -3.3808e-02, -9.9236e-01],\n          [-1.7050e+00, -6.5092e-02, -6.4913e-01,  ...,  1.6118e+00,\n           -6.3670e-01, -3.3138e-01]],\n\n         [[ 8.4090e-02, -6.8144e-02, -1.7702e-02,  ..., -3.3280e-01,\n            1.3528e+00,  6.3329e-01],\n          [ 1.9053e+00, -5.0859e+00,  1.6806e+00,  ..., -2.0356e-01,\n           -4.8222e+00,  9.6508e-01],\n          [-2.3532e+00, -4.2028e+00,  2.2080e+00,  ...,  1.7840e-01,\n           -5.2344e+00,  8.7347e-01],\n          ...,\n          [-2.2304e+00,  1.1961e+00,  1.8620e+00,  ..., -8.3341e-01,\n           -5.6237e+00,  9.5758e-01],\n          [ 1.8058e+00,  3.1312e+00,  1.1766e+00,  ...,  4.3110e-01,\n           -5.3096e+00, -4.7511e-01],\n          [ 3.0188e+00,  3.3326e+00,  9.3646e-01,  ..., -1.9392e-01,\n           -5.4617e+00, -5.2820e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5132e-02,  1.8012e-02,  1.9541e-02,  ...,  9.7809e-03,\n            2.9872e-03, -3.5270e-02],\n          [ 1.4281e-01, -1.1690e-01, -2.1661e-01,  ..., -5.3711e-02,\n           -2.7509e-01,  1.8915e-01],\n          [ 2.4982e-01, -4.4841e-01, -1.2889e-01,  ..., -3.4453e-01,\n            3.0005e-01,  2.4777e-01],\n          ...,\n          [ 3.6535e-01, -1.3034e-01, -1.3958e-01,  ..., -2.6643e-01,\n            4.5300e-01,  2.8213e-01],\n          [ 2.4861e-01, -2.5750e-01, -1.0779e-01,  ..., -4.1795e-01,\n            1.6677e-01,  4.0313e-01],\n          [ 3.0019e-01, -3.3321e-01, -1.2242e-01,  ...,  1.9265e-01,\n            5.1266e-01, -1.3264e-01]],\n\n         [[-1.9224e-02, -3.4200e-03, -4.3395e-03,  ..., -7.5942e-03,\n           -1.1307e-02,  1.7963e-02],\n          [ 2.5455e-01, -1.8495e-01,  4.3413e-01,  ..., -5.0476e-01,\n           -5.0250e-01,  8.9971e-02],\n          [ 3.3818e-02, -3.1044e-02,  5.4943e-02,  ..., -8.5431e-02,\n           -2.6772e-02, -1.2708e-01],\n          ...,\n          [-2.8894e-01, -3.3197e-02,  1.5209e-01,  ..., -5.2791e-01,\n            1.9384e-01, -4.7756e-02],\n          [-1.1962e-01,  4.4641e-02, -1.0454e-01,  ..., -8.0905e-02,\n           -6.1556e-02,  1.7375e-01],\n          [ 2.4786e-01, -8.8134e-02, -2.4973e-01,  ..., -1.6788e-01,\n           -1.0659e-01,  3.0257e-02]],\n\n         [[-1.0082e-01, -3.0075e-02,  4.1509e-02,  ..., -7.7669e-01,\n           -1.3114e-02,  1.2001e-02],\n          [ 4.5432e-02,  3.4684e-02,  1.6137e-03,  ...,  2.4898e-01,\n            6.4173e-02,  8.2674e-03],\n          [ 3.7278e-02,  2.0497e-01,  1.1244e-01,  ...,  4.9411e-01,\n            7.0016e-02,  1.0976e-01],\n          ...,\n          [-2.1570e-01, -2.0606e-01, -2.4016e-01,  ...,  4.4447e-01,\n           -7.3380e-02,  6.9049e-02],\n          [-1.0900e-02, -3.4667e-01,  2.0258e-01,  ...,  1.1281e-01,\n           -2.3070e-01,  1.1161e-01],\n          [ 3.9688e-01, -3.2756e-01, -1.8547e-01,  ...,  7.6121e-01,\n           -1.7677e-01, -1.0005e-01]],\n\n         ...,\n\n         [[-5.7387e-03,  2.5267e-02, -2.2123e-02,  ...,  1.1183e-02,\n           -2.7630e-03,  1.7944e-02],\n          [-1.3462e-01, -4.6760e-01, -4.1073e-01,  ..., -3.3048e-02,\n           -1.3100e-01, -3.1595e-01],\n          [-7.4877e-03,  1.3933e-01, -5.2769e-02,  ..., -9.8085e-02,\n            1.9186e-01,  1.9385e-02],\n          ...,\n          [-1.2679e-01,  3.3661e-01, -4.8950e-02,  ...,  7.4261e-03,\n            2.2402e-01,  3.6038e-01],\n          [-1.7555e-01, -4.5212e-02,  6.4099e-02,  ...,  2.6169e-02,\n            1.2100e-01, -1.3538e-02],\n          [-8.9102e-02, -9.6494e-02,  8.1932e-02,  ...,  9.2361e-02,\n            2.7794e-01,  3.1251e-01]],\n\n         [[ 1.8212e-01, -1.8599e-02, -2.1570e-02,  ...,  3.7844e-03,\n            1.8826e-02, -2.6143e-02],\n          [-4.6325e-01,  1.6244e-01,  2.2072e-01,  ..., -6.9552e-02,\n            6.1167e-01,  2.6188e-02],\n          [-5.3045e-03,  8.4458e-02,  6.6047e-02,  ...,  1.6932e-02,\n            2.0987e-01,  9.1609e-02],\n          ...,\n          [-4.8462e-01,  1.1965e-01,  8.3614e-02,  ..., -4.0738e-02,\n            3.4050e-01, -1.3341e-01],\n          [-3.5824e-01,  2.9223e-01,  1.8767e-02,  ...,  2.0220e-01,\n            6.3876e-02, -3.2615e-02],\n          [-7.1602e-01,  2.7131e-01, -1.1609e-01,  ...,  2.3846e-01,\n           -2.6324e-01, -6.5402e-02]],\n\n         [[-6.5726e-03,  3.8399e-03, -1.3766e-02,  ...,  6.9220e-03,\n           -4.0670e-03, -1.2715e-02],\n          [ 3.6617e-01,  2.9218e-01,  2.4028e-02,  ..., -2.0067e-04,\n           -1.1130e-01, -1.2831e-01],\n          [ 4.3841e-01, -1.9646e-01, -2.7185e-02,  ..., -6.6065e-01,\n            3.1660e-02,  2.8755e-02],\n          ...,\n          [ 2.4336e-01,  1.5147e-01, -4.0727e-01,  ...,  1.4628e-01,\n            2.9974e-02, -1.9604e-01],\n          [ 6.2762e-01,  3.9751e-01, -2.2898e-01,  ..., -5.8547e-03,\n           -1.4339e-01, -2.3870e-01],\n          [ 6.9609e-01,  3.3760e-03, -1.3496e-01,  ...,  1.7550e-01,\n           -7.9604e-02, -1.1876e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0166e-01,  6.1919e-03, -9.3783e-03,  ..., -4.7718e-02,\n           -5.6272e-01, -5.0096e-01],\n          [ 2.0590e+00, -1.9600e+00, -1.1147e+00,  ...,  1.0343e+00,\n            9.8983e-01,  2.3284e+00],\n          [-1.7154e+00, -3.9402e+00, -2.2778e+00,  ...,  9.1845e-01,\n            3.9979e+00,  2.5265e+00],\n          ...,\n          [-2.0346e+00, -1.5620e+00, -2.1357e+00,  ...,  1.7008e+00,\n            6.1601e+00,  8.7485e-01],\n          [ 1.3886e+00,  1.3091e+00, -8.0861e-01,  ..., -2.7334e-01,\n            4.8732e+00,  1.8793e+00],\n          [ 2.9248e+00,  2.4528e+00, -9.1368e-01,  ...,  2.1388e-01,\n            3.8603e+00,  1.6429e+00]],\n\n         [[-5.6379e-02,  1.4577e-03,  1.8990e-02,  ..., -1.5364e-01,\n            3.8202e-01, -2.9403e-01],\n          [-1.1381e+00, -4.8696e-01,  1.2164e+00,  ...,  1.4825e+00,\n           -1.1934e+00, -2.6655e+00],\n          [ 9.1242e-01,  1.2252e-01, -3.6523e-01,  ...,  1.1115e-02,\n           -1.4870e+00, -2.2476e+00],\n          ...,\n          [-1.7791e-01, -3.7555e-01, -1.2161e+00,  ...,  5.8366e-02,\n           -2.0055e+00, -3.0933e+00],\n          [-1.6694e+00, -4.3770e-01, -8.6095e-01,  ..., -9.8965e-02,\n           -6.4224e-01, -2.4396e+00],\n          [-6.8991e-01,  6.9570e-01, -1.6520e+00,  ...,  6.1562e-01,\n           -9.9733e-01, -2.7517e+00]],\n\n         [[ 1.8466e-02,  1.4924e-03,  1.2577e-02,  ..., -3.4818e-01,\n           -2.3574e-01, -2.2659e-02],\n          [-3.1628e-01, -1.8836e-01, -2.3994e+00,  ..., -1.8448e-01,\n            3.8541e-01,  1.1383e+00],\n          [ 4.1570e-01, -8.1133e-01, -5.0007e-01,  ...,  1.0797e+00,\n           -1.2453e+00,  8.1130e-01],\n          ...,\n          [ 5.7306e-01, -1.7102e+00, -1.1349e+00,  ...,  6.5464e-01,\n           -7.6430e-01, -1.3933e-01],\n          [-6.3758e-01, -2.0501e-01, -1.1317e+00,  ...,  1.5231e+00,\n           -9.7394e-01, -3.4505e-01],\n          [-8.7936e-01,  1.6732e-01,  1.6500e-02,  ...,  1.8085e+00,\n            3.6900e-01,  4.4737e-01]],\n\n         ...,\n\n         [[ 2.0947e-02, -6.7295e-03, -6.2162e-02,  ..., -1.3262e-01,\n            1.1672e+00,  8.1977e-01],\n          [-4.1865e+00, -2.6542e+00, -2.7694e+00,  ...,  2.7021e-01,\n           -5.0194e+00, -8.1190e-01],\n          [-1.7897e+00, -8.9176e-01, -1.9546e+00,  ..., -8.6345e-01,\n           -5.0271e+00, -7.0862e-01],\n          ...,\n          [ 1.1249e+00,  8.6208e-01,  1.1570e+00,  ...,  5.2513e-02,\n           -6.3273e+00,  8.7164e-02],\n          [ 1.5300e+00,  1.0404e+00,  5.0218e-01,  ..., -8.9805e-01,\n           -3.1471e+00, -1.1067e+00],\n          [ 1.2062e+00,  1.5224e+00,  2.3872e+00,  ...,  3.7257e-02,\n           -6.3237e+00,  2.5327e-01]],\n\n         [[-1.0139e-02, -8.0883e-03, -4.1945e-02,  ...,  1.4360e+00,\n           -3.6248e-01,  9.7605e-01],\n          [ 8.9840e-01,  2.1517e+00, -3.8953e-01,  ..., -3.9081e+00,\n            1.1726e+00, -1.3648e+00],\n          [ 1.7977e+00,  7.8718e-01,  1.1889e+00,  ..., -4.4897e+00,\n            2.2454e+00, -1.9135e+00],\n          ...,\n          [-2.4455e-01,  4.0773e-04,  2.1442e-01,  ..., -5.0472e+00,\n            2.9887e+00, -1.8345e+00],\n          [-1.7546e+00, -9.5721e-01,  1.3035e+00,  ..., -5.8629e+00,\n            1.4618e+00, -1.4617e+00],\n          [-6.8578e-01, -1.5225e-01,  1.0087e+00,  ..., -5.0745e+00,\n            1.5732e+00, -1.4222e+00]],\n\n         [[-2.0678e-02, -2.5024e-05, -2.5933e-02,  ...,  9.0611e-01,\n            1.1658e+00, -1.5922e-02],\n          [ 5.4387e-01, -1.5937e+00, -4.2302e-01,  ...,  8.4128e-02,\n           -3.1649e+00, -6.3091e-01],\n          [ 2.1531e+00, -3.2726e-01, -8.0747e-01,  ..., -9.3588e-01,\n           -1.2575e+00, -2.0457e+00],\n          ...,\n          [-1.9525e+00,  9.4569e-01,  1.2079e+00,  ..., -2.9127e+00,\n           -6.6631e-01, -4.0533e+00],\n          [-2.2828e+00,  5.9731e-01,  1.1725e+00,  ..., -1.2854e+00,\n           -1.6468e+00, -2.0581e+00],\n          [-8.2825e-01, -4.9824e-01,  2.1773e+00,  ..., -6.2644e-01,\n           -1.7132e+00, -1.9862e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3526e-02, -7.9978e-03, -6.1671e-03,  ...,  1.7907e-02,\n           -1.5949e-02, -1.2638e-02],\n          [-1.6625e-01, -7.3503e-02,  3.6931e-01,  ..., -4.4330e-01,\n            1.7235e-01,  8.7753e-01],\n          [-1.5399e-01,  2.1759e-01,  1.5064e-01,  ...,  1.4731e-02,\n            1.5621e-02,  8.8263e-02],\n          ...,\n          [-3.0861e-01,  4.5957e-01,  1.2916e-01,  ..., -2.9938e-02,\n           -6.1752e-02, -5.2606e-01],\n          [-3.3056e-01,  2.1115e-01, -1.8068e-01,  ...,  5.4267e-02,\n            7.9759e-02, -3.4859e-01],\n          [ 3.3028e-02,  1.7922e-01,  1.0573e-01,  ..., -2.1721e-01,\n            5.2501e-01, -4.0131e-01]],\n\n         [[-4.8035e-02,  1.9834e-02,  3.9587e-03,  ...,  1.7674e-02,\n            1.0605e-02,  3.7298e-04],\n          [ 1.6199e-01, -1.5647e-01,  2.5685e-01,  ..., -5.2654e-01,\n           -2.7026e-02, -5.9948e-02],\n          [ 2.1545e-01, -1.0643e-01,  4.2412e-02,  ...,  3.6600e-02,\n           -9.2282e-02, -6.1496e-02],\n          ...,\n          [ 3.1842e-01,  1.9338e-01, -3.2992e-01,  ..., -4.8378e-01,\n            3.4042e-02, -2.8658e-01],\n          [ 2.1974e-01,  1.9647e-01, -8.0642e-02,  ..., -1.7587e-01,\n           -1.4707e-01,  3.5467e-01],\n          [ 4.8984e-01,  6.0971e-02, -2.0218e-01,  ..., -5.5265e-02,\n           -1.0686e-02, -4.7457e-01]],\n\n         [[-8.0104e-03, -2.1802e-02,  1.5194e-03,  ..., -4.4974e-02,\n            3.2744e-02,  2.1210e-02],\n          [ 1.2057e-01,  1.1131e-02, -1.6751e-02,  ...,  4.2662e-01,\n            3.3568e-01,  1.9689e-01],\n          [ 1.8086e-01,  3.0699e-01, -8.3206e-02,  ..., -6.2404e-02,\n            4.9114e-01,  7.2837e-03],\n          ...,\n          [ 2.0506e-01,  2.7964e-01,  2.9877e-01,  ...,  5.7278e-02,\n            5.4048e-01, -6.4822e-02],\n          [ 3.4822e-01,  8.7741e-02,  2.1238e-01,  ..., -1.2006e-01,\n            4.1560e-01, -7.6328e-02],\n          [-7.0437e-02,  5.7161e-02,  1.2167e-01,  ..., -1.1188e-01,\n            1.5424e-01, -9.5308e-02]],\n\n         ...,\n\n         [[ 3.5515e-03, -2.1943e-02, -2.0210e-03,  ..., -1.0336e-02,\n            2.3350e-02, -1.0455e-02],\n          [ 2.4865e-01,  4.3538e-01,  4.5029e-01,  ..., -7.2358e-02,\n            1.0188e-01, -1.8383e-01],\n          [-1.0518e-01,  5.2964e-01,  3.7270e-01,  ..., -6.7872e-02,\n            1.3131e-01, -2.8911e-01],\n          ...,\n          [ 3.0064e-01,  5.5310e-01, -5.4533e-01,  ...,  1.4647e-01,\n           -1.5924e-01, -1.5693e-01],\n          [ 3.8755e-01,  3.9562e-01, -5.5967e-01,  ...,  1.4331e-01,\n           -9.4329e-02, -3.2281e-01],\n          [-3.3547e-02,  3.9899e-01, -2.8447e-01,  ..., -6.4645e-02,\n            6.6053e-02, -1.9320e-01]],\n\n         [[ 1.4852e-03,  9.5778e-03,  2.6551e-03,  ..., -1.0983e-02,\n            3.9189e-03,  1.4490e-02],\n          [ 1.8509e-02,  1.6305e-01,  1.6417e-01,  ...,  1.5492e-02,\n           -1.6470e-01,  1.1245e-01],\n          [-4.3813e-01,  3.9587e-01, -1.9904e-01,  ..., -2.9899e-01,\n            3.3618e-02, -3.4078e-02],\n          ...,\n          [-3.3382e-01,  5.3112e-01, -2.7433e-01,  ...,  1.8164e-01,\n            6.9968e-02, -5.8562e-02],\n          [-3.6744e-01,  6.0500e-01, -4.2692e-01,  ...,  1.8678e-01,\n            2.2787e-01,  1.1091e-01],\n          [-5.3681e-01,  6.5478e-01, -2.8589e-01,  ..., -3.1608e-01,\n           -2.2584e-01, -3.8646e-01]],\n\n         [[-2.9645e-01, -1.9910e-02, -1.7448e-02,  ...,  1.3408e-02,\n           -3.6502e-02, -1.3350e-02],\n          [ 7.8251e-01, -7.2778e-01,  9.3247e-01,  ...,  4.0642e-01,\n           -2.4694e-01,  6.0712e-01],\n          [ 6.3364e-01, -1.7825e-01,  2.1098e-01,  ..., -1.4767e-01,\n           -6.5873e-02,  1.0685e-02],\n          ...,\n          [ 2.3564e-01,  4.9659e-03,  2.9801e-01,  ..., -9.7692e-02,\n           -2.0164e-01,  1.4818e-01],\n          [ 5.6062e-01, -1.8731e-01, -3.0178e-01,  ..., -1.6078e-01,\n            5.3026e-04,  1.7745e-01],\n          [ 3.5423e-01, -2.3672e-01, -7.3669e-02,  ...,  1.7488e-01,\n            2.8239e-01,  1.1818e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-4.9875e-02,  2.4972e-03, -1.3115e-02,  ..., -1.0506e-01,\n           -1.0948e+00,  7.7103e-01],\n          [ 1.4863e+00,  1.5200e+00, -1.1959e+00,  ..., -1.3442e+00,\n            3.9415e+00, -5.7495e-01],\n          [-1.0452e-01,  3.3289e-01,  1.0234e-01,  ..., -2.8023e+00,\n            4.7230e+00, -3.0324e+00],\n          ...,\n          [-1.1804e-01, -2.6029e-01,  5.8443e-01,  ..., -9.4457e-01,\n            3.2178e+00, -3.4486e+00],\n          [-2.7280e-02, -4.2073e-01,  8.1108e-01,  ..., -3.5189e-01,\n            4.3311e+00, -3.7826e+00],\n          [-7.3906e-02, -1.3747e+00,  3.9041e-02,  ..., -6.8392e-01,\n            3.2543e+00, -2.6435e+00]],\n\n         [[-3.5304e-02,  2.9584e-03, -2.7550e-02,  ..., -3.4000e-01,\n            1.1422e+00,  2.6492e-02],\n          [-5.6816e-02,  4.5117e-03, -1.3083e+00,  ...,  1.1244e+00,\n           -9.8961e-02,  1.7672e+00],\n          [ 1.8967e+00,  6.0789e-01,  1.9745e-01,  ..., -6.9852e-01,\n           -1.4396e+00,  2.8527e-01],\n          ...,\n          [-5.7954e-01,  6.8881e-01,  7.5959e-01,  ..., -6.2134e-01,\n           -1.2529e+00,  5.6745e-01],\n          [-1.5116e+00,  3.8163e-01,  1.6837e+00,  ...,  4.5959e-01,\n           -1.1081e+00,  9.8418e-01],\n          [-1.2084e+00, -1.3505e-01,  2.4064e+00,  ..., -2.4949e-01,\n            7.0691e-01,  7.7508e-01]],\n\n         [[ 3.7110e-02,  1.0985e-01,  7.0407e-02,  ..., -9.0719e-01,\n           -1.1212e+00,  1.1719e+00],\n          [-4.4360e+00,  2.4716e+00,  1.2563e+00,  ...,  3.4452e+00,\n            1.7526e+00, -2.0145e-01],\n          [-3.4237e+00,  1.0477e+00,  2.8551e+00,  ...,  2.7749e+00,\n            9.4620e-01, -1.3547e+00],\n          ...,\n          [ 2.9569e+00, -1.6506e+00,  4.2026e+00,  ...,  4.4533e+00,\n            1.0390e+00, -1.0966e+00],\n          [ 4.2534e+00, -2.2310e+00,  2.0792e+00,  ...,  2.6211e+00,\n            1.2067e+00, -7.3345e-01],\n          [ 3.0529e+00, -1.4761e+00,  4.9123e-01,  ...,  3.2055e+00,\n            1.5274e+00, -1.3190e+00]],\n\n         ...,\n\n         [[ 2.9833e-03, -9.0822e-03,  1.8295e-02,  ...,  6.1501e-01,\n           -1.1246e-01,  8.5372e-01],\n          [ 3.3304e-01, -6.4003e-01, -1.0934e+00,  ..., -3.9152e+00,\n            4.6173e-01, -5.7613e-01],\n          [-5.1271e-02, -3.1483e-01, -2.7131e-01,  ..., -3.7386e+00,\n           -2.9720e-01, -2.6095e+00],\n          ...,\n          [-2.3231e-01,  7.7547e-01, -3.5580e-01,  ..., -4.7908e+00,\n            1.2966e+00, -2.2226e+00],\n          [-2.2539e-03,  1.0059e+00,  2.9037e-01,  ..., -3.9186e+00,\n            3.1690e+00, -2.5507e+00],\n          [-2.3137e-01,  4.1141e-01,  6.0183e-01,  ..., -4.8506e+00,\n            3.3978e+00, -3.1643e+00]],\n\n         [[ 3.1923e-01,  6.5008e-02, -1.5732e-01,  ...,  2.5957e-01,\n           -6.0922e-01,  5.0128e-01],\n          [ 9.7237e+00,  4.0054e+00, -2.1038e+00,  ...,  1.1219e+00,\n           -1.1703e+00,  1.6707e-01],\n          [ 7.4607e-01,  4.6296e+00, -1.1777e+00,  ...,  1.3249e+00,\n           -1.6541e+00,  1.2219e+00],\n          ...,\n          [-1.0187e+01, -5.7691e-01,  2.1336e+00,  ..., -3.7384e-02,\n           -9.9703e-01,  1.5344e+00],\n          [-2.8958e+00, -4.0741e+00,  2.4365e+00,  ...,  1.3634e-01,\n           -1.3923e+00,  1.4176e+00],\n          [ 7.5061e+00, -3.6183e+00,  3.2269e+00,  ...,  6.6178e-01,\n           -1.4693e+00,  1.5031e+00]],\n\n         [[-4.7712e-02, -1.6998e-02,  4.7513e-02,  ..., -2.0291e+00,\n           -3.1721e-01, -1.9669e+00],\n          [ 4.6644e-01, -2.0919e-01,  3.7741e+00,  ...,  1.8680e+00,\n            9.3282e-01,  4.8208e-01],\n          [ 1.7686e+00, -1.0605e+00,  2.7540e+00,  ...,  4.3334e+00,\n           -7.2462e-01,  1.3663e+00],\n          ...,\n          [ 7.0148e-01, -1.6251e-01,  9.9140e-02,  ...,  4.4427e+00,\n           -2.3735e+00,  2.5939e+00],\n          [-1.2951e+00,  1.2367e+00,  2.1290e-03,  ...,  4.9486e+00,\n           -1.9682e-01,  2.4470e+00],\n          [-1.8347e+00,  2.0486e+00,  7.5327e-02,  ...,  4.4172e+00,\n            5.1234e-01,  1.1785e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.5693e-03,  3.5057e-02,  1.6987e-02,  ..., -3.8882e-02,\n           -8.0818e-02,  3.6926e-02],\n          [ 8.1845e-02,  6.0510e-01,  6.9635e-01,  ..., -8.5384e-01,\n            5.5322e-02, -1.0471e+00],\n          [-9.1735e-02,  2.2055e-01, -3.3842e-01,  ..., -3.2321e-01,\n            8.8266e-01,  3.1458e-01],\n          ...,\n          [-2.6894e-01, -3.2540e-01, -4.2203e-01,  ..., -2.5452e-01,\n            4.4596e-02, -5.8919e-01],\n          [ 1.0207e-01,  1.5196e-01, -8.9608e-01,  ...,  1.8337e-01,\n           -1.6134e-01, -1.1361e-02],\n          [ 2.3689e-01,  9.0550e-01,  2.4622e-01,  ...,  2.5552e-01,\n            1.2596e-01,  2.7388e-01]],\n\n         [[-8.8539e-03, -1.8422e-02, -2.0399e-02,  ...,  2.3280e-02,\n           -7.1595e-03, -4.6043e-03],\n          [ 3.3777e-03, -1.4677e-01,  3.8548e-01,  ..., -2.8452e-03,\n           -2.4198e-01,  2.2888e-01],\n          [-4.7083e-02, -3.2774e-02,  3.5672e-01,  ...,  8.3398e-02,\n            3.9802e-01,  9.0960e-02],\n          ...,\n          [ 2.5897e-01,  2.0614e-01, -2.2329e-01,  ...,  2.8248e-01,\n            3.0558e-01,  4.1906e-01],\n          [ 1.6578e-01,  1.4408e-01,  1.6520e-01,  ...,  3.1801e-01,\n            1.2584e-01,  1.9883e-01],\n          [-5.2341e-01, -2.3311e-02,  1.2760e-01,  ...,  2.7007e-01,\n           -1.2230e-01,  1.1001e-01]],\n\n         [[-6.7410e-02, -4.4090e-03,  1.6212e-02,  ..., -1.1219e-02,\n            2.1429e-03, -1.5527e-03],\n          [-7.4422e-02, -4.2096e-02,  2.0127e-01,  ..., -7.1574e-02,\n            9.6221e-02,  3.5584e-01],\n          [-1.4204e-01,  8.8811e-02,  1.2295e-01,  ...,  5.4589e-02,\n           -2.7241e-01,  2.8080e-01],\n          ...,\n          [-1.0015e+00,  2.3462e-01,  1.7334e-01,  ...,  1.5034e-01,\n            2.8653e-01, -2.0339e-01],\n          [-4.9001e-01,  4.7482e-01, -6.2534e-02,  ...,  1.9007e-01,\n           -1.1071e-01, -1.2971e-01],\n          [ 4.0019e-01,  4.1475e-01, -2.3828e-01,  ...,  3.3566e-01,\n           -2.9233e-01, -2.2862e-01]],\n\n         ...,\n\n         [[ 4.7063e-04,  3.1449e-02, -1.7503e-02,  ...,  8.2695e-03,\n           -1.0970e-02, -1.1543e-02],\n          [-1.2277e+00, -3.2604e-01, -6.1393e-02,  ..., -3.8778e-02,\n           -9.9841e-02, -5.4652e-01],\n          [ 9.1386e-02,  4.2278e-01,  9.2652e-02,  ...,  8.1694e-03,\n           -3.0594e-02,  1.4136e-01],\n          ...,\n          [-2.2290e-01, -6.7235e-02,  5.7267e-02,  ...,  2.3843e-01,\n           -2.7630e-02, -1.2387e-02],\n          [ 2.0408e-01,  3.7040e-01,  2.9052e-02,  ..., -4.8617e-02,\n            2.0986e-01,  3.6221e-01],\n          [ 4.6196e-01, -4.6471e-01, -4.3433e-02,  ...,  1.3075e-01,\n            5.5972e-02,  2.5477e-01]],\n\n         [[ 9.3577e-03,  3.9018e-03,  9.3528e-04,  ..., -5.8127e-03,\n           -1.1375e-03,  1.1215e-03],\n          [-4.8039e-01, -1.3629e-01, -7.1690e-01,  ...,  5.1239e-01,\n            3.5236e-01, -1.1517e-01],\n          [-6.2005e-01, -4.2717e-01, -4.7873e-01,  ..., -3.8481e-03,\n           -4.0798e-01, -8.2157e-01],\n          ...,\n          [-5.1856e-01, -1.4422e-01, -2.7818e-01,  ...,  1.5228e-01,\n           -2.3082e-01, -3.3898e-01],\n          [-1.9378e-01,  5.9553e-01, -1.1286e-01,  ...,  5.0783e-01,\n            4.6441e-02,  8.9126e-02],\n          [-1.1452e-01, -1.2341e-01,  1.7265e-01,  ..., -9.1119e-02,\n           -1.0231e-02,  1.4952e-01]],\n\n         [[ 1.1949e-02, -2.2031e-02,  8.2400e-03,  ...,  8.8179e-03,\n            8.7592e-03,  1.6473e-04],\n          [ 3.2632e-01, -2.0327e-01, -2.1340e-01,  ...,  8.7580e-02,\n           -2.2788e-01,  3.6569e-01],\n          [ 2.6529e-01, -2.1978e-01, -2.1967e-01,  ..., -1.4477e-01,\n           -1.6774e-01,  6.5762e-02],\n          ...,\n          [ 2.4309e-01, -2.5524e-01, -2.3644e-01,  ..., -1.4594e-01,\n           -2.1775e-01, -5.8270e-01],\n          [ 3.2517e-01, -5.5770e-01, -4.9247e-01,  ...,  8.8617e-02,\n           -3.3503e-01,  1.0231e-01],\n          [ 1.8709e-01, -2.3732e-01, -2.8045e-01,  ..., -7.9458e-02,\n           -1.2273e-01,  9.1866e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0001e-02,  5.3479e-03, -9.2337e-03,  ...,  2.9982e-02,\n            8.8365e-01, -1.0928e+00],\n          [ 8.7294e-01, -2.6140e-01,  4.2966e-01,  ..., -1.0903e+00,\n           -2.5277e+00,  1.1819e+00],\n          [ 4.6817e-01,  9.8864e-01,  7.3982e-01,  ..., -4.6623e-01,\n           -2.0306e+00,  2.4614e+00],\n          ...,\n          [-2.7831e-01,  5.0420e-01,  1.2072e+00,  ...,  3.8389e-01,\n           -1.8511e+00,  2.0652e+00],\n          [-7.9975e-01,  6.8814e-01,  6.6837e-01,  ...,  1.7069e+00,\n           -2.4216e+00,  3.3887e+00],\n          [-7.2989e-01, -6.6200e-01,  2.8208e-01,  ...,  1.5478e+00,\n           -3.1226e+00,  2.1741e+00]],\n\n         [[-6.1644e-02,  2.6081e-02,  6.2417e-02,  ...,  6.3455e-01,\n            3.6131e-02,  2.4065e-01],\n          [-2.1200e+00, -1.3107e-01, -1.4443e-01,  ..., -3.1277e-01,\n           -1.2450e+00,  1.0522e+00],\n          [ 1.3968e+00, -1.1912e+00, -2.1702e+00,  ..., -9.9641e-01,\n           -2.4775e-01,  3.1098e-01],\n          ...,\n          [ 9.3467e-01, -2.4026e+00, -1.5782e+00,  ..., -6.1785e-01,\n           -1.6054e+00,  1.0960e+00],\n          [-3.8580e-01, -6.8748e-01, -1.3747e+00,  ..., -4.0310e-01,\n           -3.4155e-01,  4.8981e-01],\n          [-1.8219e+00,  6.2990e-01, -2.6710e-02,  ...,  9.7308e-01,\n           -1.9349e+00,  1.1708e+00]],\n\n         [[-1.5109e-02, -3.5196e-03,  6.1797e-02,  ...,  6.3407e-01,\n           -5.1091e-01, -8.2480e-01],\n          [ 1.5792e+00, -1.2319e+00,  1.1765e+00,  ..., -6.3072e-01,\n            1.3554e+00,  1.6394e+00],\n          [ 1.4461e+00, -4.4069e-01,  9.2721e-01,  ..., -3.9494e-01,\n            5.3216e-01,  7.6567e-01],\n          ...,\n          [-1.2211e+00,  3.1120e-02, -1.2459e+00,  ..., -2.1893e+00,\n            2.6633e+00,  1.2852e-01],\n          [-1.3804e+00,  1.4137e+00, -1.5049e+00,  ..., -1.5894e+00,\n            1.4744e+00,  7.6856e-01],\n          [-8.3890e-01,  4.5816e-01, -1.0203e+00,  ..., -2.3383e+00,\n            2.0474e-01,  8.8204e-01]],\n\n         ...,\n\n         [[ 2.0095e-02, -1.9391e-02, -1.4458e-03,  ..., -6.3438e-01,\n           -7.4186e-02,  1.2699e+00],\n          [-7.9215e-01, -1.0645e+00, -8.1103e-01,  ...,  3.6834e-01,\n            1.0701e+00, -2.1562e+00],\n          [-1.5776e-01, -1.8322e-01, -2.8168e-01,  ..., -2.7338e-02,\n           -1.2710e+00, -2.8762e+00],\n          ...,\n          [-2.1154e-01, -1.1368e-01, -4.2548e-01,  ...,  2.2172e-01,\n            1.3162e+00, -6.0494e+00],\n          [ 4.8693e-01,  6.2472e-01, -5.3281e-01,  ..., -8.7062e-01,\n           -2.0747e+00, -4.9780e+00],\n          [-3.7886e-01,  1.0626e+00,  3.1687e-01,  ...,  2.7412e-01,\n           -5.9731e-01, -3.4808e+00]],\n\n         [[ 1.5396e-01,  1.1387e-02,  1.6595e-02,  ..., -3.2535e-01,\n            1.6872e-01, -1.8801e+00],\n          [ 6.0560e-01, -1.1477e+00, -7.4180e-01,  ...,  1.6262e-01,\n            2.3737e+00,  4.6392e-01],\n          [-2.6162e-01, -2.2885e+00, -1.3641e+00,  ..., -3.8918e-01,\n            1.7129e+00,  4.5273e+00],\n          ...,\n          [-1.7206e+00, -1.4077e+00, -1.1191e+00,  ..., -6.5098e-01,\n            7.7733e-01,  3.0853e+00],\n          [ 5.1052e-01, -4.2147e-01, -1.0513e+00,  ..., -1.1379e+00,\n            1.7694e+00,  3.7997e+00],\n          [ 2.3640e+00,  1.4077e+00, -6.7150e-01,  ..., -1.7631e-01,\n            2.6162e+00,  4.0207e+00]],\n\n         [[-9.3324e-03,  1.2826e-02,  1.0013e-02,  ...,  1.6950e-01,\n            5.9047e-01,  9.9431e-01],\n          [ 1.0309e-01,  5.6140e-02,  2.8503e-02,  ..., -2.4224e+00,\n           -2.5067e+00, -2.2298e+00],\n          [-2.9537e-01,  2.1882e-01, -2.3786e-01,  ..., -1.1905e+00,\n           -2.1368e+00, -3.1597e+00],\n          ...,\n          [-4.5620e-02, -2.2910e-02, -7.9293e-02,  ..., -1.2766e+00,\n           -2.2234e+00, -2.7992e+00],\n          [ 5.6757e-01,  4.9995e-02,  4.2694e-02,  ...,  1.8118e+00,\n           -2.6661e+00, -2.8046e+00],\n          [ 1.2093e-01,  3.0445e-01, -7.8137e-02,  ...,  4.3429e-03,\n           -1.2910e+00, -1.7973e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0155, -0.0141,  0.0053,  ..., -0.0020, -0.0138, -0.0081],\n          [ 0.8893, -0.2964,  0.6060,  ..., -0.1289,  0.1849,  0.7236],\n          [ 0.0516,  0.4002,  0.5555,  ...,  0.0818, -0.7465,  0.1008],\n          ...,\n          [ 0.0592, -0.2722, -0.1607,  ..., -0.1261, -0.0721, -0.5363],\n          [ 0.5351, -0.0285,  0.3295,  ..., -0.1638, -0.3833, -0.1046],\n          [-0.8901,  0.3488,  0.5780,  ..., -0.4821,  0.0581, -0.2783]],\n\n         [[ 0.1125,  0.0091, -0.0121,  ..., -0.0521, -0.0187,  0.0434],\n          [-0.1717, -0.0280,  0.0977,  ...,  0.2039,  0.0755,  0.0779],\n          [-0.0912, -0.1047, -0.0915,  ...,  0.2498,  0.2045, -0.1584],\n          ...,\n          [ 0.1055,  0.1303, -0.6172,  ...,  0.2406, -0.3415,  0.1561],\n          [ 0.1275,  0.6940, -0.4025,  ...,  0.4515, -0.3610,  0.4613],\n          [-0.0474,  0.1060, -0.5638,  ..., -0.1677, -0.0342, -0.1656]],\n\n         [[-0.0043,  0.0186, -0.1110,  ..., -0.0808, -0.0089, -0.0305],\n          [-0.3964, -0.1324,  0.5198,  ...,  1.0464, -0.9780,  0.0426],\n          [-0.1032,  0.1052,  0.0733,  ..., -0.1005, -0.2119, -0.1483],\n          ...,\n          [ 0.5971, -0.0133, -0.1638,  ...,  0.7413, -0.2981,  0.5895],\n          [-0.3874,  0.2512,  0.1428,  ..., -0.1238, -0.3346,  0.1015],\n          [-0.5534,  0.6762, -0.0895,  ...,  0.5219, -1.0554, -0.1006]],\n\n         ...,\n\n         [[-0.0131,  0.0143, -0.0275,  ..., -0.0653, -0.0193,  0.0376],\n          [ 0.8291, -0.1804, -0.8852,  ..., -0.2940,  0.9659,  1.1397],\n          [-0.0343, -0.3794,  0.1225,  ..., -0.3974,  0.3114, -0.2152],\n          ...,\n          [ 0.2208, -0.3272, -0.4170,  ...,  0.4110, -0.2584,  0.1152],\n          [ 0.3429,  0.2526,  0.5178,  ...,  0.0889, -0.2551, -0.0285],\n          [-0.3247,  0.7592, -0.4231,  ..., -0.3010,  0.5201, -0.0014]],\n\n         [[-0.0173, -0.0193, -0.0596,  ..., -0.0225, -0.0123,  0.6400],\n          [-0.3333, -0.1722, -0.1961,  ...,  0.1566, -0.3654, -0.0344],\n          [ 0.1247,  0.0793,  0.1349,  ...,  0.2280,  0.2710, -0.4460],\n          ...,\n          [-0.0558, -0.2370,  0.1744,  ...,  0.4919, -0.3033, -0.4331],\n          [-0.0634,  0.1525,  0.0596,  ...,  0.4171,  0.0781, -0.6077],\n          [-0.1919, -0.1908,  0.0544,  ...,  0.5652, -0.0541, -0.7545]],\n\n         [[ 0.0253, -0.0222, -0.0151,  ...,  0.0164, -0.0272,  0.0803],\n          [ 0.2412,  0.8380, -0.4417,  ...,  0.6459, -0.6842,  0.2706],\n          [-0.2601,  0.0518, -0.1952,  ...,  0.2642, -0.4465,  0.4295],\n          ...,\n          [ 0.2313, -1.3138,  0.6014,  ..., -0.3423,  0.3664, -1.1917],\n          [ 0.5604,  0.0668,  0.1291,  ..., -0.2465,  0.0116, -0.3682],\n          [-0.4762,  0.0341,  0.3839,  ..., -0.5269, -0.5123,  0.2965]]]],\n       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.5414e-02,  4.6433e-02,  1.4184e-01,  ...,  5.4812e-01,\n           -4.8990e-01,  7.3753e-01],\n          [ 4.3876e+00,  5.3694e-01,  1.1721e+00,  ...,  1.3193e+00,\n           -1.3237e+00, -2.3558e-01],\n          [ 9.9249e-01,  3.1166e-02,  3.6171e-01,  ..., -1.5124e+00,\n           -2.4290e+00, -6.7962e-01],\n          ...,\n          [-1.1431e+00, -2.3096e+00, -3.1234e-01,  ..., -2.1145e+00,\n           -1.1770e+00, -4.5518e-01],\n          [-1.2081e+00, -1.9764e+00, -1.5585e-01,  ..., -2.3098e+00,\n           -2.8112e+00, -5.7975e-01],\n          [-1.8406e+00, -1.0024e+00, -2.8180e+00,  ..., -2.2056e+00,\n           -2.6257e+00, -4.7722e-01]],\n\n         [[ 1.2824e-02, -1.6541e-02,  1.6095e-03,  ..., -1.4238e-02,\n           -1.3309e+00,  2.2263e+00],\n          [-1.3956e+00,  8.9412e-02,  1.3159e-01,  ..., -4.4196e-01,\n            8.5787e-01, -9.0655e+00],\n          [-9.7274e-01, -5.4072e-01, -4.7400e-02,  ..., -3.0700e-01,\n            2.3624e+00, -5.6773e+00],\n          ...,\n          [ 1.3576e+00, -6.6439e-01,  1.1036e+00,  ..., -1.6988e+00,\n            3.5039e+00, -9.4972e+00],\n          [ 5.6010e-01, -9.3117e-01,  2.8272e+00,  ...,  5.8490e-01,\n            3.5013e+00, -5.3245e+00],\n          [ 5.9122e-01,  8.4591e-02,  2.3271e+00,  ...,  1.8021e+00,\n            3.4855e+00, -7.5563e+00]],\n\n         [[-1.3162e-01, -1.1779e-01, -8.8633e-02,  ...,  1.2191e+00,\n           -4.0260e-01, -9.9839e-01],\n          [-2.8954e+00, -3.4110e+00, -2.7247e+00,  ...,  4.0143e-01,\n           -2.4058e-01, -8.6029e-01],\n          [ 1.9712e+00, -1.7039e-02,  1.4560e-01,  ...,  1.4807e+00,\n           -6.6160e-01,  7.3650e-01],\n          ...,\n          [ 2.6792e+00,  3.1427e+00,  1.5565e+00,  ...,  1.3970e+00,\n            2.8025e-01,  4.6396e-01],\n          [-1.7699e+00,  3.4014e+00,  2.0562e+00,  ...,  1.0994e+00,\n            7.3417e-01, -1.7674e-01],\n          [-3.7179e+00,  2.6620e+00,  2.3894e+00,  ...,  1.0780e+00,\n            7.0768e-01,  8.1163e-01]],\n\n         ...,\n\n         [[ 2.6366e-02, -1.0544e-02,  6.2874e-04,  ...,  1.4033e-01,\n           -8.2465e-01, -3.7078e-01],\n          [ 1.6810e+00, -8.1752e-03, -1.3071e+00,  ...,  1.3426e+00,\n            2.8860e+00, -9.1175e-01],\n          [ 3.1420e+00,  1.8545e+00, -2.0833e+00,  ...,  2.5170e+00,\n            2.2813e+00, -2.3463e-01],\n          ...,\n          [-3.6326e+00,  2.8218e+00, -1.7481e+00,  ...,  4.2701e+00,\n            4.2890e+00, -3.0365e+00],\n          [-3.9798e+00,  1.4441e+00, -2.9004e-01,  ...,  4.0922e+00,\n            3.2314e+00, -1.7111e+00],\n          [ 4.9226e-01, -1.0174e+00, -2.0602e-01,  ...,  4.6128e+00,\n            3.7229e-01, -7.3213e-01]],\n\n         [[ 5.1096e-02,  1.2720e-01,  6.0000e-02,  ..., -1.7026e+00,\n            2.6214e-01, -1.8377e+00],\n          [-3.9598e+00,  1.7712e+00,  2.9831e+00,  ...,  2.2901e+00,\n           -7.9707e-01,  2.7464e+00],\n          [-2.6725e+00, -1.8800e-01,  2.2646e+00,  ...,  1.1920e+00,\n           -4.4188e-01,  2.6991e+00],\n          ...,\n          [ 1.1788e+00, -3.0626e+00,  8.2353e-01,  ...,  4.1268e+00,\n           -5.6293e-01,  2.0691e+00],\n          [ 3.1263e+00, -1.6974e+00, -2.8394e+00,  ...,  1.7192e+00,\n            1.1469e+00,  4.7347e+00],\n          [ 1.9080e+00, -1.7398e+00, -3.5397e+00,  ...,  3.0596e+00,\n            5.7570e-01,  1.9988e+00]],\n\n         [[ 1.9741e-02, -1.3131e-02,  1.8652e-02,  ..., -9.7016e-01,\n           -5.0637e-02,  4.6040e-01],\n          [-2.7769e-01, -5.1565e-01, -1.4570e-01,  ...,  2.3636e+00,\n            2.2137e+00, -1.0835e+00],\n          [-4.5133e-01,  1.6131e-01,  6.5179e-01,  ...,  3.1053e+00,\n            9.6139e-01,  3.5502e-01],\n          ...,\n          [ 3.0989e-01,  2.3544e-01, -5.7656e-01,  ...,  1.9273e+00,\n            8.0125e-01, -1.4189e+00],\n          [ 6.2809e-01,  1.4889e-01, -2.7151e-01,  ...,  3.2865e+00,\n            2.2598e-01, -1.6323e+00],\n          [ 6.7133e-02,  5.2969e-01, -7.0295e-01,  ...,  2.6760e+00,\n            4.5591e-02, -2.5868e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.4963e-02,  2.4116e-02, -1.4267e-02,  ...,  1.2166e-03,\n            2.0411e-02, -9.7046e-04],\n          [ 2.2547e-01,  1.4090e-01, -1.3826e-01,  ...,  2.4552e-02,\n           -2.0530e-01, -1.1754e-01],\n          [ 5.6882e-02,  1.7094e-01, -4.2025e-01,  ..., -9.0925e-02,\n           -1.7636e-01,  2.0896e-01],\n          ...,\n          [-2.9027e-01,  9.1443e-03, -4.6778e-01,  ...,  2.0404e-02,\n           -7.6829e-01, -8.6308e-02],\n          [-4.3335e-01,  3.2009e-01, -6.4599e-01,  ...,  2.0640e-01,\n           -6.1962e-01,  6.1385e-02],\n          [-3.5915e-01,  2.0407e-01, -4.2703e-01,  ...,  5.8975e-02,\n           -4.5818e-01,  4.4285e-01]],\n\n         [[ 3.4210e-02, -3.4803e-03, -1.2506e-03,  ..., -2.4340e-02,\n           -3.4289e-02,  4.0968e-02],\n          [-2.5548e-02,  8.6896e-01, -3.9595e-01,  ...,  5.3780e-01,\n           -2.1199e-01,  1.3437e+00],\n          [-1.8097e-01,  2.0235e-01,  3.2570e-02,  ..., -9.3057e-02,\n            2.3465e-01,  3.6229e-01],\n          ...,\n          [-5.9932e-02,  6.5331e-01,  2.2758e-01,  ..., -8.5440e-02,\n           -1.8942e-02,  5.7877e-01],\n          [ 1.5629e-01,  3.0888e-01,  1.4111e-01,  ...,  3.8784e-01,\n            3.4181e-01,  3.7496e-01],\n          [ 3.0771e-01, -1.3075e-01,  2.9143e-01,  ..., -2.0617e-01,\n           -3.7792e-01,  2.2385e-01]],\n\n         [[ 9.4470e-02,  1.9155e-01,  3.5195e-02,  ..., -3.0459e-02,\n            3.4772e-01, -7.3498e-03],\n          [-5.1428e-01,  3.0442e-01,  4.2104e-01,  ..., -5.7540e-01,\n           -1.5479e+00,  3.6643e-01],\n          [ 7.6056e-02,  1.6991e-01,  4.9027e-01,  ...,  1.1887e-01,\n           -2.2015e+00,  4.4847e-01],\n          ...,\n          [ 1.4213e-01,  5.0866e-01,  1.2624e-01,  ..., -2.8348e-01,\n           -2.0461e+00,  2.5732e-01],\n          [ 1.4330e-02, -2.6393e-01,  4.7206e-01,  ..., -2.3471e-01,\n           -2.9259e+00, -3.8417e-01],\n          [-1.4677e-01,  2.3432e-01,  1.5036e-02,  ..., -2.7339e-01,\n           -2.3934e+00,  2.3624e-02]],\n\n         ...,\n\n         [[-8.7270e-02, -6.6465e-02, -3.0371e-02,  ...,  1.7711e-02,\n            6.4740e-02, -3.5456e-03],\n          [-6.0243e-01,  3.2345e-02, -9.2731e-01,  ..., -2.6678e-01,\n            2.1238e+00, -8.8268e-01],\n          [-2.0218e-02, -2.0306e-01, -2.8669e-01,  ...,  4.7327e-01,\n           -2.6078e-01,  7.2375e-03],\n          ...,\n          [ 3.9089e-01,  1.1831e+00,  5.6502e-01,  ..., -7.5588e-01,\n           -7.9102e-01,  7.6052e-01],\n          [-1.4055e+00, -6.4177e-01, -5.4610e-01,  ..., -6.1377e-01,\n            5.9486e-02,  8.7491e-01],\n          [ 9.0679e-01,  5.2043e-02,  8.9971e-01,  ...,  3.3745e-01,\n            1.2728e+00, -7.4647e-02]],\n\n         [[ 2.7315e-02,  9.0491e-02, -7.3440e-02,  ..., -2.2172e-03,\n            4.7945e-02,  2.3947e-02],\n          [-4.5239e-02, -4.2668e-01,  4.3743e-01,  ..., -2.0161e-01,\n           -7.7504e-02, -3.8835e-01],\n          [-2.5332e-01, -1.4399e-01, -8.7746e-02,  ...,  2.9873e-01,\n           -1.4090e-01,  1.6136e-01],\n          ...,\n          [ 3.7944e-01,  4.1455e-01,  3.3392e-01,  ..., -2.2405e-01,\n           -4.0171e-01,  1.4341e-01],\n          [-2.0055e-01,  5.0283e-01, -2.5512e-01,  ..., -1.3557e-01,\n           -3.7245e-01, -1.0968e-01],\n          [ 8.1803e-02, -1.1857e+00,  8.5675e-01,  ..., -2.0428e-01,\n            1.8285e-01, -1.6003e-02]],\n\n         [[ 2.5544e-02, -4.4024e-02, -3.5656e-02,  ...,  9.9875e-03,\n            5.0772e-02,  1.3128e-03],\n          [-8.6449e-01, -1.1419e+00,  1.4385e+00,  ..., -2.5004e-01,\n           -1.8690e-01,  6.5207e-01],\n          [-2.7724e-01,  1.3995e-01,  2.3331e-01,  ..., -2.2554e-01,\n            2.5031e-01,  4.4207e-02],\n          ...,\n          [-9.7353e-02, -1.2743e-01,  3.8092e-01,  ..., -1.7040e+00,\n            1.8006e-01,  6.6705e-01],\n          [ 9.8546e-01,  2.8762e-01, -2.3287e-01,  ..., -2.0138e-01,\n           -2.4919e-01,  6.5736e-02],\n          [-1.5538e+00,  9.0598e-01,  6.9267e-01,  ...,  6.6153e-02,\n            3.3056e-01, -3.1213e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m traced_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2-1b-quantized.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:1000\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    994\u001b[0m     check_if_torch_exportable,\n\u001b[1;32m    995\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[1;32m    996\u001b[0m     log_torchscript_usage,\n\u001b[1;32m    997\u001b[0m )\n\u001b[1;32m    999\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1000\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_export\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TS2EPConverter\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:695\u001b[0m, in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:1275\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of BaseModelOutputWithPast(last_hidden_state=tensor([[[ 1.5326, -1.3593,  2.4385,  ..., -1.0574,  0.4010, -1.1256],\n         [-1.1032,  3.1969,  3.7620,  ..., -5.2792, -4.0093,  2.8944],\n         [ 1.5108,  1.3452, -0.2563,  ..., -5.3894, -6.9593, -0.1786],\n         ...,\n         [ 1.4442,  1.9402,  1.1227,  ..., -3.1643, -6.0105, -2.4108],\n         [ 3.6687,  4.9128,  1.4363,  ..., -3.1012, -4.0978, -0.2268],\n         [-0.4561,  4.2190,  2.2034,  ..., -2.9271, -3.0164, -0.9648]]],\n       grad_fn=<MulBackward0>), past_key_values=((tensor([[[[ 9.5610e-02,  1.7827e-01,  3.6755e-02,  ..., -1.7652e+00,\n            1.7963e+00,  7.9892e-03],\n          [-1.0441e+00, -1.9604e+00, -1.3867e+00,  ...,  6.3562e-01,\n           -2.3245e+00, -2.2302e+00],\n          [-4.0632e+00, -8.6298e-01, -3.0124e+00,  ...,  2.7366e+00,\n           -1.4887e+00, -2.8741e-01],\n          ...,\n          [-1.1385e+00, -3.3855e+00, -2.5555e+00,  ...,  1.7879e+00,\n           -1.7340e+00, -2.0997e+00],\n          [ 3.8586e+00, -3.0442e+00, -1.6802e+00,  ...,  2.6446e+00,\n           -1.3799e+00, -2.4386e+00],\n          [ 6.2070e+00, -9.6388e-02, -1.1381e+00,  ...,  1.5129e+00,\n           -1.4761e+00, -1.9522e+00]],\n\n         [[ 1.3574e-03, -6.1648e-03,  1.7492e-02,  ...,  1.3465e+00,\n           -3.6406e-01,  2.4415e-01],\n          [ 4.1891e+00, -2.6343e+00,  2.6864e+00,  ...,  1.2204e+00,\n            2.0671e+00, -2.0118e+00],\n          [ 4.8540e-01, -4.4151e-01,  4.9244e-01,  ..., -2.1734e-01,\n            1.3597e+00, -7.4817e-01],\n          ...,\n          [-2.6706e+00,  6.8963e-01,  2.3554e+00,  ...,  3.0610e-01,\n            1.7619e+00, -1.2003e+00],\n          [-2.0480e+00,  3.3310e-01,  1.2774e+00,  ..., -3.3540e-01,\n            2.1050e+00, -2.6796e+00],\n          [-1.8765e+00,  2.3173e+00,  6.0927e-01,  ..., -4.6700e-01,\n            1.8176e+00, -2.1536e+00]],\n\n         [[ 9.4520e-03, -1.5944e-02, -2.4548e-02,  ...,  1.3063e+00,\n            2.7019e+00, -1.7912e+00],\n          [ 8.7218e-01, -1.0578e+00, -1.5827e+00,  ...,  3.3310e-02,\n           -3.7639e-01,  4.6344e-01],\n          [-1.4165e-01, -3.0495e-01,  3.4495e-02,  ..., -3.5958e-01,\n           -1.0394e+00,  6.4655e-01],\n          ...,\n          [-5.6807e-01,  8.5252e-01, -7.0232e-01,  ..., -1.0918e-01,\n           -2.6005e+00,  7.6607e-01],\n          [-8.6511e-02,  3.2448e-01, -1.0111e+00,  ..., -1.8479e+00,\n            6.0253e-01,  1.4022e+00],\n          [ 1.2307e+00, -1.5465e-02,  7.3055e-01,  ..., -4.3984e-01,\n           -1.7431e+00, -2.3357e+00]],\n\n         ...,\n\n         [[ 1.8422e-01,  1.7773e-01,  2.1953e-02,  ...,  1.0377e+00,\n           -1.6470e+00,  1.0830e+00],\n          [-2.4002e+00,  1.6295e+00, -1.3280e+00,  ...,  4.8186e-02,\n            1.6535e+00, -4.3970e-01],\n          [ 3.0804e-01, -3.1984e-01, -3.4331e-01,  ...,  1.0190e+00,\n           -1.2367e+00, -1.4422e+00],\n          ...,\n          [ 5.7487e-01, -2.2988e-01, -5.5945e-02,  ..., -1.1135e+00,\n            4.8710e-01, -1.2589e+00],\n          [-9.0519e-01,  5.3804e-01,  8.3072e-01,  ..., -1.3570e+00,\n           -1.9944e-02, -1.3020e+00],\n          [-7.4401e-02, -2.7901e-01,  8.7861e-02,  ..., -1.0235e+00,\n            2.4862e-01, -9.2794e-01]],\n\n         [[-2.6273e-03,  2.6221e-03, -3.0507e-02,  ...,  1.1223e+00,\n            1.1003e+00,  1.8951e-01],\n          [ 4.7903e+00,  3.9686e+00,  1.0754e-01,  ...,  2.1310e-01,\n           -2.8108e-02, -8.7820e-01],\n          [ 1.0246e+00,  1.3096e+00,  1.4995e+00,  ..., -2.0850e+00,\n           -1.6414e-02, -1.2134e-01],\n          ...,\n          [-6.4768e+00, -1.2898e+00, -1.7894e+00,  ..., -3.3122e+00,\n           -6.5641e-01, -2.1036e+00],\n          [-8.0486e-01, -8.0386e-01, -6.5649e-01,  ..., -1.9635e+00,\n            3.5160e-02, -2.1034e-01],\n          [-3.8587e-01, -4.6915e+00, -1.9725e+00,  ..., -4.0251e+00,\n           -1.0439e+00, -2.0460e+00]],\n\n         [[-1.6762e-02,  9.5977e-02, -5.1725e-02,  ...,  1.3930e+00,\n            7.2637e-01, -1.7956e+00],\n          [-9.7841e-01,  3.2913e+00, -1.2614e+00,  ..., -1.5560e+00,\n           -1.1715e+00,  1.6520e+00],\n          [ 1.6378e-01, -3.6365e-01, -1.2429e+00,  ...,  3.4448e-01,\n            1.0371e+00, -4.1897e-01],\n          ...,\n          [-1.8854e-01, -5.7922e-01,  4.0452e-01,  ..., -7.6429e-01,\n           -6.2004e-01,  8.1454e-01],\n          [ 3.4380e-01, -1.2955e+00, -1.9310e+00,  ...,  2.8621e-02,\n            1.0520e+00,  5.6551e-01],\n          [ 1.8523e+00, -2.8961e+00,  1.2598e+00,  ..., -4.3885e-01,\n           -8.9879e-01,  7.6234e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.5795e-03,  1.3599e-02, -4.5031e-02,  ...,  3.4113e-04,\n           -2.8627e-03,  1.1434e-01],\n          [-9.9041e-02, -6.0634e-02, -2.1137e-02,  ...,  3.0084e-01,\n            2.7410e-02, -2.8304e-01],\n          [ 5.0372e-02, -7.0389e-03, -5.9483e-03,  ...,  1.2424e-02,\n            9.2066e-02, -1.9506e-02],\n          ...,\n          [ 9.5086e-02, -2.9229e-04,  5.0430e-03,  ...,  3.8130e-02,\n           -3.3565e-02, -8.2930e-03],\n          [ 2.4362e-02, -2.1318e-02, -3.0878e-02,  ..., -6.8150e-02,\n            8.7928e-02,  7.7262e-02],\n          [ 3.4411e-02, -1.3991e-03, -2.1157e-02,  ..., -3.8938e-02,\n            6.3289e-02, -5.3524e-03]],\n\n         [[ 4.5735e-04,  2.5294e-04, -4.7072e-05,  ..., -4.1792e-04,\n            5.8174e-05, -1.2571e-03],\n          [-6.8077e-02,  4.3200e-02, -1.6258e-01,  ...,  1.6611e-01,\n            1.8847e-01,  1.0549e-01],\n          [-1.6937e-03, -3.3735e-03, -7.3070e-03,  ...,  7.0302e-03,\n           -5.8456e-03,  4.7597e-03],\n          ...,\n          [ 2.0076e-01, -6.8954e-02,  1.4551e-02,  ..., -4.2534e-02,\n            1.5192e-01,  3.5590e-01],\n          [-6.8439e-02, -1.4327e-01,  2.9145e-03,  ...,  2.8026e-02,\n            8.6677e-02, -1.3608e-01],\n          [-1.7211e-01,  4.3724e-02, -4.5069e-02,  ..., -1.0670e-02,\n            4.1900e-02, -3.6702e-02]],\n\n         [[-4.7917e-03, -3.6565e-03,  3.6446e-04,  ..., -3.5328e-04,\n           -6.5906e-04, -9.0832e-04],\n          [-9.4917e-02, -2.8329e-02,  4.3898e-02,  ...,  3.1837e-02,\n           -3.1751e-02, -6.8372e-02],\n          [-1.9278e-02, -3.1923e-02, -7.9513e-03,  ..., -4.0204e-02,\n            1.9414e-03, -1.5249e-02],\n          ...,\n          [ 2.0179e-01, -1.4612e-02,  3.0915e-02,  ...,  7.6138e-03,\n           -8.9133e-02,  4.7802e-02],\n          [-5.1400e-02,  5.7345e-02,  7.4082e-02,  ..., -2.9667e-02,\n           -1.5746e-02,  4.2932e-02],\n          [ 1.4780e-01,  4.6155e-03,  4.9988e-02,  ...,  6.1138e-02,\n            9.1901e-03,  1.6400e-02]],\n\n         ...,\n\n         [[-3.7790e-03,  6.0383e-04, -3.2364e-03,  ..., -5.6062e-04,\n           -1.4380e-04,  4.3967e-04],\n          [ 1.7510e-02, -3.3539e-02, -3.3570e-02,  ..., -1.7791e-02,\n           -4.1592e-03,  1.0626e-02],\n          [-1.4188e-01, -2.0370e-02, -3.2847e-02,  ...,  1.2244e-02,\n            6.5973e-03,  2.4519e-02],\n          ...,\n          [ 2.2899e-02, -7.2733e-02,  5.6826e-02,  ...,  2.8298e-02,\n            2.9018e-02, -1.1067e-03],\n          [ 3.3121e-03,  2.2296e-02, -5.5816e-02,  ...,  2.6916e-02,\n           -3.7854e-02, -2.1149e-02],\n          [ 6.6373e-02, -1.4837e-02, -1.9332e-03,  ..., -4.1016e-02,\n           -2.2104e-02,  1.5020e-02]],\n\n         [[-5.6312e-04, -1.6511e-04,  4.0350e-03,  ...,  8.4011e-05,\n           -1.4644e-03, -8.6666e-04],\n          [ 1.0925e-01,  5.0212e-02,  1.4440e-01,  ...,  6.7587e-03,\n           -1.4874e-02, -1.4358e-01],\n          [-7.5719e-03,  5.1073e-03,  1.8936e-02,  ...,  1.7475e-03,\n           -3.7511e-03,  1.0725e-02],\n          ...,\n          [-6.6414e-02,  5.5474e-02, -1.0623e-01,  ..., -7.9432e-02,\n            4.9748e-02, -1.3237e-01],\n          [ 5.7485e-03,  4.3640e-03, -2.1794e-02,  ...,  6.7570e-03,\n            3.4369e-03,  7.0808e-03],\n          [ 1.9519e-01,  4.5070e-02,  1.5320e-02,  ...,  3.4810e-02,\n           -5.0381e-02, -1.9583e-01]],\n\n         [[ 5.6741e-03,  2.7614e-03,  1.6247e-03,  ...,  2.1552e-05,\n            5.5974e-04, -1.0744e-03],\n          [ 3.1769e-02, -1.8088e-01,  7.9264e-03,  ...,  3.3389e-02,\n           -7.1930e-02,  6.4582e-02],\n          [ 7.2849e-03,  2.0841e-03, -1.0420e-03,  ...,  1.5843e-02,\n           -1.1953e-02,  9.6781e-03],\n          ...,\n          [-1.5549e-01,  1.2683e-01, -1.9559e-02,  ..., -5.6759e-02,\n            3.0501e-04,  7.7877e-02],\n          [ 1.7493e-03,  7.1972e-02,  4.0724e-02,  ..., -1.5790e-01,\n           -8.9693e-02, -8.0710e-02],\n          [-1.0907e-02, -1.9946e-01, -1.1967e-01,  ..., -5.7081e-02,\n           -4.5260e-02,  4.0982e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.7967e-02, -1.5538e-01, -3.5137e-02,  ..., -4.5287e-01,\n           -9.7894e-02,  9.9902e-01],\n          [ 2.1435e+00, -1.1052e+00,  2.9003e-01,  ...,  3.4048e-01,\n           -1.8103e-01, -4.0469e+00],\n          [-1.1528e+00,  8.0551e-02, -3.3165e+00,  ..., -4.6863e-01,\n            1.6778e-02, -3.6705e+00],\n          ...,\n          [-2.0375e+00,  2.7392e+00, -9.9266e-01,  ...,  3.6336e-01,\n            3.0317e-01, -3.8284e+00],\n          [-1.0166e+00,  2.8167e+00, -1.3171e+00,  ..., -1.0825e-01,\n           -2.5986e-01, -3.9744e+00],\n          [ 2.1676e+00,  1.0250e+00,  1.3725e-01,  ..., -1.1101e+00,\n            4.4459e-01, -4.0472e+00]],\n\n         [[ 2.0526e-01, -1.0120e-01,  2.8459e-02,  ...,  8.7780e-01,\n           -3.6544e-01, -9.6916e-01],\n          [ 1.4555e+00, -5.0022e+00, -2.4639e+00,  ..., -1.7220e+00,\n            1.2669e+00,  2.8843e+00],\n          [-1.7542e+00, -3.8467e+00, -3.0021e+00,  ..., -1.2341e+00,\n           -2.3595e+00,  3.0494e+00],\n          ...,\n          [-2.5242e+00, -5.1299e-01, -2.8127e+00,  ..., -3.1357e+00,\n            6.3081e-01,  2.5451e+00],\n          [ 3.9007e+00,  1.7564e+00, -1.6855e+00,  ..., -1.6215e+00,\n            1.2152e-01,  3.5846e+00],\n          [ 3.4854e+00,  3.3406e+00, -2.1783e+00,  ..., -2.6598e+00,\n           -1.6483e+00,  2.4277e+00]],\n\n         [[-2.4163e-02, -1.3793e-02, -1.5605e-02,  ..., -3.3829e-01,\n           -2.1568e-01, -2.7395e-01],\n          [-1.6232e+00, -9.9561e-01, -2.0859e+00,  ...,  7.0393e-01,\n           -1.0576e+00, -9.0824e-01],\n          [-7.1267e-02, -2.0515e+00, -1.4744e+00,  ...,  5.6114e-01,\n            5.6179e-01,  4.5942e-01],\n          ...,\n          [ 2.2636e+00, -3.5167e-03, -1.3527e+00,  ...,  6.2748e-01,\n           -4.1023e-01,  6.6814e-01],\n          [ 7.4718e-01,  8.5385e-01, -5.1241e-01,  ...,  6.5449e-01,\n           -2.8959e-01,  1.1189e+00],\n          [-5.5142e-01,  2.2874e+00,  1.1139e-03,  ..., -6.2647e-01,\n           -1.2708e+00,  1.7367e-01]],\n\n         ...,\n\n         [[ 1.8175e-01, -6.8501e-02,  9.8065e-02,  ...,  5.6903e-01,\n            6.9947e-01,  3.4793e-01],\n          [ 1.2064e+00, -1.8463e-01,  2.7027e+00,  ..., -5.7690e+00,\n           -3.5259e+00, -2.6167e+00],\n          [-4.0773e+00,  1.5611e+00,  1.7085e+00,  ..., -1.9952e+00,\n           -4.0612e+00, -4.2319e+00],\n          ...,\n          [-2.2839e+00,  3.9910e+00, -5.4121e-01,  ..., -4.6933e+00,\n           -5.7405e+00, -2.9257e+00],\n          [ 2.5067e+00,  1.7773e+00, -1.2392e+00,  ..., -6.0192e+00,\n           -5.4825e+00, -4.3812e+00],\n          [ 5.1151e+00, -6.7553e-02, -2.5948e+00,  ..., -3.0317e+00,\n           -4.5868e+00,  2.0757e+00]],\n\n         [[ 2.4641e-01, -7.5120e-04,  1.4931e-01,  ...,  4.2755e-02,\n            2.9609e-01, -5.9467e-01],\n          [ 2.2968e+00, -1.0072e+00,  2.1115e+00,  ..., -2.2388e+00,\n            1.3980e+00,  2.3438e+00],\n          [-2.3381e+00, -3.5296e+00,  1.1245e+00,  ...,  1.0777e+00,\n            8.0039e-02,  3.0540e+00],\n          ...,\n          [-4.5341e+00, -1.4650e+00, -1.5961e+00,  ...,  1.7733e-01,\n           -1.4686e+00,  4.1432e+00],\n          [ 7.0741e-01,  1.2569e+00, -2.8351e+00,  ..., -1.0399e+00,\n           -7.8148e-01,  3.2712e+00],\n          [ 6.3685e+00,  2.0896e+00, -4.8673e+00,  ...,  7.3676e-01,\n           -1.1852e+00,  4.6863e+00]],\n\n         [[ 6.5294e-02,  5.4152e-03,  9.5116e-02,  ..., -2.6992e-01,\n            1.3247e-01, -1.0223e+00],\n          [ 1.0522e+00,  1.3931e+00,  1.9404e+00,  ..., -1.5272e+00,\n            1.4662e+00,  1.2632e+00],\n          [ 1.4162e-01,  8.8900e-01,  3.2562e-01,  ...,  4.7265e-01,\n            1.1191e+00,  1.5841e+00],\n          ...,\n          [-9.9284e-01,  5.0924e-01, -1.1408e+00,  ...,  2.1557e+00,\n           -1.8703e-01,  7.6806e-01],\n          [ 1.0320e-01,  1.1855e-01, -1.0493e+00,  ...,  2.0507e+00,\n            1.0512e+00,  7.7381e-01],\n          [ 1.9356e+00, -2.4486e+00, -6.1142e-01,  ..., -7.0353e-01,\n           -8.7412e-01,  8.5476e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-3.2377e-03,  3.6782e-03, -2.0835e-03,  ...,  2.1884e-03,\n           -1.7065e-03, -1.4043e-02],\n          [-6.8249e-02, -9.0891e-02,  7.4523e-02,  ...,  4.0324e-02,\n            1.1355e-01,  8.2261e-02],\n          [-1.2549e-01,  8.1891e-02, -1.0486e-01,  ..., -2.0578e-02,\n           -8.8899e-02,  1.4754e-01],\n          ...,\n          [ 3.4477e-01,  2.5028e-02, -1.5197e-02,  ..., -1.4269e-01,\n            2.6218e-01,  6.2984e-02],\n          [-1.9109e-01, -1.9266e-02,  1.1804e-01,  ..., -3.0260e-01,\n            2.0018e-01,  3.0816e-01],\n          [-1.1212e-01, -8.4712e-02, -8.4004e-03,  ..., -1.1759e-01,\n            6.7740e-02,  6.6850e-02]],\n\n         [[ 4.1326e-04,  1.7973e-03, -3.2491e-03,  ..., -1.8416e-03,\n           -2.1346e-03, -5.6098e-03],\n          [ 7.9583e-02, -1.7815e-01, -5.7443e-03,  ...,  2.3471e-01,\n            1.9975e-01,  7.2629e-02],\n          [ 2.4855e-02,  1.8370e-01, -4.9052e-02,  ..., -9.4083e-02,\n           -9.4262e-03, -1.7727e-02],\n          ...,\n          [ 2.2283e-01,  5.1064e-02,  8.6225e-02,  ..., -5.5915e-02,\n            3.2762e-01,  1.3147e-01],\n          [-6.8345e-02,  6.0235e-02,  1.0391e-01,  ...,  4.2214e-02,\n            2.4259e-02, -2.2577e-01],\n          [-1.5042e-02,  8.3515e-02, -3.7921e-02,  ..., -2.3938e-01,\n            1.6055e-01, -5.0694e-02]],\n\n         [[ 7.5014e-03, -3.1590e-03,  1.1861e-02,  ...,  9.1303e-03,\n            8.1564e-03, -1.5911e-01],\n          [ 6.1164e-02, -7.1458e-03, -6.6609e-02,  ...,  7.1957e-02,\n           -2.3087e-02,  5.9988e-01],\n          [-2.5318e-01, -1.0410e-01, -5.7920e-02,  ...,  2.6127e-01,\n            3.5043e-02,  8.3340e-01],\n          ...,\n          [-3.3453e-02,  1.8735e-01,  6.7241e-02,  ...,  2.0223e-01,\n            1.4901e-01,  5.9556e-01],\n          [ 1.0063e-01,  1.1752e-01,  4.5007e-01,  ...,  6.4598e-02,\n            1.2316e-01,  3.3600e-01],\n          [ 7.8331e-02, -7.1940e-02, -2.5699e-01,  ...,  1.2051e-02,\n            2.1061e-01,  4.6554e-01]],\n\n         ...,\n\n         [[-8.6541e-03, -3.8981e-03, -2.6316e-02,  ...,  1.9426e-02,\n            1.4738e-03,  3.2282e-03],\n          [-5.9135e-02, -9.9246e-02, -3.6900e-01,  ...,  1.0190e-01,\n            2.3477e-01, -3.7322e-03],\n          [ 6.5881e-02, -9.7573e-02, -9.5627e-02,  ...,  9.0878e-02,\n           -1.9417e-02, -1.0951e-02],\n          ...,\n          [-1.9661e-01,  1.1670e-01, -2.6479e-02,  ...,  1.9701e-02,\n            2.5399e-02, -9.7587e-02],\n          [-1.2173e-01, -1.6699e-01,  9.9858e-02,  ...,  5.7304e-02,\n           -4.9886e-02, -4.7641e-02],\n          [ 1.9774e-01,  1.5135e-02, -5.3215e-02,  ...,  8.8475e-02,\n            1.5693e-01, -9.6357e-02]],\n\n         [[ 5.1806e-03, -5.2374e-03, -2.2343e-03,  ..., -1.5930e-03,\n            3.5463e-03, -4.3412e-03],\n          [-5.9287e-02,  7.1173e-02, -2.3751e-02,  ..., -2.0564e-01,\n            2.2858e-01,  3.2094e-01],\n          [ 8.5236e-02,  6.9737e-02,  6.1274e-02,  ..., -1.8012e-01,\n           -3.3260e-02,  2.3315e-02],\n          ...,\n          [ 1.1825e-02, -5.9067e-02, -1.0706e-01,  ..., -1.6356e-01,\n           -3.4370e-01,  2.4248e-02],\n          [ 4.9969e-02,  1.4826e-01, -3.9919e-02,  ..., -4.0458e-02,\n            6.6567e-02, -8.3417e-02],\n          [ 1.0430e-01, -3.2007e-02, -1.4930e-01,  ...,  3.6746e-01,\n            1.4962e-01,  1.6755e-01]],\n\n         [[-5.6988e-04, -8.3401e-03, -8.2436e-03,  ..., -1.5176e-02,\n            7.5245e-03, -3.7639e-03],\n          [ 1.8856e-01,  2.7176e-01, -1.5545e-01,  ..., -1.4014e-02,\n           -3.9068e-02,  1.3135e-01],\n          [-5.3303e-02,  1.8317e-02,  3.4295e-02,  ..., -9.3917e-02,\n            6.7584e-02,  3.9990e-02],\n          ...,\n          [-1.3570e-01,  1.3095e-01,  1.1496e-01,  ..., -5.9814e-01,\n            9.0253e-02,  1.7751e-01],\n          [ 3.8223e-02,  3.8875e-02, -1.8843e-01,  ..., -9.1675e-02,\n            5.0060e-02, -2.1880e-02],\n          [-4.1986e-01,  2.1757e-01, -1.4327e-01,  ...,  7.7314e-02,\n            9.4747e-02,  8.5389e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7701e-01,  1.2814e-01, -5.6786e-02,  ...,  1.3507e-01,\n           -1.4585e+00, -6.9457e-01],\n          [ 2.6764e-01,  2.9066e+00, -3.2170e+00,  ...,  8.7905e-02,\n            4.8198e+00,  2.8236e+00],\n          [ 9.2695e-01,  1.4156e+00, -7.8957e-01,  ...,  1.7659e-01,\n            3.0773e+00,  9.0030e-01],\n          ...,\n          [-2.9926e-01,  3.5457e-02, -1.4828e+00,  ..., -1.5730e-01,\n            4.5621e+00,  9.8181e-01],\n          [-6.7563e-01, -2.2247e+00, -1.5221e+00,  ...,  5.3594e-01,\n            4.2234e+00,  1.0064e+00],\n          [-2.3657e+00, -1.6536e+00, -1.2066e+00,  ..., -4.5254e-01,\n            5.0909e+00,  1.6524e+00]],\n\n         [[-6.3640e-02, -1.6707e-02,  1.4471e-02,  ...,  4.7435e-01,\n           -3.9084e-01, -2.5429e-01],\n          [ 1.1823e+00,  5.1502e-01, -4.1835e-01,  ...,  1.1009e-01,\n           -2.9313e-01, -8.9718e-01],\n          [ 1.3956e+00,  1.6913e+00, -8.9101e-01,  ...,  8.5906e-02,\n            7.4851e-01, -8.5068e-01],\n          ...,\n          [-1.0720e+00,  1.5869e+00, -1.4292e+00,  ...,  4.5084e-01,\n           -3.9096e-01, -2.0741e+00],\n          [-1.3015e+00, -2.1746e-01, -1.5711e+00,  ...,  8.3230e-01,\n            9.6939e-01, -7.3736e-01],\n          [-1.5033e+00, -8.2268e-01, -7.3206e-01,  ...,  1.1917e+00,\n           -1.2171e+00, -1.7185e+00]],\n\n         [[ 8.8415e-02, -7.7824e-02,  5.8160e-02,  ...,  3.3606e-01,\n           -1.3101e+00, -1.1014e+00],\n          [-2.3669e+00, -3.6616e+00, -1.2765e+00,  ...,  2.0766e+00,\n            3.3702e+00,  5.0247e+00],\n          [-3.1251e+00, -3.1575e+00, -7.4505e-01,  ...,  4.9128e-01,\n            1.8747e+00,  4.4647e+00],\n          ...,\n          [ 2.0000e+00, -6.1031e-01, -1.6045e+00,  ...,  7.2175e-01,\n            3.4087e+00,  3.7380e+00],\n          [ 3.2028e+00,  2.2939e+00, -4.2439e+00,  ...,  4.8598e-01,\n            2.9832e+00,  3.1867e+00],\n          [ 2.3772e+00,  2.5177e+00, -1.3139e+00,  ...,  1.4361e+00,\n            3.7860e+00,  4.9523e+00]],\n\n         ...,\n\n         [[-2.5395e-01, -1.4475e-01,  1.7906e-01,  ..., -1.9359e-01,\n           -3.9308e-01,  8.8821e-01],\n          [-2.6796e+00, -2.0157e+00,  3.1024e+00,  ...,  4.2189e-01,\n           -1.3306e+00, -4.9016e+00],\n          [ 1.6348e+00, -5.3055e-02,  1.9740e+00,  ..., -1.4344e+00,\n           -2.4934e-01, -3.7893e+00],\n          ...,\n          [ 2.8807e+00,  1.4479e+00, -1.4614e-03,  ..., -1.0776e+00,\n            2.9119e-01, -3.3255e+00],\n          [-1.1927e+00,  1.2245e+00, -1.0029e+00,  ...,  5.8190e-01,\n            1.8443e+00, -3.6302e+00],\n          [-3.7693e+00,  2.5642e+00, -1.8331e+00,  ..., -1.2635e+00,\n           -5.5469e-02, -3.2170e+00]],\n\n         [[ 1.0696e-02, -8.8757e-02, -1.3791e-01,  ..., -5.1921e-01,\n            2.0680e-01, -9.1169e-02],\n          [ 6.0183e-01, -8.7852e-01, -2.0071e+00,  ...,  3.2347e-01,\n            6.0076e-01, -6.7956e-01],\n          [ 1.1107e+00, -1.4100e-01, -1.3417e+00,  ...,  2.6395e+00,\n            1.8479e+00,  1.0224e+00],\n          ...,\n          [-1.5660e+00,  8.7282e-01,  9.3701e-01,  ...,  1.8857e+00,\n            8.2834e-01,  2.4639e-01],\n          [-1.4213e+00,  1.9002e+00,  7.0173e-01,  ...,  1.7888e+00,\n            1.6171e+00, -8.0769e-01],\n          [-1.2590e+00,  1.6183e+00,  1.4074e+00,  ...,  8.6553e-01,\n           -4.7060e-01,  5.8349e-01]],\n\n         [[ 7.1884e-02,  1.4028e-01,  6.8085e-02,  ...,  2.4312e-01,\n           -1.7492e-01,  2.5649e-01],\n          [ 5.1536e+00,  3.8639e+00,  2.2726e+00,  ...,  4.6500e-01,\n           -2.1876e+00, -8.3482e-01],\n          [ 2.9126e+00,  3.1976e+00,  3.2291e+00,  ...,  7.0219e-01,\n           -1.1357e-01,  1.1334e+00],\n          ...,\n          [-4.1209e+00,  2.7656e-01,  2.6617e+00,  ..., -3.8955e-01,\n           -2.3895e-01,  3.2122e+00],\n          [-2.8110e+00,  3.8436e-01,  1.9234e+00,  ..., -1.6735e+00,\n            7.6658e-01,  1.5494e+00],\n          [-1.6525e+00, -1.5763e+00,  6.6924e-01,  ..., -1.4195e+00,\n           -1.4014e+00,  4.7321e-02]]]], grad_fn=<AddBackward0>), tensor([[[[-6.1820e-03,  5.4064e-03,  8.8012e-03,  ...,  1.4157e-02,\n            5.4133e-03, -9.3104e-03],\n          [ 2.9941e-01,  6.7903e-01, -1.6327e-01,  ...,  8.3898e-02,\n            2.1157e-01, -7.8838e-02],\n          [ 3.1184e-01,  7.6078e-02,  3.5302e-01,  ..., -2.8107e-01,\n           -1.5467e-01,  4.1138e-02],\n          ...,\n          [-9.6306e-02,  6.9173e-02,  6.5667e-02,  ...,  2.6577e-01,\n           -6.4739e-02, -3.4857e-02],\n          [-2.4713e-01, -3.5561e-01, -1.4759e-01,  ...,  1.7052e-01,\n            7.5728e-02, -1.5410e-01],\n          [-2.3465e-01,  1.5675e-01,  2.8819e-01,  ...,  1.2776e-01,\n            6.8719e-02, -2.5834e-01]],\n\n         [[-1.2713e-02,  1.2328e-01,  2.5158e-03,  ..., -2.7015e-03,\n            9.6137e-03,  1.6953e-03],\n          [ 7.1951e-01, -1.6914e-01, -1.1247e-01,  ..., -4.8129e-01,\n            1.4451e-01, -6.2263e-02],\n          [-2.4158e-01, -5.0414e-02,  1.1081e-01,  ...,  4.6947e-02,\n           -1.9938e-01,  7.7349e-02],\n          ...,\n          [-1.9011e-01, -1.5957e-01,  1.9725e-01,  ...,  2.3459e-01,\n            1.6987e-01,  1.8029e-01],\n          [-3.1155e-01, -3.1968e-01, -2.1609e-01,  ...,  2.5294e-01,\n            6.0543e-02,  1.5184e-01],\n          [ 1.5025e-01,  1.5711e-01, -1.4395e-02,  ..., -1.5603e-01,\n            6.3068e-02,  3.9409e-01]],\n\n         [[ 7.1219e-03,  3.1417e-03, -1.0653e-02,  ...,  1.6981e-02,\n           -7.8259e-03,  4.3196e-03],\n          [-1.9960e-01,  2.7524e-01,  1.9772e-02,  ..., -1.6835e-01,\n            8.5904e-02, -2.8022e-01],\n          [-1.0467e-01,  3.5048e-02,  1.1350e-02,  ...,  5.4866e-02,\n           -1.1864e-01,  2.1198e-01],\n          ...,\n          [-1.8213e-01,  9.0273e-03, -1.1572e-01,  ..., -9.9041e-02,\n            8.4770e-02,  4.1519e-01],\n          [-1.2092e-01, -3.8780e-01, -3.6987e-01,  ..., -3.8170e-01,\n           -2.6693e-02,  3.6690e-01],\n          [-5.4785e-02, -3.9241e-01, -1.1562e-01,  ...,  4.1272e-01,\n            3.9609e-01,  1.8474e-01]],\n\n         ...,\n\n         [[ 3.3089e-02,  1.1034e-03, -6.1658e-03,  ..., -2.2012e-01,\n            2.6411e-03, -9.0748e-04],\n          [ 2.0938e-01, -6.8443e-01, -3.5323e-01,  ...,  1.0073e+00,\n            2.5970e-01,  2.7422e-01],\n          [-9.7218e-01, -6.1478e-01,  2.2379e-01,  ...,  1.2794e+00,\n            1.3615e-01, -8.1561e-01],\n          ...,\n          [-3.2407e-01, -5.6607e-01,  1.0611e-01,  ...,  1.0359e+00,\n           -1.1281e-01, -1.8238e-01],\n          [-2.6283e-01,  2.3867e-02, -5.1711e-01,  ...,  1.1837e+00,\n            1.3125e-01,  1.7411e-01],\n          [-3.9711e-01, -2.7209e-01, -1.6776e-01,  ...,  8.4565e-01,\n           -7.8791e-02,  3.2866e-02]],\n\n         [[ 5.5222e-03,  5.5187e-03, -2.4976e-03,  ..., -1.4439e-02,\n            2.8718e-03, -1.2763e-02],\n          [-9.6075e-02, -8.6414e-02,  7.9327e-02,  ...,  3.3177e-02,\n            5.1969e-02, -1.4187e-01],\n          [-7.6938e-02, -2.0906e-01,  1.4066e-01,  ..., -1.1291e-01,\n            1.5014e-01, -7.1458e-02],\n          ...,\n          [ 1.4898e-02, -6.8294e-02, -7.0356e-02,  ...,  2.5958e-02,\n           -4.2350e-02,  7.3146e-02],\n          [-1.7195e-01, -3.7946e-01, -6.4187e-02,  ...,  1.7663e-02,\n           -1.0420e-02,  1.2907e-01],\n          [-1.8034e-01,  2.7801e-03,  4.4255e-01,  ..., -2.3100e-01,\n           -4.6818e-02,  2.5140e-01]],\n\n         [[ 9.3314e-03, -1.0290e-02, -8.7502e-03,  ...,  1.6052e-03,\n            4.6391e-03, -5.5440e-04],\n          [ 3.4664e-01, -8.4540e-01,  1.8082e-01,  ..., -1.9415e-01,\n            1.3139e-01,  3.4814e-02],\n          [ 6.2430e-01,  1.2965e-01,  3.7513e-02,  ..., -2.3402e-02,\n            5.5743e-02,  9.9857e-02],\n          ...,\n          [ 1.9298e-01, -3.6481e-01,  2.9809e-02,  ...,  2.0951e-02,\n            6.1376e-02, -8.9263e-02],\n          [-1.6985e-01,  3.1712e-01, -2.7905e-01,  ..., -2.4076e-01,\n            5.8419e-02, -1.1336e-01],\n          [-1.5146e-01, -2.7222e-01,  1.6510e-03,  ..., -6.8705e-03,\n           -4.0594e-01,  1.8751e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.2282e-02, -1.0165e-01, -1.1627e-01,  ..., -1.1198e+00,\n            1.2367e+00,  9.2278e-01],\n          [ 2.6947e+00, -1.1263e+00, -2.5378e+00,  ...,  3.8508e-01,\n            7.6976e-01, -9.4456e-01],\n          [ 1.3896e+00, -5.4602e-01, -1.3699e+00,  ..., -3.5502e-01,\n            9.4520e-01, -1.5680e+00],\n          ...,\n          [-1.7908e+00,  9.5140e-01, -1.4063e+00,  ..., -2.5337e-01,\n            6.5877e-02, -5.4208e-01],\n          [-7.0496e-01,  1.6231e-01, -5.4211e-01,  ..., -6.9233e-01,\n            1.3365e+00, -2.8431e+00],\n          [-2.0937e-01, -1.1077e-01, -5.4180e-01,  ..., -4.4429e-01,\n            6.7587e-01, -9.7499e-01]],\n\n         [[ 2.0206e-01,  1.7876e-01,  2.1073e-01,  ..., -1.2444e-01,\n            6.5449e-01,  1.8805e-01],\n          [ 5.5494e+00,  2.7431e-01,  2.8697e+00,  ...,  7.7296e-01,\n           -4.0696e-01,  8.3838e-01],\n          [ 1.1737e+00,  3.4204e-03,  2.3668e+00,  ...,  3.9985e-01,\n            9.8103e-01, -7.5665e-01],\n          ...,\n          [-3.2539e+00, -3.5224e+00,  2.2123e+00,  ..., -1.0414e+00,\n           -5.9234e-01,  1.4053e+00],\n          [-8.2012e-01, -4.5371e+00,  3.9510e-01,  ...,  1.8237e+00,\n            1.3257e+00,  1.9354e+00],\n          [ 1.8927e+00, -2.3753e+00, -4.3274e-01,  ..., -7.4800e-01,\n           -4.4143e-01,  1.1267e+00]],\n\n         [[ 2.1973e-02,  7.3497e-02, -5.5480e-02,  ..., -1.8062e-01,\n            1.7539e+00, -1.5784e+00],\n          [ 3.3127e+00,  2.5819e+00, -2.4716e+00,  ...,  1.6049e+00,\n           -5.4206e+00,  6.3410e+00],\n          [ 2.7562e+00,  2.5899e+00, -3.3643e+00,  ..., -1.7875e-01,\n           -4.6014e+00,  6.0351e+00],\n          ...,\n          [-2.7383e+00,  8.6641e-01, -4.0272e+00,  ...,  6.0972e-01,\n           -5.1756e+00,  5.1306e+00],\n          [-2.2211e+00,  1.1909e+00, -2.4088e+00,  ...,  3.1355e-01,\n           -5.1743e+00,  4.1928e+00],\n          [-1.0697e+00, -1.9966e+00, -1.2744e+00,  ..., -1.0002e-01,\n           -5.1887e+00,  5.8612e+00]],\n\n         ...,\n\n         [[ 1.1266e-01, -1.1705e-01,  7.7527e-02,  ..., -5.3134e-01,\n           -1.2289e+00,  2.8861e-01],\n          [ 3.4821e-01, -2.3385e+00,  1.6136e+00,  ..., -1.4544e+00,\n            4.0061e-01, -3.1225e-01],\n          [-1.1731e+00, -4.6898e-02,  1.0132e+00,  ..., -1.2839e+00,\n           -1.2630e-01,  9.5137e-01],\n          ...,\n          [-2.0203e-01,  1.2111e+00, -1.6005e-02,  ..., -5.1681e-01,\n           -3.3450e-01, -2.1627e-01],\n          [ 1.1841e+00, -2.7654e-01,  1.8011e-01,  ..., -5.9574e-01,\n           -6.3698e-01,  3.0269e-01],\n          [ 1.0746e+00,  7.7070e-01, -9.6412e-01,  ..., -1.0072e+00,\n           -7.1167e-02, -1.6784e-01]],\n\n         [[-2.3820e-01, -1.1915e-01, -1.3883e-01,  ...,  1.2986e+00,\n           -1.2263e+00,  1.8063e+00],\n          [-1.6037e+00, -1.0995e+00, -2.6342e+00,  ..., -1.7757e+00,\n           -1.0154e+00, -2.1865e+00],\n          [ 1.9105e+00,  8.1207e-01, -7.8228e-01,  ..., -1.4002e+00,\n           -1.6378e+00, -2.3000e+00],\n          ...,\n          [ 9.5449e-01,  1.6893e+00,  1.3387e+00,  ..., -9.9793e-01,\n            3.8434e-01, -8.7498e-01],\n          [-1.4753e+00,  2.2700e+00,  1.4921e+00,  ...,  2.1434e+00,\n            1.2297e+00, -9.2150e-01],\n          [-2.7791e+00,  3.6354e-01,  2.2254e+00,  ..., -2.0619e+00,\n           -1.6319e+00, -2.4281e+00]],\n\n         [[ 1.3673e-01,  1.1688e-01,  1.1799e-01,  ..., -7.8946e-01,\n           -2.1710e+00,  1.2962e+00],\n          [ 1.4964e+00,  3.5646e+00,  6.2662e-01,  ..., -6.0104e-01,\n            4.9614e+00, -1.0345e-01],\n          [-9.5273e-01,  8.1729e-01,  6.4827e-01,  ...,  9.7054e-02,\n            7.1365e+00,  5.5422e-01],\n          ...,\n          [-7.5994e-01, -3.1559e-01,  3.6393e-01,  ..., -2.4653e-01,\n            5.8504e+00, -1.6054e-01],\n          [ 6.1265e-01, -4.1892e-01,  2.2466e-01,  ..., -3.9562e-01,\n            6.2402e+00,  2.3792e-01],\n          [ 1.0237e+00, -1.5125e+00, -1.0386e-02,  ...,  2.2863e-01,\n            5.2406e+00, -2.2171e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.4321e-03, -8.7532e-03, -4.7245e-03,  ...,  8.1068e-03,\n            8.7612e-04,  6.9124e-03],\n          [-1.0467e-01, -1.9611e-02,  2.4791e-02,  ..., -4.3568e-02,\n           -5.7890e-02,  2.3274e-01],\n          [-9.9035e-02, -3.5960e-01,  1.4438e-01,  ..., -2.2842e-02,\n            1.6327e-01,  3.1524e-01],\n          ...,\n          [-3.0338e-01, -7.6661e-03,  4.5093e-01,  ...,  1.9181e-01,\n            2.8696e-01, -9.1073e-02],\n          [-3.0866e-02, -6.2382e-02,  2.9084e-02,  ..., -1.5676e-01,\n           -1.7092e-01,  1.1898e-01],\n          [-2.6783e-01, -1.7185e-01,  1.7530e-01,  ...,  1.3997e-01,\n            9.0545e-02, -3.8462e-01]],\n\n         [[ 7.0619e-04, -3.2205e-03,  8.7327e-03,  ...,  1.4436e-03,\n            2.0123e-02,  9.0113e-04],\n          [ 1.2734e-02, -4.0825e-02,  4.3852e-03,  ..., -1.2760e-02,\n           -5.4517e-01, -2.6769e-01],\n          [-1.6712e-01, -1.6344e-01,  9.6333e-02,  ..., -6.8344e-03,\n           -2.1898e-02, -3.1479e-01],\n          ...,\n          [ 3.0523e-01,  2.7330e-01, -6.8375e-03,  ..., -1.5098e-01,\n           -1.0993e-01,  3.0583e-01],\n          [ 3.4018e-01, -7.7130e-02, -1.4879e-01,  ..., -3.0749e-02,\n            5.6093e-01,  1.9593e-01],\n          [ 1.0211e-01,  1.3157e-01, -1.3349e-01,  ...,  1.2502e-01,\n           -6.5941e-02, -1.1809e-01]],\n\n         [[-2.7866e-03,  1.0993e-02,  6.1155e-03,  ...,  1.5371e-02,\n            2.8720e-03, -2.3152e-02],\n          [-6.4073e-02,  1.5430e-02,  2.6343e-01,  ...,  8.0253e-02,\n            1.7867e-01,  1.2398e+00],\n          [ 5.1941e-02,  1.3583e-01, -1.6573e-01,  ..., -8.3535e-02,\n            1.3651e-01,  1.5384e-01],\n          ...,\n          [-1.2322e-01,  1.8373e-01, -8.2243e-02,  ...,  1.0742e-01,\n            3.1176e-01,  3.3107e-01],\n          [-1.7566e-02,  1.9896e-01, -1.6854e-02,  ...,  7.8815e-02,\n            2.5707e-01,  1.6920e-02],\n          [ 7.9629e-03,  1.1664e-02, -1.9343e-01,  ..., -2.2412e-02,\n            2.4916e-01,  3.5274e-01]],\n\n         ...,\n\n         [[-1.4118e-02, -3.8184e-03,  1.3744e-02,  ..., -4.0826e-04,\n           -4.0499e-03,  8.0336e-03],\n          [ 5.4029e-02, -1.0008e-01, -3.6924e-01,  ..., -5.0542e-01,\n           -1.2258e-01, -1.0312e-01],\n          [ 2.2759e-02, -7.9112e-02, -9.1752e-02,  ..., -3.1552e-01,\n           -1.4367e-01,  7.4229e-02],\n          ...,\n          [-2.3685e-01, -5.3028e-01, -7.6086e-02,  ..., -9.8673e-02,\n           -1.3463e-02,  2.3224e-01],\n          [-4.5401e-01,  1.1683e-01, -2.2798e-02,  ..., -6.0475e-01,\n           -3.5990e-01, -1.3324e-01],\n          [-1.8999e-01,  7.5736e-02,  2.8255e-02,  ..., -1.2506e-01,\n           -1.9876e-01,  2.6780e-01]],\n\n         [[-3.5600e-03,  2.4092e-04, -9.2962e-03,  ..., -1.4673e-02,\n            4.7458e-03, -1.1192e-02],\n          [-1.5482e-02,  1.1980e-01,  2.1448e-01,  ...,  6.4581e-02,\n           -1.7597e-01,  1.1077e-01],\n          [ 2.6037e-02,  7.4305e-02, -1.0224e-01,  ..., -6.9476e-02,\n           -9.4906e-02, -2.6536e-01],\n          ...,\n          [ 1.0273e-01,  8.0308e-02,  1.1856e-01,  ...,  1.3968e-01,\n            1.5410e-01, -1.1429e-01],\n          [ 1.1800e-01,  5.7852e-02,  1.4395e-01,  ...,  1.1984e-01,\n            1.3060e-01, -9.2328e-02],\n          [-1.5998e-01, -8.2837e-02,  8.2714e-02,  ..., -3.5956e-02,\n           -9.6886e-02, -8.8186e-02]],\n\n         [[ 5.8911e-03, -8.2265e-03,  5.3291e-03,  ...,  8.7417e-04,\n           -1.3503e-02, -1.0993e-02],\n          [ 2.8653e-03,  4.0489e-03, -2.4093e-01,  ..., -1.3937e-01,\n            2.8739e-01,  5.5181e-01],\n          [ 1.2024e-01, -1.2629e-02,  2.7387e-02,  ...,  1.7477e-01,\n            1.2676e-01,  1.6036e-01],\n          ...,\n          [ 5.3377e-02,  2.0845e-01,  4.0209e-03,  ...,  7.6788e-02,\n            5.8192e-01,  2.6301e-01],\n          [ 3.5664e-01, -8.8840e-02, -1.1054e-01,  ...,  7.6085e-03,\n            6.6291e-01, -1.2942e-01],\n          [ 2.7074e-02,  1.2920e-01, -1.4235e-01,  ...,  3.5492e-01,\n            7.7556e-02,  6.1752e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0673, -0.0380,  0.0587,  ..., -0.2401,  1.2458, -0.5890],\n          [-2.6230, -1.4096,  1.0336,  ..., -1.0997,  0.7870, -2.2670],\n          [ 0.0684, -0.8596, -0.4222,  ...,  0.0686,  1.8364, -1.7942],\n          ...,\n          [ 0.4360,  0.0993, -0.3778,  ...,  0.2330,  0.4170, -1.8798],\n          [-0.4057,  0.0715, -0.6293,  ...,  0.2380,  1.6907, -3.2166],\n          [-0.4575,  0.9622, -0.8712,  ...,  0.7507,  0.7713, -1.9774]],\n\n         [[-0.1482,  0.1215, -0.1029,  ..., -1.0353, -1.0146,  0.8210],\n          [-2.7964,  1.4028, -2.8288,  ..., -0.8176, -0.8423,  1.3311],\n          [ 1.2582,  0.1732, -1.6765,  ..., -0.2601, -0.9366,  1.1228],\n          ...,\n          [ 1.3964, -1.2859,  0.7055,  ...,  0.5220, -1.2482, -0.1308],\n          [-1.7408, -1.5983,  0.9078,  ...,  0.0471, -0.5481,  0.2924],\n          [-1.7288, -1.0642,  1.4368,  ...,  0.5570, -1.4549, -0.8904]],\n\n         [[ 0.0888, -0.0451, -0.0820,  ...,  1.2527, -1.6093,  0.8153],\n          [ 1.1792, -1.2164, -3.3174,  ...,  1.8395,  2.3515, -0.6136],\n          [-1.5866,  0.1771, -1.9733,  ...,  1.0520,  2.0242, -0.3927],\n          ...,\n          [-1.0689,  1.1365, -0.1952,  ...,  0.5337,  1.3976, -0.5168],\n          [ 0.3324,  1.2364, -0.5633,  ...,  0.5687,  2.1237, -0.3122],\n          [ 2.0921,  0.1781,  0.9043,  ...,  1.3126,  1.3001,  0.6655]],\n\n         ...,\n\n         [[ 0.1052, -0.0565,  0.0683,  ..., -0.1388,  0.3983,  1.4133],\n          [ 3.4661, -1.0068,  1.4672,  ..., -0.7612, -0.6297, -3.9924],\n          [ 0.2750,  0.7824,  0.9173,  ..., -0.4139,  0.6396, -3.5821],\n          ...,\n          [-1.0323,  0.2570, -0.7744,  ..., -0.0266,  0.3943, -1.8256],\n          [ 0.6397,  0.2168, -0.2871,  ..., -0.5229,  0.9866, -2.3851],\n          [ 0.4325,  0.0755, -0.7504,  ..., -0.3195,  1.3672, -1.1089]],\n\n         [[ 0.1472,  0.0827,  0.0479,  ...,  0.3101,  0.0898, -0.3013],\n          [ 1.7290,  1.5935,  0.6793,  ...,  0.7582,  1.3635,  0.5977],\n          [-0.9900,  0.1284,  1.2406,  ...,  0.2236, -0.4528, -1.5582],\n          ...,\n          [-0.7661, -0.5840,  2.1097,  ..., -0.0846,  0.3760,  0.1993],\n          [ 0.5308, -1.5368,  1.5667,  ..., -0.7289, -0.2877, -1.1430],\n          [ 1.6712, -0.8101,  1.7596,  ...,  1.8957,  0.6416,  0.1783]],\n\n         [[-0.1321,  0.1387, -0.0576,  ..., -0.8415,  1.0974,  0.2400],\n          [ 0.5177,  3.2533, -1.6478,  ..., -0.1231, -0.1318,  1.0980],\n          [ 2.1462,  0.3267, -2.0895,  ..., -0.5722,  0.1243,  0.8788],\n          ...,\n          [ 0.0263, -0.7484, -1.8913,  ..., -1.8921, -0.5946,  1.4830],\n          [-1.3986, -0.6276, -1.3345,  ..., -1.0882,  1.3606,  1.5931],\n          [-1.7844, -1.3445, -0.5183,  ..., -0.7938, -0.0828,  1.6731]]]],\n       grad_fn=<AddBackward0>), tensor([[[[ 7.0184e-03, -2.6607e-01,  8.9918e-03,  ...,  1.0008e-02,\n            9.8950e-03,  2.1153e-02],\n          [-1.7189e-02,  7.4637e-01,  7.3324e-02,  ...,  1.7711e-01,\n            1.8271e-01, -1.9652e-01],\n          [ 4.3036e-01,  4.6241e-01, -2.1777e-02,  ..., -5.7014e-02,\n           -4.1061e-01, -3.6297e-01],\n          ...,\n          [ 1.1786e-01,  3.7584e-01, -1.6500e-01,  ...,  4.3060e-01,\n            1.1172e-01, -3.4991e-01],\n          [ 1.5505e-01,  6.0234e-01, -2.4335e-03,  ..., -1.4892e-01,\n            1.3472e-01, -1.7865e-01],\n          [ 8.8841e-02,  5.1718e-01, -3.3933e-01,  ...,  2.1001e-01,\n           -2.9975e-02, -1.9431e-01]],\n\n         [[-5.5726e-04, -7.7484e-03, -5.9076e-03,  ..., -2.0097e-02,\n           -3.3195e-03, -1.2772e-02],\n          [-1.1337e-01,  2.4335e-02,  1.2576e-01,  ..., -2.8663e-02,\n            7.0448e-01,  1.6985e-01],\n          [-4.6211e-01,  5.2771e-02,  1.5753e-01,  ..., -2.2525e-01,\n            1.1754e-01, -1.0439e-01],\n          ...,\n          [-5.2622e-01,  3.3897e-01, -3.6210e-03,  ..., -1.6833e-01,\n            8.1798e-03, -3.5989e-02],\n          [-1.7898e-01,  1.4902e-01,  4.3760e-02,  ..., -6.7576e-02,\n            9.4174e-02, -1.0657e-01],\n          [ 1.6964e-01, -3.3308e-01, -1.3923e-01,  ..., -3.0010e-01,\n           -2.0117e-01,  1.6435e-02]],\n\n         [[-3.5693e-02, -7.9178e-03,  1.0561e-02,  ...,  7.9003e-03,\n            1.1250e-02,  1.1793e-02],\n          [-2.1157e-01, -9.3278e-01,  1.2789e-01,  ..., -2.2399e-02,\n           -3.4019e-01, -3.9702e-01],\n          [ 1.5232e-01, -6.8308e-01, -1.2667e-02,  ..., -4.4224e-02,\n            1.6209e-01, -4.1417e-01],\n          ...,\n          [ 1.5063e-01, -2.0814e-01,  2.1461e-01,  ..., -9.8562e-02,\n           -3.6636e-02, -3.1627e-01],\n          [ 3.9176e-01, -1.8605e-01,  2.8391e-02,  ..., -9.8264e-02,\n           -9.5677e-03, -3.8438e-01],\n          [-5.0047e-02, -4.1200e-01,  2.9480e-01,  ...,  3.6376e-01,\n           -8.1028e-02, -4.4138e-01]],\n\n         ...,\n\n         [[-1.7020e-02,  2.0514e-02, -1.3219e-02,  ..., -2.7396e-03,\n           -2.3856e-03,  1.1714e-02],\n          [-4.4627e-02, -9.1744e-02, -9.9211e-02,  ...,  7.2449e-02,\n            9.8154e-02, -3.2808e-02],\n          [ 7.0124e-02,  1.2697e-01,  1.8704e-01,  ..., -1.5831e-01,\n           -1.7343e-02,  1.6386e-01],\n          ...,\n          [-2.0358e-01, -1.5230e-01,  3.7804e-02,  ...,  7.0350e-02,\n           -4.1890e-01,  5.0514e-02],\n          [-1.2607e-02,  2.3928e-01, -3.0951e-02,  ...,  1.8131e-01,\n           -3.2832e-01, -2.9254e-01],\n          [-7.5559e-02, -2.4791e-02, -7.3714e-02,  ...,  2.6189e-01,\n           -1.7322e-01, -8.1548e-02]],\n\n         [[ 2.9759e-03,  9.9851e-03,  1.2614e-02,  ...,  4.7422e-02,\n            1.3567e-02, -3.1465e-03],\n          [ 1.0880e-01,  1.3510e+00,  1.3634e-01,  ...,  2.3499e-02,\n           -1.7103e-01,  1.1104e-01],\n          [ 1.5093e-01,  5.5053e-01,  1.4742e-01,  ..., -5.6128e-02,\n            8.8363e-02,  1.1552e-01],\n          ...,\n          [-8.5937e-02,  1.9849e-01, -5.3613e-02,  ..., -2.0514e-01,\n           -2.6807e-02, -6.9927e-03],\n          [-3.7672e-01, -3.1404e-01, -1.6301e-01,  ..., -1.0568e-01,\n           -2.0208e-01,  3.4364e-01],\n          [ 1.1627e-01,  4.4837e-01, -6.7039e-01,  ...,  2.4221e-01,\n           -4.9293e-01,  2.4366e-01]],\n\n         [[-9.0115e-03,  5.8156e-03,  5.7625e-03,  ...,  1.5618e-03,\n           -8.1958e-03, -1.0273e-02],\n          [ 3.4790e-01, -3.6076e-01,  2.4894e-01,  ..., -3.2158e-01,\n           -5.9973e-02, -1.2560e-01],\n          [ 2.7315e-01, -8.5368e-02,  1.3310e-01,  ...,  1.5860e-01,\n           -1.6372e-01, -8.2068e-03],\n          ...,\n          [-1.0695e-01, -2.5185e-03,  1.4617e-01,  ..., -4.5404e-02,\n           -1.9578e-01, -2.9609e-02],\n          [ 1.1120e-01,  9.2414e-02,  3.8157e-02,  ..., -4.1317e-02,\n           -1.5420e-01, -2.1623e-01],\n          [-1.6257e-01, -1.2010e-02,  6.4325e-02,  ...,  2.4202e-01,\n           -1.9851e-01, -1.4417e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.6317e-02,  1.3466e-01,  5.4830e-02,  ..., -9.1992e-01,\n           -1.4019e+00,  1.4818e+00],\n          [ 1.2326e+00,  3.1613e+00,  1.6568e+00,  ...,  1.8509e-01,\n           -1.3722e+00, -6.6491e+00],\n          [ 1.9328e+00,  1.8872e+00,  2.0568e+00,  ...,  3.9007e-01,\n           -7.1264e-01, -6.8318e+00],\n          ...,\n          [-1.2359e+00, -1.2981e+00,  2.6715e+00,  ..., -5.8398e-01,\n            1.1117e-01, -7.2999e+00],\n          [-1.6475e+00, -1.7938e+00,  9.2559e-02,  ..., -3.8479e-02,\n           -1.5146e-01, -6.6211e+00],\n          [-1.8923e+00, -2.7682e+00,  4.9398e-01,  ..., -9.5186e-01,\n           -5.1755e-01, -7.0788e+00]],\n\n         [[-5.3628e-02,  8.0139e-02, -8.0991e-02,  ..., -1.3485e+00,\n           -1.5041e+00,  6.0794e-01],\n          [-1.0587e+00,  1.2573e+00, -1.7250e+00,  ...,  2.2708e-01,\n            5.9032e-01,  5.7291e-01],\n          [-2.1097e-01,  4.3254e-01, -1.0558e+00,  ...,  1.4978e+00,\n            1.1671e+00,  1.4344e+00],\n          ...,\n          [ 1.2665e+00, -7.3746e-01, -6.9133e-01,  ...,  5.8056e+00,\n            1.6770e+00,  1.7636e+00],\n          [-6.8971e-01, -3.6889e-01,  6.9248e-01,  ...,  6.6919e+00,\n            4.2297e+00,  1.0080e+00],\n          [-4.3376e-01, -1.0968e+00,  5.8477e-01,  ...,  5.3802e+00,\n            6.0880e-01,  1.7648e+00]],\n\n         [[-5.5008e-05, -5.0495e-02, -5.0035e-02,  ..., -3.3727e-01,\n            2.7300e-01, -4.0619e-01],\n          [ 6.1092e-01, -4.9943e-01, -5.3615e-01,  ...,  4.8645e-01,\n           -1.3054e+00,  7.6184e-01],\n          [ 8.0350e-01, -2.8751e-01, -1.0037e+00,  ...,  5.0757e+00,\n           -3.8508e+00,  2.2675e+00],\n          ...,\n          [-3.5077e-01,  3.9110e-01, -7.7770e-02,  ...,  6.1296e+00,\n           -2.7130e+00,  1.4375e+00],\n          [-2.9203e-01,  4.3968e-01, -1.1092e-01,  ...,  5.5444e+00,\n           -1.5172e+00,  3.0202e+00],\n          [-2.7358e-02, -3.0619e-02,  3.5029e-01,  ...,  5.5981e+00,\n           -5.3268e-01,  3.4868e+00]],\n\n         ...,\n\n         [[ 1.0437e-01, -3.3548e-02,  1.2506e-01,  ...,  1.6803e+00,\n           -1.1270e+00, -1.2944e-01],\n          [ 1.7417e+00, -2.8875e-01,  2.3643e+00,  ..., -2.9597e+00,\n           -1.9080e+00,  1.2661e+00],\n          [ 2.9395e-02,  7.7660e-01,  4.5813e-01,  ..., -5.1198e+00,\n            2.3659e-02, -2.0262e-01],\n          ...,\n          [-1.4781e+00,  1.3819e+00, -1.3117e+00,  ..., -3.8288e+00,\n           -1.2726e+00, -1.0869e+00],\n          [ 3.4382e-01,  7.7357e-01, -2.1937e+00,  ..., -3.9219e+00,\n            3.8463e-01, -1.7025e+00],\n          [ 5.3140e-01, -1.3983e-01, -1.8373e+00,  ..., -4.2518e+00,\n           -1.2880e-02, -1.2317e+00]],\n\n         [[-6.8644e-02, -8.1956e-02, -4.0383e-02,  ...,  8.9328e-01,\n            1.8409e+00,  1.4446e+00],\n          [-1.1454e+00, -1.9503e+00, -9.7478e-01,  ...,  3.1692e-01,\n           -3.3628e+00, -1.6152e-01],\n          [ 4.7400e-01, -8.7426e-01, -1.1007e+00,  ...,  3.3010e-01,\n           -3.5354e+00, -2.1006e-01],\n          ...,\n          [ 1.1901e+00,  1.3551e+00, -4.0182e-01,  ..., -9.0979e-02,\n           -3.8868e+00,  6.3973e-01],\n          [ 1.3428e-01,  7.1527e-01, -3.9368e-03,  ...,  7.7908e-02,\n           -3.9934e+00,  3.0507e-01],\n          [-1.0753e+00,  9.4585e-01,  1.7405e-01,  ...,  1.5428e-01,\n           -4.0549e+00, -2.9338e-01]],\n\n         [[-4.6730e-02,  3.6169e-02,  1.8060e-01,  ..., -1.3124e+00,\n            3.0565e-01, -9.7563e-01],\n          [-4.1502e+00,  1.2285e+00,  3.0599e+00,  ..., -1.2249e+00,\n            1.0287e+00, -3.6803e-01],\n          [-1.2745e+00,  9.7856e-01,  1.4054e+00,  ..., -1.7665e+00,\n            7.9501e-01, -9.4531e-01],\n          ...,\n          [ 2.1988e+00,  2.1710e-01, -1.2996e+00,  ..., -7.9220e-01,\n           -2.9255e-01, -7.1018e-01],\n          [ 1.0463e-01, -9.0516e-01, -6.1460e-01,  ..., -1.5183e+00,\n           -2.8626e-01, -1.4258e-01],\n          [ 3.0811e-01, -2.4144e+00, -2.5698e+00,  ...,  7.1495e-01,\n           -7.5689e-01,  3.9869e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5741e-02,  1.2704e-02,  1.0975e-02,  ..., -1.9629e-02,\n           -4.1710e-02,  1.3925e-02],\n          [ 1.0290e-01, -8.6253e-02, -7.6577e-02,  ..., -3.2430e-02,\n            4.8261e-01, -1.7361e-01],\n          [ 7.6148e-02,  1.4310e-01, -2.5367e-01,  ..., -1.9399e-01,\n            5.5266e-01, -2.5191e-01],\n          ...,\n          [-1.1625e-01,  1.5059e-01, -1.2364e-02,  ...,  3.2351e-02,\n            5.4142e-01,  1.3567e-01],\n          [-3.2112e-01,  1.6964e-01, -1.5878e-02,  ..., -8.1791e-02,\n            4.7819e-01,  5.1879e-02],\n          [ 1.2166e-01, -2.2661e-02,  1.4029e-01,  ...,  2.2474e-01,\n            6.1302e-01,  7.6070e-02]],\n\n         [[-7.9166e-04,  2.3437e-03,  8.0485e-03,  ...,  7.7399e-03,\n            9.5045e-03,  4.6631e-03],\n          [ 4.2380e-02, -8.7425e-02, -1.3761e-01,  ...,  7.3599e-02,\n           -2.6021e-01, -2.5217e-01],\n          [-1.7206e-01,  1.6248e-01, -2.3086e-01,  ..., -3.4205e-01,\n           -2.5810e-01, -3.5532e-01],\n          ...,\n          [-9.6249e-02,  2.2249e-01, -6.6095e-01,  ..., -2.5532e-01,\n           -2.2615e-01, -2.3028e-01],\n          [ 9.3356e-02,  2.3495e-01, -5.7557e-01,  ..., -4.1006e-01,\n           -3.8410e-01,  2.0350e-02],\n          [ 1.2238e-01,  2.7590e-01, -1.5102e-01,  ..., -1.0727e-01,\n           -3.4931e-01, -3.5034e-01]],\n\n         [[ 3.9210e-02,  1.9393e-04,  3.3262e-03,  ...,  8.1781e-03,\n           -9.4381e-03, -7.9167e-03],\n          [-9.4547e-01,  1.1973e-01, -2.8968e-01,  ..., -1.8359e-01,\n            1.4042e-01,  8.3514e-03],\n          [-8.3745e-01,  2.7255e-01, -3.6366e-01,  ...,  3.1175e-01,\n           -9.0469e-01,  3.0255e-02],\n          ...,\n          [-8.4997e-01,  2.1602e-01,  7.3421e-02,  ...,  1.1707e-02,\n           -1.1145e-01, -2.7753e-02],\n          [-8.5703e-01, -2.1940e-02, -5.1277e-02,  ..., -1.4274e-01,\n            1.5582e-01,  1.0579e-01],\n          [-8.0754e-01, -1.9184e-02,  1.6539e-01,  ...,  6.7397e-02,\n            3.5809e-02, -1.3506e-01]],\n\n         ...,\n\n         [[ 1.9551e-02,  1.0297e-02,  1.5773e-03,  ..., -1.1819e-02,\n           -1.2796e-03, -1.1804e-02],\n          [ 1.8765e-01, -2.8786e-02, -2.9936e-02,  ..., -3.0006e-02,\n           -2.2696e-01, -1.0051e-01],\n          [-6.9160e-02,  2.4853e-01, -1.6608e-01,  ..., -4.4749e-01,\n            2.3618e-02,  1.7608e-01],\n          ...,\n          [ 1.6023e-01,  2.2103e-01, -3.2907e-01,  ..., -1.1427e-01,\n            1.2678e-01,  1.8312e-02],\n          [-2.5502e-02,  4.3437e-02, -8.4752e-02,  ..., -1.3916e-01,\n            1.1505e-02, -9.1755e-02],\n          [ 9.1606e-02,  1.1718e-01, -2.1248e-01,  ...,  1.8931e-01,\n            1.1512e-01, -1.6510e-01]],\n\n         [[ 2.2793e-02,  2.3541e-02, -2.8061e-02,  ..., -1.2769e-01,\n           -7.2716e-03,  6.0188e-04],\n          [-3.0313e-01,  2.7208e-02, -1.7947e-01,  ...,  3.1877e-01,\n            8.2309e-03, -1.9664e-01],\n          [-1.6683e-01,  2.7815e-01,  1.8993e-02,  ...,  1.8573e-01,\n            4.8218e-02, -3.9488e-02],\n          ...,\n          [-2.5782e-02, -2.7187e-01,  9.3562e-02,  ...,  4.0017e-02,\n           -1.2506e-01, -2.9782e-02],\n          [ 1.4955e-01,  1.9041e-01,  2.7136e-01,  ...,  4.8256e-02,\n           -3.2006e-01,  1.2765e-01],\n          [-7.3341e-02, -8.2576e-02,  1.6007e-02,  ...,  4.9811e-02,\n           -5.3329e-02, -6.1567e-02]],\n\n         [[-2.4003e-02,  1.4413e-03, -9.7189e-03,  ..., -1.0038e-02,\n           -4.0050e-03, -7.8792e-05],\n          [ 4.3207e-02, -1.1919e-01, -5.4457e-01,  ..., -1.4651e-01,\n            1.0372e-01,  2.7977e-01],\n          [-1.0743e-01, -2.7909e-01, -5.8523e-01,  ..., -1.0493e-01,\n           -1.3021e-01, -2.6057e-01],\n          ...,\n          [ 2.4492e-01,  4.2667e-01, -2.2949e-01,  ...,  1.7911e-01,\n            1.6487e-01,  9.0538e-02],\n          [-1.0448e-01,  1.6701e-01, -4.7813e-01,  ..., -1.7286e-02,\n            1.5522e-01,  9.7537e-02],\n          [-1.7028e-02,  8.0929e-01,  9.4702e-01,  ..., -3.3726e-02,\n           -1.0608e-01,  4.0858e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.7858e-02,  1.7457e-01, -2.2763e-02,  ..., -1.1239e+00,\n            9.3052e-01,  9.3458e-01],\n          [ 4.1589e+00,  3.1935e+00, -2.4707e+00,  ..., -1.0910e+00,\n            2.0046e+00,  2.3571e+00],\n          [ 4.1812e+00,  7.1243e-01, -4.3240e+00,  ..., -5.7747e-01,\n            1.6257e+00,  1.6119e+00],\n          ...,\n          [-2.7209e+00, -4.7509e+00, -4.3103e+00,  ..., -2.4002e+00,\n            1.1299e-01,  1.4929e+00],\n          [-3.3909e+00, -3.7879e+00, -2.5160e+00,  ..., -1.4558e+00,\n            6.1843e-01,  2.3745e+00],\n          [-2.5503e+00, -2.7173e+00, -1.4323e+00,  ..., -1.3430e+00,\n           -6.9395e-01,  1.7852e+00]],\n\n         [[ 5.7449e-02, -3.3637e-03, -1.6319e-02,  ..., -4.2206e-01,\n            1.2233e+00,  3.4318e-01],\n          [ 3.2573e+00, -1.4257e+00, -1.1359e+00,  ..., -6.8138e-01,\n            1.1662e+00, -1.3537e-01],\n          [-4.8836e-01, -1.0289e+00, -1.0035e-03,  ..., -5.0336e-01,\n            4.7493e-01,  2.7646e-01],\n          ...,\n          [-1.5530e+00, -1.6113e+00, -3.6268e-01,  ..., -4.6333e-01,\n           -7.9367e-01,  8.5537e-01],\n          [ 9.4621e-01,  1.2660e+00, -6.1222e-01,  ...,  4.1511e-01,\n           -2.5061e-01,  1.0104e+00],\n          [ 2.2199e+00,  2.2013e+00, -1.1727e+00,  ...,  5.7435e-01,\n           -7.1837e-01,  1.9721e-01]],\n\n         [[ 8.8649e-05, -1.0856e-02,  6.7287e-03,  ..., -1.1209e+00,\n           -8.4819e-01,  1.1060e+00],\n          [ 3.0503e-02,  3.2406e-01,  3.5938e-01,  ...,  9.3989e+00,\n            4.5584e+00, -9.7403e+00],\n          [ 6.1180e-03, -4.0424e-02,  1.3981e-01,  ...,  8.2844e+00,\n            9.3161e-01, -9.1253e+00],\n          ...,\n          [-3.2833e-01, -3.9624e-03,  3.8422e-02,  ...,  1.5234e+00,\n            3.0882e+00, -8.6050e+00],\n          [-7.1255e-01, -1.5154e-01, -4.9607e-02,  ...,  5.0184e-01,\n            3.0396e+00, -6.9793e+00],\n          [-4.2383e-02, -2.2849e-01, -5.1579e-01,  ..., -1.3567e+00,\n            8.6881e+00, -4.9598e+00]],\n\n         ...,\n\n         [[ 4.7751e-03,  2.4765e-02,  6.5427e-02,  ...,  1.1498e+00,\n           -5.5660e-01, -2.3739e+00],\n          [-2.7048e-01,  1.9240e+00,  9.2797e-01,  ...,  6.1590e-01,\n           -5.3501e-02,  3.5105e+00],\n          [-1.4208e+00,  7.9953e-01,  7.2649e-01,  ...,  1.4090e+00,\n           -1.0943e-01,  4.6916e+00],\n          ...,\n          [ 1.4458e+00, -1.2390e+00, -1.6505e-01,  ...,  1.6354e+00,\n           -5.8472e-01,  4.2420e+00],\n          [ 1.2140e+00, -1.6530e+00, -7.6998e-01,  ...,  1.3969e+00,\n           -6.5632e-01,  4.7695e+00],\n          [ 4.2530e-01, -1.0632e+00, -1.5249e+00,  ...,  9.5883e-01,\n           -1.4171e+00,  4.5755e+00]],\n\n         [[ 4.9551e-03,  4.8713e-03, -6.7961e-02,  ..., -9.5603e-01,\n           -2.0773e+00, -1.4528e+00],\n          [-2.4987e+00,  4.8178e-01, -8.5864e-01,  ..., -1.1032e+00,\n           -4.5302e-01, -2.4382e+00],\n          [-1.3630e+00,  1.4044e+00, -7.8732e-01,  ..., -3.2960e-01,\n           -6.6378e-01, -1.6802e+00],\n          ...,\n          [ 1.0112e+00,  6.0710e-01,  3.2059e-01,  ..., -1.3903e-01,\n           -1.1601e+00, -1.1141e+00],\n          [ 5.1350e-01, -1.1856e+00, -1.4531e-01,  ...,  3.1771e-01,\n           -4.8929e-02, -3.4188e-01],\n          [ 5.9443e-01, -1.3259e+00,  2.0242e-02,  ..., -1.4919e+00,\n           -5.6953e-01, -7.6237e-01]],\n\n         [[-7.9350e-02, -5.4308e-02, -9.5349e-03,  ...,  9.4083e-01,\n            2.5170e+00, -3.2786e+00],\n          [-2.1497e+00, -1.7756e+00,  8.0995e-01,  ..., -3.3403e+00,\n           -1.8374e-01,  4.8019e+00],\n          [ 1.6129e+00,  5.4331e-01,  6.1357e-01,  ..., -3.7763e+00,\n            1.2750e+00,  7.3814e+00],\n          ...,\n          [ 9.7731e-01,  1.4124e+00,  3.8733e-01,  ..., -2.2185e+00,\n            1.4152e+00,  8.3236e+00],\n          [-1.5950e+00, -1.5879e-01,  4.6176e-01,  ..., -1.2807e+00,\n            1.1276e+00,  8.4439e+00],\n          [-7.3651e-01, -4.5895e-03, -7.2717e-02,  ..., -3.6994e+00,\n           -1.4185e+00,  6.0466e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 9.3590e-04,  2.8529e-02,  5.6978e-03,  ...,  1.2661e-02,\n            8.9264e-03, -8.7175e-03],\n          [ 3.0078e-01,  2.6767e-01,  1.9271e-01,  ...,  1.4708e-01,\n            7.1249e-02,  1.2822e-01],\n          [-8.4698e-02,  5.7607e-01, -3.0367e-01,  ...,  1.0784e-01,\n           -2.7994e-02,  4.0537e-01],\n          ...,\n          [ 1.2736e-01, -7.2686e-02,  3.0685e-01,  ..., -1.6118e-02,\n           -5.6757e-01, -3.7781e-01],\n          [ 1.8854e-01,  4.3236e-01,  2.4099e-01,  ..., -4.3005e-02,\n           -9.0187e-01,  1.1908e-01],\n          [ 1.0048e-01, -7.0765e-03,  4.2359e-02,  ..., -6.1907e-01,\n           -1.0317e-02,  2.9996e-01]],\n\n         [[ 4.2125e-02, -1.4949e-02,  6.8945e-03,  ...,  6.5633e-03,\n           -1.9918e-03, -1.2263e-02],\n          [-8.6123e-01,  2.1418e-02, -4.6317e-02,  ..., -4.7484e-01,\n           -2.6559e-01, -2.0109e-01],\n          [-3.3328e-01,  3.3497e-02, -1.6863e-01,  ..., -2.9473e-01,\n           -3.0380e-01,  8.4597e-02],\n          ...,\n          [-3.1169e-01,  1.6245e-01, -5.2212e-02,  ..., -2.4400e-01,\n           -3.6954e-01, -2.5663e-02],\n          [-9.2145e-02,  4.4051e-01,  4.6001e-02,  ..., -1.2436e-03,\n           -9.6444e-02, -4.9024e-01],\n          [ 2.0358e-01,  1.3464e-01, -2.1163e-01,  ..., -1.3686e-01,\n           -1.3435e-01,  1.6847e-01]],\n\n         [[ 4.8812e-03,  6.3428e-03, -1.5907e-02,  ..., -7.9164e-03,\n            1.9445e-03,  9.0020e-03],\n          [-9.8095e-02, -8.6381e-02, -3.6844e-01,  ..., -5.7589e-02,\n            2.1959e-01, -7.0982e-02],\n          [ 3.0170e-01, -1.0434e-01, -3.8194e-03,  ...,  6.2394e-02,\n            1.2737e-01, -1.1202e-01],\n          ...,\n          [ 2.2344e-01,  9.3603e-03, -2.2775e-01,  ..., -7.9702e-03,\n            2.8950e-02,  1.9388e-01],\n          [ 3.4498e-01,  8.0839e-02, -7.5499e-02,  ...,  5.3918e-02,\n           -1.1760e-01,  1.5815e-01],\n          [ 1.9497e-01, -1.1156e-01, -2.7446e-01,  ..., -6.2212e-02,\n            1.6979e-01,  5.9713e-02]],\n\n         ...,\n\n         [[-2.1932e-03,  6.7271e-03,  1.6275e-02,  ..., -1.2165e-02,\n            6.1163e-03, -3.1671e-02],\n          [ 9.7741e-02,  9.4036e-02,  2.4281e-02,  ...,  1.0980e-01,\n           -1.0405e-01,  1.0067e-01],\n          [-3.2719e-02,  5.3270e-02, -6.5072e-02,  ...,  1.3246e-01,\n           -4.2380e-01,  1.8715e-01],\n          ...,\n          [ 4.3528e-01,  1.0122e-01,  1.3358e-01,  ...,  4.0793e-01,\n           -2.2985e-01, -1.7101e-01],\n          [ 3.1636e-02,  1.8576e-02,  2.6358e-01,  ...,  3.2357e-01,\n           -3.8288e-01, -1.5802e-01],\n          [ 1.3856e-01, -7.9769e-02,  8.5927e-02,  ...,  4.6440e-01,\n           -3.4063e-02, -3.4477e-01]],\n\n         [[ 1.3362e-02,  9.6465e-03, -3.3372e-03,  ..., -2.1057e-02,\n           -1.5878e-02,  1.7561e-02],\n          [-1.7414e-01,  1.1742e-01,  1.4036e-01,  ...,  2.4521e-01,\n            9.3921e-02,  1.9606e-01],\n          [ 8.1271e-02,  4.4230e-01,  2.0403e-02,  ..., -1.2735e-01,\n           -1.8625e-01,  1.1084e-01],\n          ...,\n          [-2.4271e-01,  1.0439e-01, -1.1056e-01,  ...,  3.6153e-01,\n           -3.2894e-01,  2.8341e-02],\n          [ 5.9896e-02,  1.3758e-01, -2.8612e-01,  ...,  2.2576e-01,\n           -4.9372e-01,  5.5741e-02],\n          [-3.2227e-01,  4.0608e-01, -5.1128e-01,  ...,  4.2920e-01,\n           -1.1371e-01,  7.4510e-01]],\n\n         [[ 1.2613e-02,  4.4390e-03, -2.8977e-02,  ...,  1.5352e-02,\n           -3.1253e-03,  1.0260e-02],\n          [ 2.6141e-02,  4.4875e-01,  4.2975e-01,  ...,  3.2282e-01,\n           -2.1937e-01, -2.1450e-01],\n          [-1.0085e-01,  1.5843e-01,  9.9052e-01,  ...,  1.2787e-01,\n           -2.0459e-01, -5.0992e-01],\n          ...,\n          [-1.1244e-01,  6.0204e-02,  4.3369e-02,  ...,  1.0944e-01,\n            3.6751e-03, -9.8643e-02],\n          [-6.7898e-02, -1.1371e-01,  2.6787e-01,  ..., -1.7534e-01,\n            2.3881e-01, -4.7611e-02],\n          [-1.0737e-01, -1.8074e-01,  1.1761e-01,  ...,  5.3748e-03,\n           -3.4666e-01, -4.4969e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.2986e-02, -5.6758e-02,  4.2181e-02,  ...,  6.8194e-01,\n           -2.4365e-01, -2.6245e-01],\n          [-2.0277e+00,  3.6989e-01,  1.0358e+00,  ...,  1.5145e+00,\n           -2.0044e+00,  8.4235e-01],\n          [-1.3090e+00,  8.2125e-01,  5.5625e-01,  ..., -1.8567e-01,\n           -1.1091e+00,  5.4527e-01],\n          ...,\n          [ 1.5006e+00,  4.5542e-01,  4.3176e-01,  ...,  7.1436e-01,\n           -8.7396e-01, -4.8069e-01],\n          [ 1.0901e+00,  3.9606e-01, -4.7152e-01,  ...,  3.2983e-01,\n           -5.0065e-01,  5.0166e-01],\n          [ 1.2840e-01, -8.3067e-01, -8.1293e-01,  ..., -4.0872e-01,\n           -1.9316e+00, -1.9433e+00]],\n\n         [[-6.8734e-02,  8.0266e-02, -9.0677e-03,  ...,  7.3990e-01,\n            8.9014e-02, -5.7725e-01],\n          [-2.7317e+00,  2.1390e+00,  1.1494e+00,  ..., -3.4212e+00,\n           -3.1490e-01, -1.6709e+00],\n          [-2.9074e-01, -5.3336e-02, -5.1275e-01,  ..., -2.8581e+00,\n            4.7676e-02, -2.8024e-01],\n          ...,\n          [ 1.6968e+00, -1.9554e+00,  1.7006e+00,  ..., -4.3257e+00,\n           -2.2115e-01,  6.2416e-01],\n          [-9.2230e-01, -9.7474e-01,  3.9166e-01,  ..., -4.3167e+00,\n           -2.2765e-01, -4.8433e-01],\n          [-6.0461e-01, -1.0264e+00,  3.7290e-01,  ..., -3.7289e+00,\n           -6.2502e-02,  5.3949e-01]],\n\n         [[ 1.7127e-03, -3.6500e-02, -9.9987e-03,  ...,  3.8029e-01,\n            7.0632e-01, -9.5575e-02],\n          [-1.7413e-01, -2.3737e-01, -9.7911e-01,  ...,  1.6357e+00,\n           -9.5725e-02,  5.3678e-01],\n          [-8.8745e-01,  3.5649e-02, -1.8132e-01,  ...,  3.8635e-01,\n           -3.3454e-01,  3.4820e-01],\n          ...,\n          [ 4.0956e-01,  9.8702e-01, -1.6676e+00,  ..., -6.2349e-02,\n           -3.2785e-01,  3.2371e-01],\n          [ 1.3126e+00, -1.0274e-01, -5.2804e-01,  ...,  3.1959e-01,\n           -1.4227e-01,  1.1870e-01],\n          [ 7.6431e-01,  1.3596e-01, -2.6380e-01,  ...,  5.5026e-02,\n            4.8367e-02,  1.2360e+00]],\n\n         ...,\n\n         [[ 4.9501e-02, -3.4285e-02,  4.6124e-02,  ..., -3.8699e-01,\n            1.4646e+00, -1.7441e-01],\n          [ 1.2642e+00, -1.4507e+00,  1.6048e+00,  ...,  2.4330e+00,\n           -4.2889e+00, -5.5782e-01],\n          [-1.4796e-01,  3.3218e-01,  3.1144e-01,  ...,  2.9231e+00,\n           -3.6052e+00, -1.9999e+00],\n          ...,\n          [-1.2134e+00,  1.0892e+00, -3.7682e-01,  ...,  1.4461e+00,\n           -4.1793e+00, -2.2762e+00],\n          [ 7.4900e-01,  1.1938e-01, -4.1763e-01,  ...,  1.6771e+00,\n           -4.0144e+00, -2.6294e+00],\n          [ 4.3123e-01,  6.7885e-01, -1.1292e+00,  ...,  1.6564e+00,\n           -2.8780e+00, -1.3151e+00]],\n\n         [[-4.2510e-02,  3.7889e-02,  6.0747e-02,  ..., -5.7372e-02,\n           -1.2709e+00, -7.8387e-01],\n          [-5.0359e-01,  8.3469e-01,  1.2501e+00,  ...,  2.3276e-02,\n           -1.6576e+00, -4.7826e+00],\n          [ 3.5352e-01, -6.4913e-02,  5.3792e-01,  ..., -1.1122e-01,\n           -1.0168e+00, -5.8695e+00],\n          ...,\n          [ 5.3220e-02, -7.1725e-01,  4.9906e-01,  ..., -9.5418e-01,\n           -3.0370e+00, -2.7932e+00],\n          [-4.3403e-03, -3.6534e-01, -2.3779e-02,  ..., -1.4896e+00,\n           -2.6524e+00, -4.4556e+00],\n          [-3.7900e-01, -5.6974e-01, -3.5380e-02,  ..., -2.3949e+00,\n           -1.5284e+00, -4.1054e+00]],\n\n         [[ 5.9208e-02, -7.4041e-02, -3.8141e-02,  ...,  4.1372e-01,\n            1.9973e+00,  1.8169e+00],\n          [-3.5135e-01, -1.5177e+00, -1.0858e+00,  ...,  2.8560e-01,\n           -6.1455e-01, -2.1975e+00],\n          [-1.7711e+00, -5.5017e-01, -8.7693e-01,  ...,  1.6907e-01,\n           -5.6715e-02, -2.5102e+00],\n          ...,\n          [ 1.6425e-01,  2.4681e+00, -1.4838e+00,  ...,  4.7841e-01,\n           -1.6130e+00, -4.9014e+00],\n          [ 1.0952e+00,  1.1468e+00,  6.9954e-01,  ..., -7.0833e-01,\n           -1.6066e+00, -4.8652e+00],\n          [ 1.0568e+00,  6.1719e-01,  4.0996e-01,  ..., -2.6305e-02,\n           -3.2821e+00, -4.5679e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.5075e-03, -4.3818e-02,  3.2011e-02,  ...,  3.3098e-02,\n           -1.9107e-02, -3.6052e-02],\n          [-2.1269e-03,  1.6216e-01, -1.5261e-01,  ..., -2.4489e-01,\n           -1.2112e-01,  1.9746e-01],\n          [-2.4059e-01,  3.1051e-01,  5.7210e-02,  ..., -2.6637e-01,\n           -6.1936e-02,  2.2837e-01],\n          ...,\n          [-1.1204e-01,  2.9352e-01,  2.7793e-01,  ..., -1.0572e-01,\n            2.2897e-02,  5.1392e-01],\n          [-1.3869e-01,  1.0130e-01, -6.4869e-02,  ...,  4.4783e-02,\n            4.8176e-03,  2.2737e-01],\n          [-4.1347e-03,  2.8174e-02, -1.6069e-01,  ...,  2.7191e-01,\n            2.8911e-04,  4.5144e-01]],\n\n         [[-1.2707e-02,  7.7928e-03, -2.4912e-02,  ...,  4.3719e-04,\n            1.3313e-02,  3.7711e-03],\n          [ 1.4313e-01, -3.8084e-01,  4.3599e-01,  ..., -1.1904e-01,\n            2.5198e-01,  2.1550e-01],\n          [-5.0879e-03, -3.7911e-02,  2.5485e-01,  ..., -2.7624e-01,\n           -1.2211e-01,  2.4520e-01],\n          ...,\n          [-1.3643e-01, -3.5685e-01,  4.4867e-01,  ...,  1.2322e-01,\n            6.0183e-01,  2.4804e-01],\n          [-2.0965e-01, -3.1830e-01,  3.1960e-01,  ...,  3.6249e-02,\n            3.7764e-01,  6.0210e-03],\n          [-8.3376e-02, -2.1156e-01,  1.5272e-01,  ..., -6.9034e-02,\n            2.5154e-01, -1.3800e-01]],\n\n         [[ 4.9766e-03,  1.6422e-02, -9.3895e-04,  ..., -1.0992e-02,\n            2.7222e-02, -3.3118e-03],\n          [ 7.0952e-02,  1.4994e-01, -1.4358e-01,  ...,  1.3321e-01,\n           -9.9199e-01, -5.0039e-02],\n          [ 2.2315e-03,  1.3689e-02,  5.4795e-01,  ...,  4.3925e-01,\n           -3.8316e-01,  1.2275e-01],\n          ...,\n          [ 1.4568e-01,  3.6804e-01,  1.8879e-01,  ...,  6.3231e-01,\n           -6.8411e-01,  8.1384e-02],\n          [ 1.6347e-01,  5.0384e-02,  1.6249e-01,  ...,  2.5391e-01,\n            4.1089e-02,  8.6733e-02],\n          [ 1.0754e-01,  2.0364e-03,  3.1858e-02,  ...,  1.2100e-01,\n           -6.8371e-01, -2.3201e-02]],\n\n         ...,\n\n         [[-6.8232e-03, -3.4541e-03,  3.0257e-03,  ..., -1.1610e-02,\n            1.4196e-02, -4.2651e-03],\n          [-1.9102e-01, -9.6374e-02, -3.8347e-01,  ..., -6.8594e-02,\n            1.5969e-01, -2.9198e-01],\n          [-1.1796e-01, -3.7198e-01, -1.7730e-01,  ...,  3.0197e-01,\n            4.4423e-01, -3.5287e-01],\n          ...,\n          [ 2.5382e-01,  1.8584e-02, -1.4926e-01,  ...,  1.1287e-01,\n            1.5608e-01,  1.8151e-01],\n          [ 5.1760e-02,  1.7294e-02, -1.8609e-01,  ..., -1.3589e-01,\n            6.0654e-02, -8.9075e-02],\n          [-3.4706e-01, -2.0907e-01, -1.7131e-01,  ..., -2.4358e-01,\n           -1.2957e-01, -2.4586e-01]],\n\n         [[ 1.8800e-02, -2.2288e-02, -1.8583e-02,  ...,  2.8067e-02,\n            2.0518e-02,  7.2470e-03],\n          [-2.2676e-01,  2.8546e-01,  1.0846e-01,  ...,  7.3832e-02,\n           -2.8026e-02,  2.3904e-06],\n          [-2.1163e-02,  7.5956e-01,  1.8756e-01,  ...,  3.1446e-01,\n            5.6013e-01, -2.0784e-01],\n          ...,\n          [ 1.9506e-01,  1.2693e-01, -2.0888e-01,  ...,  1.7235e-01,\n            6.0297e-02, -8.2514e-02],\n          [ 4.7009e-01,  4.0924e-01, -2.4973e-01,  ..., -1.2039e-01,\n            1.1012e-01,  5.1201e-02],\n          [ 2.7584e-01,  3.3358e-01, -1.8232e-01,  ...,  2.0032e-01,\n            2.2118e-01,  5.0746e-02]],\n\n         [[ 1.1655e-02, -1.1191e-02, -2.4126e-02,  ...,  1.7493e-02,\n           -1.3610e-02, -2.2237e-02],\n          [-1.6543e-01, -2.5003e-01,  8.6125e-02,  ...,  5.6394e-02,\n            3.4471e-02,  9.6381e-02],\n          [-1.6387e-01, -1.1931e-01,  5.5219e-01,  ...,  5.0510e-01,\n           -1.5565e-01,  8.6988e-02],\n          ...,\n          [-3.0309e-01, -2.2586e-02,  1.5457e-01,  ...,  1.6099e-01,\n           -5.5603e-01, -2.0773e-01],\n          [-4.4897e-01, -2.6678e-02, -3.1037e-01,  ...,  1.3030e-01,\n           -9.0136e-01,  5.7729e-02],\n          [ 2.0204e-01,  3.6599e-02,  1.9856e-01,  ...,  7.2303e-02,\n           -2.2883e-01, -1.2853e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4815e-02,  3.8855e-03, -6.5161e-02,  ...,  3.6477e-01,\n           -7.6282e-01,  3.9310e-01],\n          [ 1.7472e-01,  8.0178e-01, -5.7563e-01,  ..., -1.6855e+00,\n           -3.8904e+00, -1.9497e+00],\n          [-9.5987e-01,  5.8723e-01, -7.1325e-01,  ..., -1.8820e+00,\n           -1.9519e+00, -3.5142e+00],\n          ...,\n          [ 2.9018e-01, -2.2332e-01, -2.2718e-01,  ..., -1.3363e+00,\n           -1.4313e+00, -2.5188e+00],\n          [ 3.7244e-01, -1.9810e-01, -1.9881e-01,  ..., -6.5588e-01,\n           -1.1756e+00, -4.1595e+00],\n          [ 6.5614e-01, -1.0736e+00,  8.7181e-01,  ..., -9.0678e-01,\n           -7.1075e-01, -3.9002e+00]],\n\n         [[-5.2089e-03,  1.4205e-02, -1.2689e-03,  ..., -5.0026e-01,\n           -5.8205e-01,  5.3615e-01],\n          [-1.5271e+00,  8.8098e-01, -5.7725e-02,  ...,  4.1800e+00,\n           -1.7192e+00,  1.1539e+00],\n          [ 3.9878e-01, -2.5377e-01, -2.3752e-01,  ...,  4.6732e+00,\n           -8.8458e-01, -5.0242e-01],\n          ...,\n          [ 7.0503e-01, -1.1254e+00,  2.2898e-01,  ...,  3.6139e+00,\n           -7.5675e-02,  9.0863e-01],\n          [-3.5906e-01, -1.2928e-01,  1.5099e-01,  ...,  2.8251e+00,\n           -3.7178e-01, -2.4456e+00],\n          [-7.6586e-01, -1.8641e-01,  8.9007e-01,  ...,  7.6406e-01,\n           -8.1659e-01, -3.2165e-01]],\n\n         [[-4.6029e-02, -7.7140e-02,  7.5468e-02,  ..., -7.1893e-02,\n            1.2510e+00, -1.8972e-01],\n          [-1.7925e+00, -1.6732e+00,  2.4657e+00,  ..., -3.3280e-01,\n            1.4177e+00, -1.9065e-01],\n          [-1.3771e-01, -1.2481e-01,  1.1394e+00,  ...,  2.8269e-03,\n            1.3836e+00,  4.2271e-01],\n          ...,\n          [ 1.1215e+00,  1.7611e+00, -2.1272e+00,  ...,  4.1990e-02,\n            1.2248e+00,  4.4655e-01],\n          [-4.5170e-03,  1.8393e+00, -5.3411e-01,  ..., -2.2830e-05,\n            5.9196e-01,  8.4567e-01],\n          [-1.4081e+00,  2.5823e-01, -2.0935e+00,  ...,  8.9313e-02,\n           -9.1095e-03, -7.3424e-01]],\n\n         ...,\n\n         [[ 3.8313e-02,  3.1387e-02, -4.9776e-02,  ...,  1.4955e+00,\n            1.4901e+00,  3.7326e-01],\n          [ 4.7177e-01, -1.9497e-01, -9.5550e-01,  ..., -1.4786e+00,\n           -5.3615e+00, -4.5147e+00],\n          [-2.1454e-01, -3.4196e-01,  1.6850e-01,  ..., -2.6179e+00,\n           -6.5332e+00, -5.7047e+00],\n          ...,\n          [-5.5921e-01,  7.6688e-01,  5.1687e-01,  ..., -2.3091e+00,\n           -6.1471e+00, -5.2471e+00],\n          [ 5.3142e-01, -1.4104e-02,  2.5696e-01,  ..., -2.2951e+00,\n           -6.7571e+00, -5.0329e+00],\n          [ 6.1794e-01, -4.1871e-01,  8.8736e-01,  ..., -9.6804e-01,\n           -6.1409e+00, -5.1368e+00]],\n\n         [[ 1.6163e-01,  7.1201e-02,  7.7224e-02,  ..., -9.5703e-01,\n            5.5020e-01,  6.5097e-01],\n          [ 6.2034e-01,  1.2192e+00,  2.8485e+00,  ..., -4.1860e+00,\n            2.0176e+00, -1.7881e+00],\n          [-3.9736e+00, -1.6081e+00,  2.0517e+00,  ..., -4.9265e+00,\n            2.1788e+00,  1.6002e-01],\n          ...,\n          [-1.7394e+00, -3.7851e+00, -5.4124e-01,  ..., -4.0162e+00,\n            2.0071e+00, -5.5163e-01],\n          [ 3.5902e+00, -2.1066e+00, -1.7955e+00,  ..., -2.4751e+00,\n            3.4249e+00, -7.4834e-03],\n          [ 5.7654e+00, -5.9770e-01, -2.7081e+00,  ..., -4.5666e+00,\n            3.4483e+00, -4.5146e-01]],\n\n         [[ 9.2309e-03, -1.3726e-02, -6.7525e-02,  ...,  5.8759e-01,\n            1.1254e+00,  8.7347e-01],\n          [ 6.7081e-01,  1.3083e+00, -2.8793e-01,  ..., -1.8503e+00,\n            1.0474e+00, -1.1659e+00],\n          [-1.9691e+00,  6.1067e-02, -4.9139e-02,  ..., -2.0803e+00,\n            1.0640e+00, -1.6873e+00],\n          ...,\n          [-1.9309e-01, -8.1366e-01, -1.0745e+00,  ..., -1.1412e+00,\n            1.2140e+00, -2.7790e+00],\n          [ 1.2697e+00, -4.0457e-01,  5.4169e-01,  ..., -2.0708e+00,\n           -3.0930e-01, -3.3942e+00],\n          [ 1.1935e+00, -3.6645e-01,  1.2198e+00,  ..., -2.5969e+00,\n            3.4379e-02, -4.6407e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3331e-02, -4.6928e-03, -7.3019e-03,  ..., -1.9747e-02,\n           -3.0523e-03,  2.9128e-02],\n          [ 1.2601e-01, -5.6824e-02,  1.2379e-01,  ...,  1.2374e-01,\n           -3.3303e-02, -3.7515e-01],\n          [ 1.9364e-01, -1.2125e-01,  2.8124e-01,  ..., -1.0768e-01,\n            6.3876e-02, -6.0966e-01],\n          ...,\n          [-4.2872e-02, -1.5920e-01,  3.1075e-01,  ...,  7.3740e-02,\n           -1.0305e-01, -3.5542e-01],\n          [ 9.1024e-02, -1.0297e-01,  4.4406e-01,  ...,  3.4327e-01,\n           -1.5741e-01, -6.0071e-01],\n          [ 1.5528e-01, -2.7940e-01,  3.7453e-01,  ..., -7.5135e-02,\n           -3.9484e-02, -1.8948e-01]],\n\n         [[-4.6705e-03,  2.1753e-03,  2.7855e-02,  ..., -1.3684e-03,\n            6.2462e-02,  2.0781e-03],\n          [-1.2567e-03, -1.0440e-01, -1.3493e-02,  ..., -1.8711e-01,\n           -9.1693e-02,  6.7549e-02],\n          [ 2.9302e-01,  2.4074e-01, -4.7760e-02,  ..., -3.8108e-01,\n            2.2722e-02,  2.2241e-01],\n          ...,\n          [ 6.7676e-02,  3.6794e-01,  2.8198e-01,  ..., -1.3390e-01,\n            5.1727e-01,  2.9591e-01],\n          [ 5.2915e-02,  3.9826e-01,  1.7824e-01,  ..., -8.1494e-03,\n            1.1654e-01,  4.1005e-01],\n          [ 6.6410e-02,  2.2054e-01,  2.0727e-01,  ...,  6.0972e-02,\n            2.6168e-01,  1.0454e-01]],\n\n         [[-6.5995e-03, -3.0940e-02,  3.0484e-02,  ...,  5.9859e-04,\n            1.1084e-02, -1.7138e-01],\n          [ 1.1578e-01,  2.1733e-03, -2.2893e-01,  ..., -1.7371e-01,\n           -2.2726e-03, -5.9387e-02],\n          [ 2.0877e-01,  2.2600e-01, -1.2645e-01,  ..., -3.3830e-01,\n            6.7676e-02,  1.6034e-01],\n          ...,\n          [ 2.2702e-01,  3.6602e-02, -2.1516e-01,  ..., -9.3808e-01,\n            2.2783e-01, -1.7914e-01],\n          [ 4.6318e-01,  1.4795e-02,  1.8363e-02,  ..., -5.6283e-01,\n            1.2641e-01,  4.9621e-01],\n          [-1.6959e-01, -2.2225e-03,  2.8095e-01,  ..., -4.9871e-01,\n            1.7634e-01,  7.5611e-01]],\n\n         ...,\n\n         [[ 4.8023e-03,  4.0914e-01, -1.4497e-02,  ...,  7.0290e-03,\n            1.2445e-02, -2.1458e-02],\n          [ 1.3051e-02, -9.3685e-01,  1.1798e-01,  ..., -1.4003e-01,\n           -1.2717e-01,  8.9570e-02],\n          [-1.6168e-01, -1.6029e+00,  2.3556e-02,  ...,  4.6295e-02,\n            6.0343e-02,  1.0125e-01],\n          ...,\n          [-1.6702e-01, -1.0712e+00,  2.1793e-01,  ...,  1.0436e-01,\n           -4.7951e-02, -1.9403e-01],\n          [-1.1241e-01, -1.2524e+00,  2.6625e-01,  ..., -2.1663e-02,\n           -7.1329e-02, -2.4839e-01],\n          [-1.3651e-01, -6.7578e-01, -2.5005e-02,  ...,  6.8521e-02,\n           -2.7406e-01, -2.5056e-01]],\n\n         [[-1.6400e-02, -9.9874e-03, -1.3735e-02,  ..., -2.2474e-02,\n            2.0281e-02,  2.2781e-01],\n          [ 4.7074e-01, -4.9741e-01, -3.5659e-01,  ...,  9.1268e-02,\n            1.1694e-01, -2.1805e-01],\n          [ 2.0684e-01,  4.0959e-01, -5.0544e-02,  ...,  9.8892e-02,\n            2.3886e-01,  1.7650e-01],\n          ...,\n          [ 2.1865e-01,  2.6039e-01, -1.8954e-02,  ..., -1.6628e-01,\n           -2.1735e-02, -5.7966e-01],\n          [ 1.3507e-01,  1.4209e-01,  4.9038e-02,  ...,  1.2472e-01,\n           -8.2507e-02,  2.5974e-01],\n          [ 2.4348e-01, -3.2005e-02, -2.9278e-01,  ..., -3.8628e-02,\n            1.9323e-01, -4.1019e-01]],\n\n         [[-5.9746e-03,  1.0048e-02, -4.2604e-03,  ..., -1.0930e-01,\n            3.4326e-02,  8.4342e-02],\n          [ 1.0278e-01, -3.1082e-02,  5.8721e-02,  ...,  6.1986e-01,\n           -2.0531e-01, -6.2644e-01],\n          [ 1.1943e-01, -5.8391e-02,  5.6865e-02,  ...,  7.2164e-01,\n           -1.2859e-01, -6.6454e-01],\n          ...,\n          [-3.3965e-01, -1.6304e-01,  5.2154e-01,  ...,  7.4580e-01,\n           -5.2450e-02, -6.4519e-01],\n          [-2.0817e-01,  2.0681e-01,  3.1963e-01,  ...,  7.1918e-01,\n           -3.6935e-02, -6.8810e-01],\n          [-3.5905e-01,  3.3189e-01,  2.7067e-01,  ...,  2.4084e-01,\n           -5.4943e-02, -6.5760e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7947e-02,  7.9241e-03,  5.4580e-02,  ...,  7.3166e-01,\n            7.5432e-01, -5.3334e-01],\n          [-1.4227e+00, -3.1328e-01,  7.6773e-01,  ..., -1.4749e+00,\n            1.3679e+00,  1.1102e+00],\n          [ 2.1317e-01, -3.5648e-01, -4.1964e-01,  ...,  1.3892e-01,\n            8.3991e-01, -9.4518e-01],\n          ...,\n          [ 2.0481e-01,  6.3052e-01,  5.3492e-01,  ...,  1.6286e+00,\n            7.7440e-01, -2.1364e+00],\n          [-9.4441e-01,  1.3181e+00, -9.8881e-01,  ...,  9.1665e-01,\n            8.6446e-01, -2.0505e+00],\n          [-9.4097e-01,  1.0915e+00, -3.3495e-02,  ..., -2.5536e-01,\n            1.0748e+00, -1.3074e+00]],\n\n         [[-2.9458e-02,  1.3561e-02, -1.8575e-02,  ..., -1.3401e+00,\n           -1.5849e+00,  8.7977e-01],\n          [-2.5556e+00,  6.0933e-01, -1.5862e+00,  ..., -2.3096e+00,\n           -2.1301e+00,  1.4235e-01],\n          [-1.0071e+00,  4.9559e-02, -6.3062e-01,  ..., -3.2193e+00,\n           -1.8662e+00, -1.0737e-02],\n          ...,\n          [ 6.9421e-01, -4.1986e-01, -9.7129e-01,  ..., -2.5432e+00,\n           -1.3412e+00,  3.5665e-01],\n          [ 1.0528e+00, -1.8289e+00, -3.9715e-01,  ..., -2.5711e+00,\n           -1.0718e+00, -2.1864e-01],\n          [ 1.1092e+00, -1.4605e+00, -1.4248e+00,  ..., -1.9361e+00,\n            7.9981e-01,  4.2452e-01]],\n\n         [[ 4.9120e-02,  7.5759e-02,  2.0578e-02,  ...,  5.4245e-01,\n            3.2780e-01, -1.0823e+00],\n          [ 1.9434e+00,  1.4119e+00,  8.9912e-01,  ..., -2.4708e+00,\n           -5.8531e-01,  1.9458e+00],\n          [ 1.3223e+00,  6.1247e-01,  1.3897e+00,  ..., -2.6598e+00,\n           -2.1375e+00,  9.3404e-01],\n          ...,\n          [-1.0583e+00, -1.9213e+00,  1.4193e+00,  ..., -1.9858e+00,\n           -2.4528e+00,  1.8812e+00],\n          [-1.6972e+00, -1.7027e+00,  1.3032e-01,  ..., -1.7238e+00,\n           -3.0209e+00,  1.1120e+00],\n          [ 6.4561e-02, -9.5799e-01,  4.1644e-01,  ..., -2.1372e+00,\n           -2.3745e+00,  6.9682e-01]],\n\n         ...,\n\n         [[ 3.2195e-02,  5.6718e-02,  3.6552e-02,  ..., -1.1915e+00,\n            2.4083e+00, -8.5296e-01],\n          [ 1.9427e+00,  8.3545e-01,  1.8341e+00,  ..., -2.2661e+00,\n           -6.1698e+00, -2.3955e+00],\n          [ 1.3729e+00, -1.2718e+00,  9.0652e-01,  ..., -2.7209e+00,\n           -4.1600e+00, -2.1662e+00],\n          ...,\n          [-1.8673e+00, -1.3913e+00, -1.8973e+00,  ..., -2.7562e+00,\n           -3.9062e+00, -3.6888e+00],\n          [-1.5251e+00, -1.6881e+00, -7.6363e-01,  ..., -1.6459e+00,\n           -3.8735e+00, -1.6431e+00],\n          [-1.5214e-01, -6.2781e-01, -2.1714e+00,  ..., -1.4013e+00,\n           -4.1542e+00, -2.9377e+00]],\n\n         [[-3.5472e-02, -2.0856e-02,  8.1324e-03,  ..., -5.8972e-02,\n           -1.6879e+00, -5.6755e-01],\n          [-9.1058e-01, -1.8759e+00,  1.3397e+00,  ..., -6.5463e-01,\n            2.4758e+00,  1.8893e+00],\n          [ 7.3099e-01, -1.5181e+00,  3.8525e-01,  ...,  9.1481e-02,\n            2.7941e+00,  7.3755e-01],\n          ...,\n          [ 5.8331e-01, -2.1469e-01,  6.6975e-01,  ...,  9.8887e-03,\n            3.0421e+00,  9.7136e-01],\n          [-9.5213e-01,  1.7801e+00, -4.6554e-01,  ..., -1.5791e+00,\n            3.7799e+00,  1.1569e+00],\n          [-2.1433e+00,  2.5012e+00, -1.9069e-01,  ..., -8.2747e-01,\n            3.1538e+00,  4.1107e-01]],\n\n         [[ 6.5458e-03,  4.6341e-03,  1.3568e-01,  ...,  6.9160e-01,\n            6.4990e-01,  5.2006e-01],\n          [ 4.8618e+00,  4.0616e+00,  3.7445e+00,  ...,  1.9428e+00,\n            1.0039e+00,  1.3118e+00],\n          [ 3.8549e+00,  1.3905e+00,  2.3375e+00,  ...,  7.9430e-01,\n           -8.6311e-01,  2.3621e+00],\n          ...,\n          [-3.1003e+00,  8.0585e-01, -1.8532e+00,  ..., -3.3204e-01,\n           -1.2169e+00,  1.0019e+00],\n          [-3.9758e+00,  5.6328e-01, -3.1805e+00,  ..., -5.5188e-01,\n           -1.2724e+00,  3.1984e+00],\n          [-6.5815e-01, -5.3763e-01, -4.4679e+00,  ...,  1.2719e+00,\n           -1.3666e+00,  7.4437e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.1082e-04,  3.5931e-03,  1.1365e-02,  ...,  2.7242e-02,\n           -1.2164e-02,  4.3938e-03],\n          [ 7.1937e-01, -3.0655e-01, -7.0021e-02,  ...,  7.0112e-02,\n            6.7781e-02, -5.4168e-01],\n          [ 7.3877e-01,  2.1703e-02, -1.0473e-01,  ..., -2.1154e-02,\n            2.0039e-01, -5.4422e-01],\n          ...,\n          [ 6.1225e-01, -1.2619e-01, -3.8412e-01,  ..., -1.9074e-01,\n            3.5200e-01, -5.5218e-01],\n          [ 5.8258e-01,  1.7563e-02, -1.8405e-01,  ..., -3.2325e-02,\n            1.4457e-01, -4.9312e-01],\n          [ 6.1921e-01, -1.5764e-01, -2.0414e-01,  ...,  8.6097e-02,\n           -6.4259e-02, -8.1969e-01]],\n\n         [[-1.2056e-02, -5.5574e-03,  3.8054e-02,  ..., -1.7010e-01,\n            3.2242e-02, -1.3090e-02],\n          [-1.2494e-01,  1.3325e-01,  6.3530e-02,  ...,  3.1447e-02,\n            9.3956e-02,  1.3487e-01],\n          [-2.0624e-01,  2.0004e-01,  1.4182e-01,  ..., -3.0684e-02,\n           -2.0348e-01,  2.2170e-01],\n          ...,\n          [ 8.2619e-02, -4.3028e-02,  5.5799e-02,  ..., -3.4660e-01,\n           -3.2581e-01,  5.0147e-01],\n          [ 8.1035e-02,  2.4790e-01,  1.1859e-01,  ..., -1.9436e-02,\n           -1.4303e-01,  1.4264e-01],\n          [-1.4075e-01,  2.3101e-01, -2.6854e-01,  ...,  1.1051e-01,\n           -3.3257e-01,  5.5553e-02]],\n\n         [[ 3.2441e-02,  7.3343e-03, -7.6267e-03,  ..., -6.4834e-03,\n           -1.7868e-02, -1.3072e-02],\n          [ 3.0421e-01,  8.5115e-02, -7.7033e-02,  ..., -1.5478e-01,\n            1.5215e-01, -6.6995e-02],\n          [ 3.4015e-01,  3.5630e-02, -1.5668e-02,  ..., -1.7520e-01,\n            2.5132e-01,  1.6824e-01],\n          ...,\n          [ 1.7575e-01, -3.0521e-01,  4.1171e-01,  ...,  7.2286e-02,\n           -2.5222e-01, -1.4530e-01],\n          [ 1.4619e-01,  1.0188e-01, -2.4380e-01,  ..., -1.6767e-01,\n           -3.9960e-01,  1.6171e-01],\n          [-2.3076e-01,  2.3107e-02,  4.7495e-03,  ...,  7.6523e-02,\n            3.8540e-02, -1.1297e-01]],\n\n         ...,\n\n         [[-7.2353e-03,  1.2753e-02,  5.2595e-03,  ..., -1.2895e-02,\n           -5.8279e-03, -7.6916e-03],\n          [ 2.4093e-01, -6.4406e-02,  1.2542e-01,  ...,  1.0988e-01,\n            3.0498e-01,  1.3181e-01],\n          [-1.0176e-01,  1.6784e-02, -1.7389e-01,  ..., -1.2551e-01,\n            1.2583e-01, -1.7929e-02],\n          ...,\n          [-5.6606e-02,  4.1440e-02, -2.0578e-01,  ...,  3.2717e-01,\n            1.3850e-03, -8.0089e-02],\n          [ 1.3123e-01, -6.5005e-02, -1.2321e-02,  ...,  1.7689e-02,\n            1.9605e-01, -9.2188e-02],\n          [-3.0912e-02, -2.2582e-01,  2.3516e-01,  ...,  1.8610e-01,\n            3.1048e-01, -4.0471e-02]],\n\n         [[ 2.8551e-03,  4.7691e-02, -3.8349e-02,  ...,  4.4290e-02,\n           -2.6177e-04, -3.4950e-01],\n          [ 4.8099e-01, -3.4549e-01,  1.2902e-01,  ...,  7.0070e-01,\n            3.1902e-01,  1.7167e-01],\n          [ 2.5321e-01, -3.6839e-02, -1.0152e-01,  ...,  3.4972e-01,\n            2.1394e-01,  1.3799e-01],\n          ...,\n          [ 1.2337e-01,  1.2475e-01, -2.7210e-01,  ...,  3.3952e-01,\n           -2.4348e-01, -2.5896e-01],\n          [ 1.2148e-01, -5.1883e-02, -2.5086e-01,  ...,  5.8157e-01,\n            3.1913e-03, -1.3361e-02],\n          [ 5.2966e-01, -4.1191e-02, -3.2714e-01,  ...,  2.6910e-01,\n           -2.4594e-01,  1.3633e-01]],\n\n         [[-8.1304e-03,  1.4941e-02, -1.0689e-02,  ...,  7.0810e-03,\n            5.8582e-03, -1.3757e-02],\n          [-2.2038e-01,  2.3056e-01,  2.9953e-02,  ..., -4.9608e-02,\n           -1.3103e-01, -1.4165e-01],\n          [-5.8053e-01,  9.6231e-03,  2.1723e-01,  ...,  2.3857e-01,\n            1.9165e-01,  2.1279e-02],\n          ...,\n          [-2.3823e-02,  1.5250e-01, -4.2803e-01,  ..., -1.1787e-01,\n            4.9306e-01, -3.7298e-01],\n          [-1.1632e-03,  2.6019e-03, -1.5353e-01,  ..., -2.3680e-01,\n            2.4624e-01, -7.7515e-02],\n          [-3.3636e-01, -1.6598e-01,  2.2549e-01,  ..., -7.8078e-02,\n            2.5051e-01,  2.2947e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2312e-01,  8.0175e-04,  6.5801e-02,  ...,  3.4184e-01,\n            7.8137e-01, -1.7893e-01],\n          [-2.0273e+00,  2.5088e-01,  2.2136e+00,  ...,  2.1602e+00,\n           -3.6832e+00,  5.5925e-02],\n          [ 5.8843e-01, -2.2986e+00,  2.9580e+00,  ...,  2.8248e+00,\n           -4.8020e+00, -7.1777e-01],\n          ...,\n          [ 3.2741e+00, -1.0181e+00,  8.7457e-01,  ...,  2.4720e+00,\n           -6.6660e+00, -1.2930e+00],\n          [ 3.3239e-01,  1.3234e+00,  1.0918e+00,  ...,  3.1143e+00,\n           -6.8487e+00, -1.3001e+00],\n          [-1.6789e+00,  1.6649e+00,  6.4973e-01,  ...,  4.9136e+00,\n           -5.9759e+00, -1.3086e+00]],\n\n         [[-4.0276e-02, -6.4417e-02, -5.3369e-02,  ...,  4.9013e-01,\n            9.7800e-02, -3.2408e-01],\n          [-2.2293e+00, -1.7687e+00, -1.3600e+00,  ..., -1.2043e+00,\n           -3.0898e-01, -8.6754e-01],\n          [-1.8947e+00, -4.6684e-01, -4.1457e-01,  ..., -2.0658e+00,\n           -5.5696e-02,  2.4413e-01],\n          ...,\n          [ 7.9429e-01,  2.2565e+00,  9.3412e-01,  ..., -8.6943e-01,\n           -6.8542e-01, -3.4692e-01],\n          [ 1.1933e+00,  7.4079e-01,  1.6663e+00,  ..., -9.9109e-01,\n           -1.0008e+00,  5.6319e-01],\n          [ 5.9951e-01,  1.5651e+00,  2.5191e+00,  ..., -3.8812e-02,\n           -6.5149e-01, -1.8521e-02]],\n\n         [[ 3.3561e-02,  8.4039e-02, -2.8396e-02,  ..., -4.7250e-01,\n            1.9230e+00, -2.1323e-01],\n          [ 5.7200e-01,  2.2279e+00,  1.5472e-01,  ..., -6.2765e-01,\n            8.7105e-01, -2.3120e-01],\n          [-1.0779e+00,  2.0015e-01,  1.5254e+00,  ...,  1.1903e+00,\n           -1.7934e-01,  2.8011e-02],\n          ...,\n          [-5.1735e-01, -1.3757e+00,  1.9088e+00,  ...,  4.7389e+00,\n           -1.3282e+00, -6.7751e-01],\n          [ 8.7289e-01, -1.1860e+00,  1.6457e+00,  ...,  1.5667e+00,\n           -1.8226e+00,  1.0986e-01],\n          [ 1.0428e+00, -1.9213e+00,  2.9362e-01,  ..., -5.2215e-01,\n           -1.1983e+00,  2.4624e-01]],\n\n         ...,\n\n         [[ 3.5742e-03, -1.2710e-02, -1.4841e-02,  ...,  9.3606e-03,\n           -1.9389e-01,  7.3719e-02],\n          [-4.5218e-01,  6.1590e-02,  2.1453e-01,  ...,  1.5978e+00,\n            4.0628e+00, -3.0448e-01],\n          [-2.9151e-03,  2.6232e-01,  3.1400e-01,  ...,  4.0447e+00,\n            5.4559e+00, -3.1808e+00],\n          ...,\n          [ 4.8316e-01,  1.1920e-02,  8.1735e-01,  ...,  4.6892e+00,\n            4.6682e+00, -1.2101e+00],\n          [ 6.1053e-01, -5.4636e-01,  1.2391e+00,  ...,  4.1324e+00,\n            6.1381e+00, -2.8616e+00],\n          [ 7.9651e-01,  2.1781e-01,  8.0535e-01,  ...,  4.9993e+00,\n            4.2333e+00, -2.6715e+00]],\n\n         [[ 3.2944e-02, -4.3002e-03,  7.3976e-03,  ..., -7.6260e-01,\n            7.9404e-01, -7.5428e-01],\n          [-5.1165e-01,  3.3352e-01,  1.6246e-01,  ...,  1.1631e+00,\n            1.7283e+00,  3.4625e-01],\n          [ 1.5882e-01, -2.4296e-02, -2.3312e-01,  ...,  1.3329e+00,\n            8.8855e-01, -4.3772e-02],\n          ...,\n          [ 3.1424e-01, -2.7989e-01, -6.4704e-01,  ...,  2.0611e+00,\n            8.6480e-01,  1.1318e+00],\n          [ 1.9290e-02,  2.0101e-01, -5.4484e-01,  ...,  1.2569e+00,\n            5.4770e-01,  2.0927e+00],\n          [ 1.4652e-01,  7.5938e-01, -9.8990e-02,  ...,  1.3441e+00,\n            1.2334e+00,  3.1849e+00]],\n\n         [[-2.4856e-02,  5.0377e-02, -2.0058e-02,  ..., -7.2910e-02,\n            1.7153e+00,  6.1020e-01],\n          [ 1.2633e+00,  1.7550e+00, -1.0946e+00,  ..., -4.0205e-01,\n           -3.2863e+00,  1.8942e+00],\n          [ 1.4678e+00,  9.7763e-01, -9.2193e-01,  ..., -1.7589e-01,\n           -3.5836e+00,  3.3629e+00],\n          ...,\n          [-4.3405e-01,  4.0004e-01, -1.2932e+00,  ..., -8.0985e-01,\n           -2.9375e+00,  3.4383e+00],\n          [-1.2732e+00,  6.0986e-02, -6.0903e-01,  ..., -6.1697e-01,\n           -3.2768e+00,  3.0371e+00],\n          [-6.4617e-01, -2.6350e-01, -7.1758e-01,  ..., -1.4704e+00,\n           -2.5376e+00,  4.9321e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.8676e-03,  1.1584e-03, -2.5280e-03,  ...,  1.0427e-03,\n           -1.0717e-02, -7.4770e-03],\n          [-7.7805e-01, -9.1382e-02, -8.1707e-02,  ..., -4.8083e-01,\n           -7.2615e-01,  4.8550e-01],\n          [-1.7969e-01,  5.2268e-03, -3.1901e-01,  ..., -2.2387e-01,\n           -2.4940e-01,  4.2651e-02],\n          ...,\n          [ 4.2639e-01, -4.6097e-01,  9.1851e-02,  ...,  2.3704e-01,\n           -1.4049e-01,  4.5012e-01],\n          [ 5.4303e-02,  2.1139e-01, -2.1290e-01,  ..., -2.3935e-01,\n            1.9690e-01, -1.4556e-01],\n          [ 6.2408e-01,  1.3162e-01,  5.5379e-02,  ..., -1.1796e-01,\n            1.4404e-01, -4.0336e-01]],\n\n         [[ 2.7190e-01,  1.4988e-02, -2.0839e-03,  ...,  3.9396e-03,\n           -1.3605e-02, -1.2758e-03],\n          [-2.7506e-01,  9.9242e-02,  5.8154e-03,  ..., -7.6369e-02,\n            3.0214e-01,  5.2639e-02],\n          [-7.1955e-01,  1.5563e-01, -2.2284e-01,  ...,  2.7428e-02,\n           -1.4784e-02,  1.6643e-01],\n          ...,\n          [-8.5632e-01, -1.5496e-01, -6.9391e-03,  ..., -3.6154e-01,\n           -1.1608e-01,  7.1647e-02],\n          [-4.2170e-01,  4.5486e-02,  1.2847e-01,  ..., -1.3755e-01,\n           -4.6646e-01,  1.7543e-01],\n          [-9.4145e-01, -2.0402e-02, -8.6336e-02,  ..., -6.6149e-02,\n           -3.6288e-01,  3.5880e-01]],\n\n         [[ 3.8737e-02,  1.7042e-02, -4.3715e-03,  ...,  3.2801e-02,\n            3.5758e-03, -2.1997e-01],\n          [ 6.4525e-02, -1.3032e-01, -7.0709e-02,  ..., -1.1451e-01,\n           -1.4236e-01, -5.1637e-01],\n          [ 1.2331e-01,  7.8746e-02, -9.9049e-03,  ..., -1.1019e-01,\n           -2.0571e-01, -1.6622e-01],\n          ...,\n          [-5.6203e-02, -3.0834e-02,  8.1040e-02,  ...,  2.3187e-01,\n           -3.1130e-01, -1.1976e-01],\n          [ 5.7486e-02, -1.6409e-01,  9.7397e-02,  ...,  8.6678e-02,\n           -1.8303e-01, -2.1524e-01],\n          [ 9.5699e-02, -9.4756e-02,  1.3789e-01,  ..., -1.6573e-01,\n           -5.1339e-02, -5.8899e-01]],\n\n         ...,\n\n         [[ 1.0134e-02,  8.3906e-03, -1.4678e-02,  ...,  2.0061e-02,\n            8.9872e-03, -4.0096e-03],\n          [-6.4184e-01,  2.4045e-01,  9.8672e-02,  ...,  4.4117e-02,\n            9.3468e-02, -7.7275e-01],\n          [-7.0650e-01, -4.5859e-01, -1.3923e-01,  ..., -4.0585e-01,\n           -2.4234e-02,  5.8095e-01],\n          ...,\n          [-7.0595e-01,  4.2824e-02, -1.0454e-01,  ..., -1.7282e-01,\n            3.1412e-03,  4.0021e-02],\n          [-1.8547e-01, -5.4997e-01,  5.4040e-01,  ...,  8.5589e-01,\n           -6.9312e-01, -6.8143e-02],\n          [-4.1477e-02, -3.4422e-01, -1.2055e-01,  ..., -5.7615e-01,\n           -6.1381e-01,  3.2405e-01]],\n\n         [[ 4.6773e-03, -6.1752e-03, -1.3456e-02,  ...,  2.2459e-03,\n           -1.4624e-02,  1.7275e-02],\n          [ 1.0933e-01, -2.4646e-01,  4.2456e-01,  ..., -5.0560e-02,\n           -5.1515e-01,  7.0203e-02],\n          [-5.8753e-01, -2.8683e-01,  5.6241e-01,  ...,  8.7894e-02,\n           -5.3783e-01, -1.3549e-01],\n          ...,\n          [-3.6709e-01,  1.0368e-02, -2.6492e-01,  ...,  3.0226e-01,\n            1.7847e-01, -3.9443e-01],\n          [-1.8174e-01, -1.5944e-01, -1.2444e-01,  ...,  1.7185e-01,\n           -1.4663e-01, -3.7696e-01],\n          [ 8.7055e-02, -4.1232e-02,  1.1925e-02,  ..., -4.3146e-02,\n           -3.0805e-01, -1.8966e-01]],\n\n         [[ 9.5875e-03,  1.9877e-03,  5.5560e-04,  ...,  1.6537e-02,\n            1.5532e-03,  1.2090e-02],\n          [-2.8249e-02,  3.7383e-02,  3.8696e-01,  ..., -5.9451e-02,\n            1.7590e-01, -3.6832e-02],\n          [-2.0332e-03, -1.6847e-01,  1.1968e-02,  ...,  6.5603e-02,\n           -1.3205e-01,  2.7796e-01],\n          ...,\n          [-5.3895e-01, -9.4709e-02, -2.4603e-01,  ..., -9.8091e-02,\n            1.9656e-01, -9.3505e-02],\n          [-1.3160e-01,  2.0801e-01, -1.3776e-01,  ..., -5.2161e-02,\n           -6.2677e-02,  3.4716e-02],\n          [-1.6502e-01,  1.7395e-01,  9.3074e-02,  ..., -6.8535e-02,\n           -1.7071e-01,  6.4562e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.5351e-02, -8.4757e-02, -6.0248e-02,  ...,  6.7227e-01,\n            1.3881e+00, -7.4223e-01],\n          [ 1.2616e+00, -1.2370e+00, -7.4736e-01,  ...,  1.2731e+00,\n           -1.0633e+00, -4.8718e-01],\n          [ 1.1614e-01,  1.6054e-01, -4.4539e-01,  ...,  4.4137e-01,\n           -1.3666e+00,  1.7992e-01],\n          ...,\n          [-5.4168e-01, -2.6959e-01,  6.0514e-01,  ..., -2.5940e-01,\n           -5.2639e-01, -1.0426e+00],\n          [-5.8534e-01, -1.8146e-01,  1.1400e+00,  ..., -2.3092e-01,\n           -1.5982e+00, -1.6913e-01],\n          [ 7.7766e-01,  1.6303e+00,  2.2796e+00,  ..., -5.4150e-01,\n           -1.6284e+00, -1.7295e+00]],\n\n         [[-1.8946e-02, -2.2157e-02,  2.3029e-02,  ...,  3.3776e-01,\n            1.0489e-01, -1.6274e+00],\n          [-2.2000e-01, -1.2279e-01,  4.6447e-01,  ...,  4.0435e-01,\n           -1.0897e+00,  5.0793e+00],\n          [ 3.8183e-01, -1.3501e-01,  9.4150e-01,  ...,  3.5868e+00,\n           -2.5323e+00,  3.0625e+00],\n          ...,\n          [ 7.1628e-01,  2.1930e-01,  6.7505e-01,  ...,  2.6556e+00,\n           -5.0718e-01,  4.5530e+00],\n          [-5.0165e-01, -7.0535e-01, -3.4468e-01,  ...,  3.8036e+00,\n           -1.9527e+00,  3.5220e+00],\n          [-6.0048e-01, -9.2246e-01, -7.6930e-01,  ...,  4.0905e+00,\n           -1.6349e+00,  3.0920e+00]],\n\n         [[ 2.1701e-02, -9.1714e-03,  9.7642e-02,  ...,  9.4590e-01,\n           -2.8308e-01, -5.0261e-01],\n          [ 6.7626e+00, -2.7791e+00,  3.9439e+00,  ...,  2.1430e+00,\n           -6.6616e-01,  9.2521e-02],\n          [ 5.1229e+00, -4.7000e+00,  2.7205e+00,  ...,  1.3188e+00,\n            1.8473e+00,  2.0944e+00],\n          ...,\n          [-6.0245e+00, -1.9102e+00,  4.0358e-01,  ...,  2.3604e+00,\n            1.3408e+00,  1.3355e+00],\n          [-5.4936e+00,  2.1390e+00, -1.3403e-01,  ...,  2.7181e+00,\n            2.1750e+00,  1.3737e+00],\n          [-1.2934e+00,  4.0860e+00, -2.1220e+00,  ...,  2.4340e+00,\n            3.3896e-01,  1.1140e+00]],\n\n         ...,\n\n         [[ 5.4727e-03,  1.4969e-02, -4.3817e-02,  ..., -3.0039e-01,\n            1.1738e-01,  2.3453e+00],\n          [-9.3585e-01,  1.6610e+00, -1.2045e+00,  ..., -1.0590e+00,\n            6.9103e-01, -2.5385e+00],\n          [-8.8610e-01,  2.2702e+00,  1.1747e+00,  ..., -1.7107e+00,\n           -1.5240e+00, -3.1733e+00],\n          ...,\n          [ 9.4467e-01, -1.8372e-01, -2.0256e-02,  ...,  2.1084e-01,\n           -2.7350e+00, -4.0278e+00],\n          [ 1.1101e+00, -2.2151e+00,  1.6296e+00,  ..., -1.1708e+00,\n           -1.5052e+00, -4.0883e+00],\n          [ 8.3699e-01, -2.0846e+00,  2.7970e-01,  ...,  8.6965e-01,\n           -2.0266e+00, -3.0972e+00]],\n\n         [[ 2.7228e-02,  2.5961e-02, -3.5073e-02,  ...,  5.4443e-01,\n            4.4475e-01,  8.4028e-01],\n          [ 2.2954e-01,  1.0210e+00, -1.6859e+00,  ...,  3.5193e+00,\n            6.4974e-01, -5.3879e-01],\n          [ 1.8964e-01,  9.0448e-02,  8.7046e-01,  ...,  2.1939e+00,\n           -3.3477e-01,  5.3138e-01],\n          ...,\n          [ 1.9603e-01, -2.4385e-01, -6.1381e-01,  ...,  2.2383e+00,\n           -1.2850e+00, -4.3273e-01],\n          [-1.3237e+00, -1.1143e+00, -2.3366e-01,  ...,  1.7414e+00,\n           -3.3808e-02, -9.9236e-01],\n          [-1.7050e+00, -6.5092e-02, -6.4913e-01,  ...,  1.6118e+00,\n           -6.3670e-01, -3.3138e-01]],\n\n         [[ 8.4090e-02, -6.8144e-02, -1.7702e-02,  ..., -3.3280e-01,\n            1.3528e+00,  6.3329e-01],\n          [ 1.9053e+00, -5.0859e+00,  1.6806e+00,  ..., -2.0356e-01,\n           -4.8222e+00,  9.6508e-01],\n          [-2.3532e+00, -4.2028e+00,  2.2080e+00,  ...,  1.7840e-01,\n           -5.2344e+00,  8.7347e-01],\n          ...,\n          [-2.2304e+00,  1.1961e+00,  1.8620e+00,  ..., -8.3341e-01,\n           -5.6237e+00,  9.5758e-01],\n          [ 1.8058e+00,  3.1312e+00,  1.1766e+00,  ...,  4.3110e-01,\n           -5.3096e+00, -4.7511e-01],\n          [ 3.0188e+00,  3.3326e+00,  9.3646e-01,  ..., -1.9392e-01,\n           -5.4617e+00, -5.2820e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5132e-02,  1.8012e-02,  1.9541e-02,  ...,  9.7809e-03,\n            2.9872e-03, -3.5270e-02],\n          [ 1.4281e-01, -1.1690e-01, -2.1661e-01,  ..., -5.3711e-02,\n           -2.7509e-01,  1.8915e-01],\n          [ 2.4982e-01, -4.4841e-01, -1.2889e-01,  ..., -3.4453e-01,\n            3.0005e-01,  2.4777e-01],\n          ...,\n          [ 3.6535e-01, -1.3034e-01, -1.3958e-01,  ..., -2.6643e-01,\n            4.5300e-01,  2.8213e-01],\n          [ 2.4861e-01, -2.5750e-01, -1.0779e-01,  ..., -4.1795e-01,\n            1.6677e-01,  4.0313e-01],\n          [ 3.0019e-01, -3.3321e-01, -1.2242e-01,  ...,  1.9265e-01,\n            5.1266e-01, -1.3264e-01]],\n\n         [[-1.9224e-02, -3.4200e-03, -4.3395e-03,  ..., -7.5942e-03,\n           -1.1307e-02,  1.7963e-02],\n          [ 2.5455e-01, -1.8495e-01,  4.3413e-01,  ..., -5.0476e-01,\n           -5.0250e-01,  8.9971e-02],\n          [ 3.3818e-02, -3.1044e-02,  5.4943e-02,  ..., -8.5431e-02,\n           -2.6772e-02, -1.2708e-01],\n          ...,\n          [-2.8894e-01, -3.3197e-02,  1.5209e-01,  ..., -5.2791e-01,\n            1.9384e-01, -4.7756e-02],\n          [-1.1962e-01,  4.4641e-02, -1.0454e-01,  ..., -8.0905e-02,\n           -6.1556e-02,  1.7375e-01],\n          [ 2.4786e-01, -8.8134e-02, -2.4973e-01,  ..., -1.6788e-01,\n           -1.0659e-01,  3.0257e-02]],\n\n         [[-1.0082e-01, -3.0075e-02,  4.1509e-02,  ..., -7.7669e-01,\n           -1.3114e-02,  1.2001e-02],\n          [ 4.5432e-02,  3.4684e-02,  1.6137e-03,  ...,  2.4898e-01,\n            6.4173e-02,  8.2674e-03],\n          [ 3.7278e-02,  2.0497e-01,  1.1244e-01,  ...,  4.9411e-01,\n            7.0016e-02,  1.0976e-01],\n          ...,\n          [-2.1570e-01, -2.0606e-01, -2.4016e-01,  ...,  4.4447e-01,\n           -7.3380e-02,  6.9049e-02],\n          [-1.0900e-02, -3.4667e-01,  2.0258e-01,  ...,  1.1281e-01,\n           -2.3070e-01,  1.1161e-01],\n          [ 3.9688e-01, -3.2756e-01, -1.8547e-01,  ...,  7.6121e-01,\n           -1.7677e-01, -1.0005e-01]],\n\n         ...,\n\n         [[-5.7387e-03,  2.5267e-02, -2.2123e-02,  ...,  1.1183e-02,\n           -2.7630e-03,  1.7944e-02],\n          [-1.3462e-01, -4.6760e-01, -4.1073e-01,  ..., -3.3048e-02,\n           -1.3100e-01, -3.1595e-01],\n          [-7.4877e-03,  1.3933e-01, -5.2769e-02,  ..., -9.8085e-02,\n            1.9186e-01,  1.9385e-02],\n          ...,\n          [-1.2679e-01,  3.3661e-01, -4.8950e-02,  ...,  7.4261e-03,\n            2.2402e-01,  3.6038e-01],\n          [-1.7555e-01, -4.5212e-02,  6.4099e-02,  ...,  2.6169e-02,\n            1.2100e-01, -1.3538e-02],\n          [-8.9102e-02, -9.6494e-02,  8.1932e-02,  ...,  9.2361e-02,\n            2.7794e-01,  3.1251e-01]],\n\n         [[ 1.8212e-01, -1.8599e-02, -2.1570e-02,  ...,  3.7844e-03,\n            1.8826e-02, -2.6143e-02],\n          [-4.6325e-01,  1.6244e-01,  2.2072e-01,  ..., -6.9552e-02,\n            6.1167e-01,  2.6188e-02],\n          [-5.3045e-03,  8.4458e-02,  6.6047e-02,  ...,  1.6932e-02,\n            2.0987e-01,  9.1609e-02],\n          ...,\n          [-4.8462e-01,  1.1965e-01,  8.3614e-02,  ..., -4.0738e-02,\n            3.4050e-01, -1.3341e-01],\n          [-3.5824e-01,  2.9223e-01,  1.8767e-02,  ...,  2.0220e-01,\n            6.3876e-02, -3.2615e-02],\n          [-7.1602e-01,  2.7131e-01, -1.1609e-01,  ...,  2.3846e-01,\n           -2.6324e-01, -6.5402e-02]],\n\n         [[-6.5726e-03,  3.8399e-03, -1.3766e-02,  ...,  6.9220e-03,\n           -4.0670e-03, -1.2715e-02],\n          [ 3.6617e-01,  2.9218e-01,  2.4028e-02,  ..., -2.0067e-04,\n           -1.1130e-01, -1.2831e-01],\n          [ 4.3841e-01, -1.9646e-01, -2.7185e-02,  ..., -6.6065e-01,\n            3.1660e-02,  2.8755e-02],\n          ...,\n          [ 2.4336e-01,  1.5147e-01, -4.0727e-01,  ...,  1.4628e-01,\n            2.9974e-02, -1.9604e-01],\n          [ 6.2762e-01,  3.9751e-01, -2.2898e-01,  ..., -5.8547e-03,\n           -1.4339e-01, -2.3870e-01],\n          [ 6.9609e-01,  3.3760e-03, -1.3496e-01,  ...,  1.7550e-01,\n           -7.9604e-02, -1.1876e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0166e-01,  6.1919e-03, -9.3783e-03,  ..., -4.7718e-02,\n           -5.6272e-01, -5.0096e-01],\n          [ 2.0590e+00, -1.9600e+00, -1.1147e+00,  ...,  1.0343e+00,\n            9.8983e-01,  2.3284e+00],\n          [-1.7154e+00, -3.9402e+00, -2.2778e+00,  ...,  9.1845e-01,\n            3.9979e+00,  2.5265e+00],\n          ...,\n          [-2.0346e+00, -1.5620e+00, -2.1357e+00,  ...,  1.7008e+00,\n            6.1601e+00,  8.7485e-01],\n          [ 1.3886e+00,  1.3091e+00, -8.0861e-01,  ..., -2.7334e-01,\n            4.8732e+00,  1.8793e+00],\n          [ 2.9248e+00,  2.4528e+00, -9.1368e-01,  ...,  2.1388e-01,\n            3.8603e+00,  1.6429e+00]],\n\n         [[-5.6379e-02,  1.4577e-03,  1.8990e-02,  ..., -1.5364e-01,\n            3.8202e-01, -2.9403e-01],\n          [-1.1381e+00, -4.8696e-01,  1.2164e+00,  ...,  1.4825e+00,\n           -1.1934e+00, -2.6655e+00],\n          [ 9.1242e-01,  1.2252e-01, -3.6523e-01,  ...,  1.1115e-02,\n           -1.4870e+00, -2.2476e+00],\n          ...,\n          [-1.7791e-01, -3.7555e-01, -1.2161e+00,  ...,  5.8366e-02,\n           -2.0055e+00, -3.0933e+00],\n          [-1.6694e+00, -4.3770e-01, -8.6095e-01,  ..., -9.8965e-02,\n           -6.4224e-01, -2.4396e+00],\n          [-6.8991e-01,  6.9570e-01, -1.6520e+00,  ...,  6.1562e-01,\n           -9.9733e-01, -2.7517e+00]],\n\n         [[ 1.8466e-02,  1.4924e-03,  1.2577e-02,  ..., -3.4818e-01,\n           -2.3574e-01, -2.2659e-02],\n          [-3.1628e-01, -1.8836e-01, -2.3994e+00,  ..., -1.8448e-01,\n            3.8541e-01,  1.1383e+00],\n          [ 4.1570e-01, -8.1133e-01, -5.0007e-01,  ...,  1.0797e+00,\n           -1.2453e+00,  8.1130e-01],\n          ...,\n          [ 5.7306e-01, -1.7102e+00, -1.1349e+00,  ...,  6.5464e-01,\n           -7.6430e-01, -1.3933e-01],\n          [-6.3758e-01, -2.0501e-01, -1.1317e+00,  ...,  1.5231e+00,\n           -9.7394e-01, -3.4505e-01],\n          [-8.7936e-01,  1.6732e-01,  1.6500e-02,  ...,  1.8085e+00,\n            3.6900e-01,  4.4737e-01]],\n\n         ...,\n\n         [[ 2.0947e-02, -6.7295e-03, -6.2162e-02,  ..., -1.3262e-01,\n            1.1672e+00,  8.1977e-01],\n          [-4.1865e+00, -2.6542e+00, -2.7694e+00,  ...,  2.7021e-01,\n           -5.0194e+00, -8.1190e-01],\n          [-1.7897e+00, -8.9176e-01, -1.9546e+00,  ..., -8.6345e-01,\n           -5.0271e+00, -7.0862e-01],\n          ...,\n          [ 1.1249e+00,  8.6208e-01,  1.1570e+00,  ...,  5.2513e-02,\n           -6.3273e+00,  8.7164e-02],\n          [ 1.5300e+00,  1.0404e+00,  5.0218e-01,  ..., -8.9805e-01,\n           -3.1471e+00, -1.1067e+00],\n          [ 1.2062e+00,  1.5224e+00,  2.3872e+00,  ...,  3.7257e-02,\n           -6.3237e+00,  2.5327e-01]],\n\n         [[-1.0139e-02, -8.0883e-03, -4.1945e-02,  ...,  1.4360e+00,\n           -3.6248e-01,  9.7605e-01],\n          [ 8.9840e-01,  2.1517e+00, -3.8953e-01,  ..., -3.9081e+00,\n            1.1726e+00, -1.3648e+00],\n          [ 1.7977e+00,  7.8718e-01,  1.1889e+00,  ..., -4.4897e+00,\n            2.2454e+00, -1.9135e+00],\n          ...,\n          [-2.4455e-01,  4.0773e-04,  2.1442e-01,  ..., -5.0472e+00,\n            2.9887e+00, -1.8345e+00],\n          [-1.7546e+00, -9.5721e-01,  1.3035e+00,  ..., -5.8629e+00,\n            1.4618e+00, -1.4617e+00],\n          [-6.8578e-01, -1.5225e-01,  1.0087e+00,  ..., -5.0745e+00,\n            1.5732e+00, -1.4222e+00]],\n\n         [[-2.0678e-02, -2.5024e-05, -2.5933e-02,  ...,  9.0611e-01,\n            1.1658e+00, -1.5922e-02],\n          [ 5.4387e-01, -1.5937e+00, -4.2302e-01,  ...,  8.4128e-02,\n           -3.1649e+00, -6.3091e-01],\n          [ 2.1531e+00, -3.2726e-01, -8.0747e-01,  ..., -9.3588e-01,\n           -1.2575e+00, -2.0457e+00],\n          ...,\n          [-1.9525e+00,  9.4569e-01,  1.2079e+00,  ..., -2.9127e+00,\n           -6.6631e-01, -4.0533e+00],\n          [-2.2828e+00,  5.9731e-01,  1.1725e+00,  ..., -1.2854e+00,\n           -1.6468e+00, -2.0581e+00],\n          [-8.2825e-01, -4.9824e-01,  2.1773e+00,  ..., -6.2644e-01,\n           -1.7132e+00, -1.9862e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3526e-02, -7.9978e-03, -6.1671e-03,  ...,  1.7907e-02,\n           -1.5949e-02, -1.2638e-02],\n          [-1.6625e-01, -7.3503e-02,  3.6931e-01,  ..., -4.4330e-01,\n            1.7235e-01,  8.7753e-01],\n          [-1.5399e-01,  2.1759e-01,  1.5064e-01,  ...,  1.4731e-02,\n            1.5621e-02,  8.8263e-02],\n          ...,\n          [-3.0861e-01,  4.5957e-01,  1.2916e-01,  ..., -2.9938e-02,\n           -6.1752e-02, -5.2606e-01],\n          [-3.3056e-01,  2.1115e-01, -1.8068e-01,  ...,  5.4267e-02,\n            7.9759e-02, -3.4859e-01],\n          [ 3.3028e-02,  1.7922e-01,  1.0573e-01,  ..., -2.1721e-01,\n            5.2501e-01, -4.0131e-01]],\n\n         [[-4.8035e-02,  1.9834e-02,  3.9587e-03,  ...,  1.7674e-02,\n            1.0605e-02,  3.7298e-04],\n          [ 1.6199e-01, -1.5647e-01,  2.5685e-01,  ..., -5.2654e-01,\n           -2.7026e-02, -5.9948e-02],\n          [ 2.1545e-01, -1.0643e-01,  4.2412e-02,  ...,  3.6600e-02,\n           -9.2282e-02, -6.1496e-02],\n          ...,\n          [ 3.1842e-01,  1.9338e-01, -3.2992e-01,  ..., -4.8378e-01,\n            3.4042e-02, -2.8658e-01],\n          [ 2.1974e-01,  1.9647e-01, -8.0642e-02,  ..., -1.7587e-01,\n           -1.4707e-01,  3.5467e-01],\n          [ 4.8984e-01,  6.0971e-02, -2.0218e-01,  ..., -5.5265e-02,\n           -1.0686e-02, -4.7457e-01]],\n\n         [[-8.0104e-03, -2.1802e-02,  1.5194e-03,  ..., -4.4974e-02,\n            3.2744e-02,  2.1210e-02],\n          [ 1.2057e-01,  1.1131e-02, -1.6751e-02,  ...,  4.2662e-01,\n            3.3568e-01,  1.9689e-01],\n          [ 1.8086e-01,  3.0699e-01, -8.3206e-02,  ..., -6.2404e-02,\n            4.9114e-01,  7.2837e-03],\n          ...,\n          [ 2.0506e-01,  2.7964e-01,  2.9877e-01,  ...,  5.7278e-02,\n            5.4048e-01, -6.4822e-02],\n          [ 3.4822e-01,  8.7741e-02,  2.1238e-01,  ..., -1.2006e-01,\n            4.1560e-01, -7.6328e-02],\n          [-7.0437e-02,  5.7161e-02,  1.2167e-01,  ..., -1.1188e-01,\n            1.5424e-01, -9.5308e-02]],\n\n         ...,\n\n         [[ 3.5515e-03, -2.1943e-02, -2.0210e-03,  ..., -1.0336e-02,\n            2.3350e-02, -1.0455e-02],\n          [ 2.4865e-01,  4.3538e-01,  4.5029e-01,  ..., -7.2358e-02,\n            1.0188e-01, -1.8383e-01],\n          [-1.0518e-01,  5.2964e-01,  3.7270e-01,  ..., -6.7872e-02,\n            1.3131e-01, -2.8911e-01],\n          ...,\n          [ 3.0064e-01,  5.5310e-01, -5.4533e-01,  ...,  1.4647e-01,\n           -1.5924e-01, -1.5693e-01],\n          [ 3.8755e-01,  3.9562e-01, -5.5967e-01,  ...,  1.4331e-01,\n           -9.4329e-02, -3.2281e-01],\n          [-3.3547e-02,  3.9899e-01, -2.8447e-01,  ..., -6.4645e-02,\n            6.6053e-02, -1.9320e-01]],\n\n         [[ 1.4852e-03,  9.5778e-03,  2.6551e-03,  ..., -1.0983e-02,\n            3.9189e-03,  1.4490e-02],\n          [ 1.8509e-02,  1.6305e-01,  1.6417e-01,  ...,  1.5492e-02,\n           -1.6470e-01,  1.1245e-01],\n          [-4.3813e-01,  3.9587e-01, -1.9904e-01,  ..., -2.9899e-01,\n            3.3618e-02, -3.4078e-02],\n          ...,\n          [-3.3382e-01,  5.3112e-01, -2.7433e-01,  ...,  1.8164e-01,\n            6.9968e-02, -5.8562e-02],\n          [-3.6744e-01,  6.0500e-01, -4.2692e-01,  ...,  1.8678e-01,\n            2.2787e-01,  1.1091e-01],\n          [-5.3681e-01,  6.5478e-01, -2.8589e-01,  ..., -3.1608e-01,\n           -2.2584e-01, -3.8646e-01]],\n\n         [[-2.9645e-01, -1.9910e-02, -1.7448e-02,  ...,  1.3408e-02,\n           -3.6502e-02, -1.3350e-02],\n          [ 7.8251e-01, -7.2778e-01,  9.3247e-01,  ...,  4.0642e-01,\n           -2.4694e-01,  6.0712e-01],\n          [ 6.3364e-01, -1.7825e-01,  2.1098e-01,  ..., -1.4767e-01,\n           -6.5873e-02,  1.0685e-02],\n          ...,\n          [ 2.3564e-01,  4.9659e-03,  2.9801e-01,  ..., -9.7692e-02,\n           -2.0164e-01,  1.4818e-01],\n          [ 5.6062e-01, -1.8731e-01, -3.0178e-01,  ..., -1.6078e-01,\n            5.3026e-04,  1.7745e-01],\n          [ 3.5423e-01, -2.3672e-01, -7.3669e-02,  ...,  1.7488e-01,\n            2.8239e-01,  1.1818e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-4.9875e-02,  2.4972e-03, -1.3115e-02,  ..., -1.0506e-01,\n           -1.0948e+00,  7.7103e-01],\n          [ 1.4863e+00,  1.5200e+00, -1.1959e+00,  ..., -1.3442e+00,\n            3.9415e+00, -5.7495e-01],\n          [-1.0452e-01,  3.3289e-01,  1.0234e-01,  ..., -2.8023e+00,\n            4.7230e+00, -3.0324e+00],\n          ...,\n          [-1.1804e-01, -2.6029e-01,  5.8443e-01,  ..., -9.4457e-01,\n            3.2178e+00, -3.4486e+00],\n          [-2.7280e-02, -4.2073e-01,  8.1108e-01,  ..., -3.5189e-01,\n            4.3311e+00, -3.7826e+00],\n          [-7.3906e-02, -1.3747e+00,  3.9041e-02,  ..., -6.8392e-01,\n            3.2543e+00, -2.6435e+00]],\n\n         [[-3.5304e-02,  2.9584e-03, -2.7550e-02,  ..., -3.4000e-01,\n            1.1422e+00,  2.6492e-02],\n          [-5.6816e-02,  4.5117e-03, -1.3083e+00,  ...,  1.1244e+00,\n           -9.8961e-02,  1.7672e+00],\n          [ 1.8967e+00,  6.0789e-01,  1.9745e-01,  ..., -6.9852e-01,\n           -1.4396e+00,  2.8527e-01],\n          ...,\n          [-5.7954e-01,  6.8881e-01,  7.5959e-01,  ..., -6.2134e-01,\n           -1.2529e+00,  5.6745e-01],\n          [-1.5116e+00,  3.8163e-01,  1.6837e+00,  ...,  4.5959e-01,\n           -1.1081e+00,  9.8418e-01],\n          [-1.2084e+00, -1.3505e-01,  2.4064e+00,  ..., -2.4949e-01,\n            7.0691e-01,  7.7508e-01]],\n\n         [[ 3.7110e-02,  1.0985e-01,  7.0407e-02,  ..., -9.0719e-01,\n           -1.1212e+00,  1.1719e+00],\n          [-4.4360e+00,  2.4716e+00,  1.2563e+00,  ...,  3.4452e+00,\n            1.7526e+00, -2.0145e-01],\n          [-3.4237e+00,  1.0477e+00,  2.8551e+00,  ...,  2.7749e+00,\n            9.4620e-01, -1.3547e+00],\n          ...,\n          [ 2.9569e+00, -1.6506e+00,  4.2026e+00,  ...,  4.4533e+00,\n            1.0390e+00, -1.0966e+00],\n          [ 4.2534e+00, -2.2310e+00,  2.0792e+00,  ...,  2.6211e+00,\n            1.2067e+00, -7.3345e-01],\n          [ 3.0529e+00, -1.4761e+00,  4.9123e-01,  ...,  3.2055e+00,\n            1.5274e+00, -1.3190e+00]],\n\n         ...,\n\n         [[ 2.9833e-03, -9.0822e-03,  1.8295e-02,  ...,  6.1501e-01,\n           -1.1246e-01,  8.5372e-01],\n          [ 3.3304e-01, -6.4003e-01, -1.0934e+00,  ..., -3.9152e+00,\n            4.6173e-01, -5.7613e-01],\n          [-5.1271e-02, -3.1483e-01, -2.7131e-01,  ..., -3.7386e+00,\n           -2.9720e-01, -2.6095e+00],\n          ...,\n          [-2.3231e-01,  7.7547e-01, -3.5580e-01,  ..., -4.7908e+00,\n            1.2966e+00, -2.2226e+00],\n          [-2.2539e-03,  1.0059e+00,  2.9037e-01,  ..., -3.9186e+00,\n            3.1690e+00, -2.5507e+00],\n          [-2.3137e-01,  4.1141e-01,  6.0183e-01,  ..., -4.8506e+00,\n            3.3978e+00, -3.1643e+00]],\n\n         [[ 3.1923e-01,  6.5008e-02, -1.5732e-01,  ...,  2.5957e-01,\n           -6.0922e-01,  5.0128e-01],\n          [ 9.7237e+00,  4.0054e+00, -2.1038e+00,  ...,  1.1219e+00,\n           -1.1703e+00,  1.6707e-01],\n          [ 7.4607e-01,  4.6296e+00, -1.1777e+00,  ...,  1.3249e+00,\n           -1.6541e+00,  1.2219e+00],\n          ...,\n          [-1.0187e+01, -5.7691e-01,  2.1336e+00,  ..., -3.7384e-02,\n           -9.9703e-01,  1.5344e+00],\n          [-2.8958e+00, -4.0741e+00,  2.4365e+00,  ...,  1.3634e-01,\n           -1.3923e+00,  1.4176e+00],\n          [ 7.5061e+00, -3.6183e+00,  3.2269e+00,  ...,  6.6178e-01,\n           -1.4693e+00,  1.5031e+00]],\n\n         [[-4.7712e-02, -1.6998e-02,  4.7513e-02,  ..., -2.0291e+00,\n           -3.1721e-01, -1.9669e+00],\n          [ 4.6644e-01, -2.0919e-01,  3.7741e+00,  ...,  1.8680e+00,\n            9.3282e-01,  4.8208e-01],\n          [ 1.7686e+00, -1.0605e+00,  2.7540e+00,  ...,  4.3334e+00,\n           -7.2462e-01,  1.3663e+00],\n          ...,\n          [ 7.0148e-01, -1.6251e-01,  9.9140e-02,  ...,  4.4427e+00,\n           -2.3735e+00,  2.5939e+00],\n          [-1.2951e+00,  1.2367e+00,  2.1290e-03,  ...,  4.9486e+00,\n           -1.9682e-01,  2.4470e+00],\n          [-1.8347e+00,  2.0486e+00,  7.5327e-02,  ...,  4.4172e+00,\n            5.1234e-01,  1.1785e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.5693e-03,  3.5057e-02,  1.6987e-02,  ..., -3.8882e-02,\n           -8.0818e-02,  3.6926e-02],\n          [ 8.1845e-02,  6.0510e-01,  6.9635e-01,  ..., -8.5384e-01,\n            5.5322e-02, -1.0471e+00],\n          [-9.1735e-02,  2.2055e-01, -3.3842e-01,  ..., -3.2321e-01,\n            8.8266e-01,  3.1458e-01],\n          ...,\n          [-2.6894e-01, -3.2540e-01, -4.2203e-01,  ..., -2.5452e-01,\n            4.4596e-02, -5.8919e-01],\n          [ 1.0207e-01,  1.5196e-01, -8.9608e-01,  ...,  1.8337e-01,\n           -1.6134e-01, -1.1361e-02],\n          [ 2.3689e-01,  9.0550e-01,  2.4622e-01,  ...,  2.5552e-01,\n            1.2596e-01,  2.7388e-01]],\n\n         [[-8.8539e-03, -1.8422e-02, -2.0399e-02,  ...,  2.3280e-02,\n           -7.1595e-03, -4.6043e-03],\n          [ 3.3777e-03, -1.4677e-01,  3.8548e-01,  ..., -2.8452e-03,\n           -2.4198e-01,  2.2888e-01],\n          [-4.7083e-02, -3.2774e-02,  3.5672e-01,  ...,  8.3398e-02,\n            3.9802e-01,  9.0960e-02],\n          ...,\n          [ 2.5897e-01,  2.0614e-01, -2.2329e-01,  ...,  2.8248e-01,\n            3.0558e-01,  4.1906e-01],\n          [ 1.6578e-01,  1.4408e-01,  1.6520e-01,  ...,  3.1801e-01,\n            1.2584e-01,  1.9883e-01],\n          [-5.2341e-01, -2.3311e-02,  1.2760e-01,  ...,  2.7007e-01,\n           -1.2230e-01,  1.1001e-01]],\n\n         [[-6.7410e-02, -4.4090e-03,  1.6212e-02,  ..., -1.1219e-02,\n            2.1429e-03, -1.5527e-03],\n          [-7.4422e-02, -4.2096e-02,  2.0127e-01,  ..., -7.1574e-02,\n            9.6221e-02,  3.5584e-01],\n          [-1.4204e-01,  8.8811e-02,  1.2295e-01,  ...,  5.4589e-02,\n           -2.7241e-01,  2.8080e-01],\n          ...,\n          [-1.0015e+00,  2.3462e-01,  1.7334e-01,  ...,  1.5034e-01,\n            2.8653e-01, -2.0339e-01],\n          [-4.9001e-01,  4.7482e-01, -6.2534e-02,  ...,  1.9007e-01,\n           -1.1071e-01, -1.2971e-01],\n          [ 4.0019e-01,  4.1475e-01, -2.3828e-01,  ...,  3.3566e-01,\n           -2.9233e-01, -2.2862e-01]],\n\n         ...,\n\n         [[ 4.7063e-04,  3.1449e-02, -1.7503e-02,  ...,  8.2695e-03,\n           -1.0970e-02, -1.1543e-02],\n          [-1.2277e+00, -3.2604e-01, -6.1393e-02,  ..., -3.8778e-02,\n           -9.9841e-02, -5.4652e-01],\n          [ 9.1386e-02,  4.2278e-01,  9.2652e-02,  ...,  8.1694e-03,\n           -3.0594e-02,  1.4136e-01],\n          ...,\n          [-2.2290e-01, -6.7235e-02,  5.7267e-02,  ...,  2.3843e-01,\n           -2.7630e-02, -1.2387e-02],\n          [ 2.0408e-01,  3.7040e-01,  2.9052e-02,  ..., -4.8617e-02,\n            2.0986e-01,  3.6221e-01],\n          [ 4.6196e-01, -4.6471e-01, -4.3433e-02,  ...,  1.3075e-01,\n            5.5972e-02,  2.5477e-01]],\n\n         [[ 9.3577e-03,  3.9018e-03,  9.3528e-04,  ..., -5.8127e-03,\n           -1.1375e-03,  1.1215e-03],\n          [-4.8039e-01, -1.3629e-01, -7.1690e-01,  ...,  5.1239e-01,\n            3.5236e-01, -1.1517e-01],\n          [-6.2005e-01, -4.2717e-01, -4.7873e-01,  ..., -3.8481e-03,\n           -4.0798e-01, -8.2157e-01],\n          ...,\n          [-5.1856e-01, -1.4422e-01, -2.7818e-01,  ...,  1.5228e-01,\n           -2.3082e-01, -3.3898e-01],\n          [-1.9378e-01,  5.9553e-01, -1.1286e-01,  ...,  5.0783e-01,\n            4.6441e-02,  8.9126e-02],\n          [-1.1452e-01, -1.2341e-01,  1.7265e-01,  ..., -9.1119e-02,\n           -1.0231e-02,  1.4952e-01]],\n\n         [[ 1.1949e-02, -2.2031e-02,  8.2400e-03,  ...,  8.8179e-03,\n            8.7592e-03,  1.6473e-04],\n          [ 3.2632e-01, -2.0327e-01, -2.1340e-01,  ...,  8.7580e-02,\n           -2.2788e-01,  3.6569e-01],\n          [ 2.6529e-01, -2.1978e-01, -2.1967e-01,  ..., -1.4477e-01,\n           -1.6774e-01,  6.5762e-02],\n          ...,\n          [ 2.4309e-01, -2.5524e-01, -2.3644e-01,  ..., -1.4594e-01,\n           -2.1775e-01, -5.8270e-01],\n          [ 3.2517e-01, -5.5770e-01, -4.9247e-01,  ...,  8.8617e-02,\n           -3.3503e-01,  1.0231e-01],\n          [ 1.8709e-01, -2.3732e-01, -2.8045e-01,  ..., -7.9458e-02,\n           -1.2273e-01,  9.1866e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0001e-02,  5.3479e-03, -9.2337e-03,  ...,  2.9982e-02,\n            8.8365e-01, -1.0928e+00],\n          [ 8.7294e-01, -2.6140e-01,  4.2966e-01,  ..., -1.0903e+00,\n           -2.5277e+00,  1.1819e+00],\n          [ 4.6817e-01,  9.8864e-01,  7.3982e-01,  ..., -4.6623e-01,\n           -2.0306e+00,  2.4614e+00],\n          ...,\n          [-2.7831e-01,  5.0420e-01,  1.2072e+00,  ...,  3.8389e-01,\n           -1.8511e+00,  2.0652e+00],\n          [-7.9975e-01,  6.8814e-01,  6.6837e-01,  ...,  1.7069e+00,\n           -2.4216e+00,  3.3887e+00],\n          [-7.2989e-01, -6.6200e-01,  2.8208e-01,  ...,  1.5478e+00,\n           -3.1226e+00,  2.1741e+00]],\n\n         [[-6.1644e-02,  2.6081e-02,  6.2417e-02,  ...,  6.3455e-01,\n            3.6131e-02,  2.4065e-01],\n          [-2.1200e+00, -1.3107e-01, -1.4443e-01,  ..., -3.1277e-01,\n           -1.2450e+00,  1.0522e+00],\n          [ 1.3968e+00, -1.1912e+00, -2.1702e+00,  ..., -9.9641e-01,\n           -2.4775e-01,  3.1098e-01],\n          ...,\n          [ 9.3467e-01, -2.4026e+00, -1.5782e+00,  ..., -6.1785e-01,\n           -1.6054e+00,  1.0960e+00],\n          [-3.8580e-01, -6.8748e-01, -1.3747e+00,  ..., -4.0310e-01,\n           -3.4155e-01,  4.8981e-01],\n          [-1.8219e+00,  6.2990e-01, -2.6710e-02,  ...,  9.7308e-01,\n           -1.9349e+00,  1.1708e+00]],\n\n         [[-1.5109e-02, -3.5196e-03,  6.1797e-02,  ...,  6.3407e-01,\n           -5.1091e-01, -8.2480e-01],\n          [ 1.5792e+00, -1.2319e+00,  1.1765e+00,  ..., -6.3072e-01,\n            1.3554e+00,  1.6394e+00],\n          [ 1.4461e+00, -4.4069e-01,  9.2721e-01,  ..., -3.9494e-01,\n            5.3216e-01,  7.6567e-01],\n          ...,\n          [-1.2211e+00,  3.1120e-02, -1.2459e+00,  ..., -2.1893e+00,\n            2.6633e+00,  1.2852e-01],\n          [-1.3804e+00,  1.4137e+00, -1.5049e+00,  ..., -1.5894e+00,\n            1.4744e+00,  7.6856e-01],\n          [-8.3890e-01,  4.5816e-01, -1.0203e+00,  ..., -2.3383e+00,\n            2.0474e-01,  8.8204e-01]],\n\n         ...,\n\n         [[ 2.0095e-02, -1.9391e-02, -1.4458e-03,  ..., -6.3438e-01,\n           -7.4186e-02,  1.2699e+00],\n          [-7.9215e-01, -1.0645e+00, -8.1103e-01,  ...,  3.6834e-01,\n            1.0701e+00, -2.1562e+00],\n          [-1.5776e-01, -1.8322e-01, -2.8168e-01,  ..., -2.7338e-02,\n           -1.2710e+00, -2.8762e+00],\n          ...,\n          [-2.1154e-01, -1.1368e-01, -4.2548e-01,  ...,  2.2172e-01,\n            1.3162e+00, -6.0494e+00],\n          [ 4.8693e-01,  6.2472e-01, -5.3281e-01,  ..., -8.7062e-01,\n           -2.0747e+00, -4.9780e+00],\n          [-3.7886e-01,  1.0626e+00,  3.1687e-01,  ...,  2.7412e-01,\n           -5.9731e-01, -3.4808e+00]],\n\n         [[ 1.5396e-01,  1.1387e-02,  1.6595e-02,  ..., -3.2535e-01,\n            1.6872e-01, -1.8801e+00],\n          [ 6.0560e-01, -1.1477e+00, -7.4180e-01,  ...,  1.6262e-01,\n            2.3737e+00,  4.6392e-01],\n          [-2.6162e-01, -2.2885e+00, -1.3641e+00,  ..., -3.8918e-01,\n            1.7129e+00,  4.5273e+00],\n          ...,\n          [-1.7206e+00, -1.4077e+00, -1.1191e+00,  ..., -6.5098e-01,\n            7.7733e-01,  3.0853e+00],\n          [ 5.1052e-01, -4.2147e-01, -1.0513e+00,  ..., -1.1379e+00,\n            1.7694e+00,  3.7997e+00],\n          [ 2.3640e+00,  1.4077e+00, -6.7150e-01,  ..., -1.7631e-01,\n            2.6162e+00,  4.0207e+00]],\n\n         [[-9.3324e-03,  1.2826e-02,  1.0013e-02,  ...,  1.6950e-01,\n            5.9047e-01,  9.9431e-01],\n          [ 1.0309e-01,  5.6140e-02,  2.8503e-02,  ..., -2.4224e+00,\n           -2.5067e+00, -2.2298e+00],\n          [-2.9537e-01,  2.1882e-01, -2.3786e-01,  ..., -1.1905e+00,\n           -2.1368e+00, -3.1597e+00],\n          ...,\n          [-4.5620e-02, -2.2910e-02, -7.9293e-02,  ..., -1.2766e+00,\n           -2.2234e+00, -2.7992e+00],\n          [ 5.6757e-01,  4.9995e-02,  4.2694e-02,  ...,  1.8118e+00,\n           -2.6661e+00, -2.8046e+00],\n          [ 1.2093e-01,  3.0445e-01, -7.8137e-02,  ...,  4.3429e-03,\n           -1.2910e+00, -1.7973e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0155, -0.0141,  0.0053,  ..., -0.0020, -0.0138, -0.0081],\n          [ 0.8893, -0.2964,  0.6060,  ..., -0.1289,  0.1849,  0.7236],\n          [ 0.0516,  0.4002,  0.5555,  ...,  0.0818, -0.7465,  0.1008],\n          ...,\n          [ 0.0592, -0.2722, -0.1607,  ..., -0.1261, -0.0721, -0.5363],\n          [ 0.5351, -0.0285,  0.3295,  ..., -0.1638, -0.3833, -0.1046],\n          [-0.8901,  0.3488,  0.5780,  ..., -0.4821,  0.0581, -0.2783]],\n\n         [[ 0.1125,  0.0091, -0.0121,  ..., -0.0521, -0.0187,  0.0434],\n          [-0.1717, -0.0280,  0.0977,  ...,  0.2039,  0.0755,  0.0779],\n          [-0.0912, -0.1047, -0.0915,  ...,  0.2498,  0.2045, -0.1584],\n          ...,\n          [ 0.1055,  0.1303, -0.6172,  ...,  0.2406, -0.3415,  0.1561],\n          [ 0.1275,  0.6940, -0.4025,  ...,  0.4515, -0.3610,  0.4613],\n          [-0.0474,  0.1060, -0.5638,  ..., -0.1677, -0.0342, -0.1656]],\n\n         [[-0.0043,  0.0186, -0.1110,  ..., -0.0808, -0.0089, -0.0305],\n          [-0.3964, -0.1324,  0.5198,  ...,  1.0464, -0.9780,  0.0426],\n          [-0.1032,  0.1052,  0.0733,  ..., -0.1005, -0.2119, -0.1483],\n          ...,\n          [ 0.5971, -0.0133, -0.1638,  ...,  0.7413, -0.2981,  0.5895],\n          [-0.3874,  0.2512,  0.1428,  ..., -0.1238, -0.3346,  0.1015],\n          [-0.5534,  0.6762, -0.0895,  ...,  0.5219, -1.0554, -0.1006]],\n\n         ...,\n\n         [[-0.0131,  0.0143, -0.0275,  ..., -0.0653, -0.0193,  0.0376],\n          [ 0.8291, -0.1804, -0.8852,  ..., -0.2940,  0.9659,  1.1397],\n          [-0.0343, -0.3794,  0.1225,  ..., -0.3974,  0.3114, -0.2152],\n          ...,\n          [ 0.2208, -0.3272, -0.4170,  ...,  0.4110, -0.2584,  0.1152],\n          [ 0.3429,  0.2526,  0.5178,  ...,  0.0889, -0.2551, -0.0285],\n          [-0.3247,  0.7592, -0.4231,  ..., -0.3010,  0.5201, -0.0014]],\n\n         [[-0.0173, -0.0193, -0.0596,  ..., -0.0225, -0.0123,  0.6400],\n          [-0.3333, -0.1722, -0.1961,  ...,  0.1566, -0.3654, -0.0344],\n          [ 0.1247,  0.0793,  0.1349,  ...,  0.2280,  0.2710, -0.4460],\n          ...,\n          [-0.0558, -0.2370,  0.1744,  ...,  0.4919, -0.3033, -0.4331],\n          [-0.0634,  0.1525,  0.0596,  ...,  0.4171,  0.0781, -0.6077],\n          [-0.1919, -0.1908,  0.0544,  ...,  0.5652, -0.0541, -0.7545]],\n\n         [[ 0.0253, -0.0222, -0.0151,  ...,  0.0164, -0.0272,  0.0803],\n          [ 0.2412,  0.8380, -0.4417,  ...,  0.6459, -0.6842,  0.2706],\n          [-0.2601,  0.0518, -0.1952,  ...,  0.2642, -0.4465,  0.4295],\n          ...,\n          [ 0.2313, -1.3138,  0.6014,  ..., -0.3423,  0.3664, -1.1917],\n          [ 0.5604,  0.0668,  0.1291,  ..., -0.2465,  0.0116, -0.3682],\n          [-0.4762,  0.0341,  0.3839,  ..., -0.5269, -0.5123,  0.2965]]]],\n       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.5414e-02,  4.6433e-02,  1.4184e-01,  ...,  5.4812e-01,\n           -4.8990e-01,  7.3753e-01],\n          [ 4.3876e+00,  5.3694e-01,  1.1721e+00,  ...,  1.3193e+00,\n           -1.3237e+00, -2.3558e-01],\n          [ 9.9249e-01,  3.1166e-02,  3.6171e-01,  ..., -1.5124e+00,\n           -2.4290e+00, -6.7962e-01],\n          ...,\n          [-1.1431e+00, -2.3096e+00, -3.1234e-01,  ..., -2.1145e+00,\n           -1.1770e+00, -4.5518e-01],\n          [-1.2081e+00, -1.9764e+00, -1.5585e-01,  ..., -2.3098e+00,\n           -2.8112e+00, -5.7975e-01],\n          [-1.8406e+00, -1.0024e+00, -2.8180e+00,  ..., -2.2056e+00,\n           -2.6257e+00, -4.7722e-01]],\n\n         [[ 1.2824e-02, -1.6541e-02,  1.6095e-03,  ..., -1.4238e-02,\n           -1.3309e+00,  2.2263e+00],\n          [-1.3956e+00,  8.9412e-02,  1.3159e-01,  ..., -4.4196e-01,\n            8.5787e-01, -9.0655e+00],\n          [-9.7274e-01, -5.4072e-01, -4.7400e-02,  ..., -3.0700e-01,\n            2.3624e+00, -5.6773e+00],\n          ...,\n          [ 1.3576e+00, -6.6439e-01,  1.1036e+00,  ..., -1.6988e+00,\n            3.5039e+00, -9.4972e+00],\n          [ 5.6010e-01, -9.3117e-01,  2.8272e+00,  ...,  5.8490e-01,\n            3.5013e+00, -5.3245e+00],\n          [ 5.9122e-01,  8.4591e-02,  2.3271e+00,  ...,  1.8021e+00,\n            3.4855e+00, -7.5563e+00]],\n\n         [[-1.3162e-01, -1.1779e-01, -8.8633e-02,  ...,  1.2191e+00,\n           -4.0260e-01, -9.9839e-01],\n          [-2.8954e+00, -3.4110e+00, -2.7247e+00,  ...,  4.0143e-01,\n           -2.4058e-01, -8.6029e-01],\n          [ 1.9712e+00, -1.7039e-02,  1.4560e-01,  ...,  1.4807e+00,\n           -6.6160e-01,  7.3650e-01],\n          ...,\n          [ 2.6792e+00,  3.1427e+00,  1.5565e+00,  ...,  1.3970e+00,\n            2.8025e-01,  4.6396e-01],\n          [-1.7699e+00,  3.4014e+00,  2.0562e+00,  ...,  1.0994e+00,\n            7.3417e-01, -1.7674e-01],\n          [-3.7179e+00,  2.6620e+00,  2.3894e+00,  ...,  1.0780e+00,\n            7.0768e-01,  8.1163e-01]],\n\n         ...,\n\n         [[ 2.6366e-02, -1.0544e-02,  6.2874e-04,  ...,  1.4033e-01,\n           -8.2465e-01, -3.7078e-01],\n          [ 1.6810e+00, -8.1752e-03, -1.3071e+00,  ...,  1.3426e+00,\n            2.8860e+00, -9.1175e-01],\n          [ 3.1420e+00,  1.8545e+00, -2.0833e+00,  ...,  2.5170e+00,\n            2.2813e+00, -2.3463e-01],\n          ...,\n          [-3.6326e+00,  2.8218e+00, -1.7481e+00,  ...,  4.2701e+00,\n            4.2890e+00, -3.0365e+00],\n          [-3.9798e+00,  1.4441e+00, -2.9004e-01,  ...,  4.0922e+00,\n            3.2314e+00, -1.7111e+00],\n          [ 4.9226e-01, -1.0174e+00, -2.0602e-01,  ...,  4.6128e+00,\n            3.7229e-01, -7.3213e-01]],\n\n         [[ 5.1096e-02,  1.2720e-01,  6.0000e-02,  ..., -1.7026e+00,\n            2.6214e-01, -1.8377e+00],\n          [-3.9598e+00,  1.7712e+00,  2.9831e+00,  ...,  2.2901e+00,\n           -7.9707e-01,  2.7464e+00],\n          [-2.6725e+00, -1.8800e-01,  2.2646e+00,  ...,  1.1920e+00,\n           -4.4188e-01,  2.6991e+00],\n          ...,\n          [ 1.1788e+00, -3.0626e+00,  8.2353e-01,  ...,  4.1268e+00,\n           -5.6293e-01,  2.0691e+00],\n          [ 3.1263e+00, -1.6974e+00, -2.8394e+00,  ...,  1.7192e+00,\n            1.1469e+00,  4.7347e+00],\n          [ 1.9080e+00, -1.7398e+00, -3.5397e+00,  ...,  3.0596e+00,\n            5.7570e-01,  1.9988e+00]],\n\n         [[ 1.9741e-02, -1.3131e-02,  1.8652e-02,  ..., -9.7016e-01,\n           -5.0637e-02,  4.6040e-01],\n          [-2.7769e-01, -5.1565e-01, -1.4570e-01,  ...,  2.3636e+00,\n            2.2137e+00, -1.0835e+00],\n          [-4.5133e-01,  1.6131e-01,  6.5179e-01,  ...,  3.1053e+00,\n            9.6139e-01,  3.5502e-01],\n          ...,\n          [ 3.0989e-01,  2.3544e-01, -5.7656e-01,  ...,  1.9273e+00,\n            8.0125e-01, -1.4189e+00],\n          [ 6.2809e-01,  1.4889e-01, -2.7151e-01,  ...,  3.2865e+00,\n            2.2598e-01, -1.6323e+00],\n          [ 6.7133e-02,  5.2969e-01, -7.0295e-01,  ...,  2.6760e+00,\n            4.5591e-02, -2.5868e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.4963e-02,  2.4116e-02, -1.4267e-02,  ...,  1.2166e-03,\n            2.0411e-02, -9.7046e-04],\n          [ 2.2547e-01,  1.4090e-01, -1.3826e-01,  ...,  2.4552e-02,\n           -2.0530e-01, -1.1754e-01],\n          [ 5.6882e-02,  1.7094e-01, -4.2025e-01,  ..., -9.0925e-02,\n           -1.7636e-01,  2.0896e-01],\n          ...,\n          [-2.9027e-01,  9.1443e-03, -4.6778e-01,  ...,  2.0404e-02,\n           -7.6829e-01, -8.6308e-02],\n          [-4.3335e-01,  3.2009e-01, -6.4599e-01,  ...,  2.0640e-01,\n           -6.1962e-01,  6.1385e-02],\n          [-3.5915e-01,  2.0407e-01, -4.2703e-01,  ...,  5.8975e-02,\n           -4.5818e-01,  4.4285e-01]],\n\n         [[ 3.4210e-02, -3.4803e-03, -1.2506e-03,  ..., -2.4340e-02,\n           -3.4289e-02,  4.0968e-02],\n          [-2.5548e-02,  8.6896e-01, -3.9595e-01,  ...,  5.3780e-01,\n           -2.1199e-01,  1.3437e+00],\n          [-1.8097e-01,  2.0235e-01,  3.2570e-02,  ..., -9.3057e-02,\n            2.3465e-01,  3.6229e-01],\n          ...,\n          [-5.9932e-02,  6.5331e-01,  2.2758e-01,  ..., -8.5440e-02,\n           -1.8942e-02,  5.7877e-01],\n          [ 1.5629e-01,  3.0888e-01,  1.4111e-01,  ...,  3.8784e-01,\n            3.4181e-01,  3.7496e-01],\n          [ 3.0771e-01, -1.3075e-01,  2.9143e-01,  ..., -2.0617e-01,\n           -3.7792e-01,  2.2385e-01]],\n\n         [[ 9.4470e-02,  1.9155e-01,  3.5195e-02,  ..., -3.0459e-02,\n            3.4772e-01, -7.3498e-03],\n          [-5.1428e-01,  3.0442e-01,  4.2104e-01,  ..., -5.7540e-01,\n           -1.5479e+00,  3.6643e-01],\n          [ 7.6056e-02,  1.6991e-01,  4.9027e-01,  ...,  1.1887e-01,\n           -2.2015e+00,  4.4847e-01],\n          ...,\n          [ 1.4213e-01,  5.0866e-01,  1.2624e-01,  ..., -2.8348e-01,\n           -2.0461e+00,  2.5732e-01],\n          [ 1.4330e-02, -2.6393e-01,  4.7206e-01,  ..., -2.3471e-01,\n           -2.9259e+00, -3.8417e-01],\n          [-1.4677e-01,  2.3432e-01,  1.5036e-02,  ..., -2.7339e-01,\n           -2.3934e+00,  2.3624e-02]],\n\n         ...,\n\n         [[-8.7270e-02, -6.6465e-02, -3.0371e-02,  ...,  1.7711e-02,\n            6.4740e-02, -3.5456e-03],\n          [-6.0243e-01,  3.2345e-02, -9.2731e-01,  ..., -2.6678e-01,\n            2.1238e+00, -8.8268e-01],\n          [-2.0218e-02, -2.0306e-01, -2.8669e-01,  ...,  4.7327e-01,\n           -2.6078e-01,  7.2375e-03],\n          ...,\n          [ 3.9089e-01,  1.1831e+00,  5.6502e-01,  ..., -7.5588e-01,\n           -7.9102e-01,  7.6052e-01],\n          [-1.4055e+00, -6.4177e-01, -5.4610e-01,  ..., -6.1377e-01,\n            5.9486e-02,  8.7491e-01],\n          [ 9.0679e-01,  5.2043e-02,  8.9971e-01,  ...,  3.3745e-01,\n            1.2728e+00, -7.4647e-02]],\n\n         [[ 2.7315e-02,  9.0491e-02, -7.3440e-02,  ..., -2.2172e-03,\n            4.7945e-02,  2.3947e-02],\n          [-4.5239e-02, -4.2668e-01,  4.3743e-01,  ..., -2.0161e-01,\n           -7.7504e-02, -3.8835e-01],\n          [-2.5332e-01, -1.4399e-01, -8.7746e-02,  ...,  2.9873e-01,\n           -1.4090e-01,  1.6136e-01],\n          ...,\n          [ 3.7944e-01,  4.1455e-01,  3.3392e-01,  ..., -2.2405e-01,\n           -4.0171e-01,  1.4341e-01],\n          [-2.0055e-01,  5.0283e-01, -2.5512e-01,  ..., -1.3557e-01,\n           -3.7245e-01, -1.0968e-01],\n          [ 8.1803e-02, -1.1857e+00,  8.5675e-01,  ..., -2.0428e-01,\n            1.8285e-01, -1.6003e-02]],\n\n         [[ 2.5544e-02, -4.4024e-02, -3.5656e-02,  ...,  9.9875e-03,\n            5.0772e-02,  1.3128e-03],\n          [-8.6449e-01, -1.1419e+00,  1.4385e+00,  ..., -2.5004e-01,\n           -1.8690e-01,  6.5207e-01],\n          [-2.7724e-01,  1.3995e-01,  2.3331e-01,  ..., -2.2554e-01,\n            2.5031e-01,  4.4207e-02],\n          ...,\n          [-9.7353e-02, -1.2743e-01,  3.8092e-01,  ..., -1.7040e+00,\n            1.8006e-01,  6.6705e-01],\n          [ 9.8546e-01,  2.8762e-01, -2.3287e-01,  ..., -2.0138e-01,\n           -2.4919e-01,  6.5736e-02],\n          [-1.5538e+00,  9.0598e-01,  6.9267e-01,  ...,  6.6153e-02,\n            3.3056e-01, -3.1213e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "traced_model = torch.jit.trace(model.model, (inputs.input_ids, inputs.attention_mask))\n",
    "traced_model.save(\"llama3.2-1b-quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tvm\n",
      "  Downloading tvm-1.0.0.tar.gz (5.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting appdirs (from tvm)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting docopt (from tvm)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting inform (from tvm)\n",
      "  Downloading inform-1.32-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting quantiphy (from tvm)\n",
      "  Downloading quantiphy-2.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting arrow (from inform->tvm)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages (from inform->tvm) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages (from arrow->inform->tvm) (2.9.0.post0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->inform->tvm)\n",
      "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading inform-1.32-py3-none-any.whl (39 kB)\n",
      "Downloading quantiphy-2.20-py3-none-any.whl (41 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
      "Building wheels for collected packages: tvm, docopt\n",
      "  Building wheel for tvm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tvm: filename=tvm-1.0.0-py3-none-any.whl size=5086 sha256=604430a85a3ac36987f17d497afe16000c863840592e759c61ae06f5863ac071\n",
      "  Stored in directory: /Users/olivergrainge/Library/Caches/pip/wheels/3c/5e/e6/2a4aace02fdd238400c8dc25848cda8b5a6367f1295453f3f9\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=b489680a2d406cce71bd00171d52061e370f6469f21bc59ca35b7a9fc6fb8bb6\n",
      "  Stored in directory: /Users/olivergrainge/Library/Caches/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built tvm docopt\n",
      "Installing collected packages: docopt, appdirs, types-python-dateutil, quantiphy, arrow, inform, tvm\n",
      "Successfully installed appdirs-1.4.4 arrow-1.3.0 docopt-0.6.2 inform-1.32 quantiphy-2.20 tvm-1.0.0 types-python-dateutil-2.9.0.20241003\n"
     ]
    }
   ],
   "source": [
    "!pip install tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
