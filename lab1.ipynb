{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Optimizing Generative AI Workloads with Embedded ARM Processors\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the **Optimizing Generative AI Workloads with Embedded ARM Processors** lab! In this hands-on session, you will explore how ARMv8-A advanced vector processing capabilities can significantly accelerate computationally intensive tasks commonly found in artificial intelligence (AI) applications. By the end of this lab, you will gain a deep understanding of how low-level optimizations can enhance performance and how to leverage these optimizations within high-level AI frameworks like PyTorch.\n",
    "\n",
    "**Requirements**: To complete this lab, you will need an ARMv8-A 64-bit system running a Linux-based operating system, such as a Raspberry Pi 4 or 5. The lab has been thoroughly tested on the Raspberry Pi 5 for compatibility and performance.\n",
    "\n",
    "### **Why ARM Vector Instructions?**\n",
    "\n",
    "ARM processors are ubiquitous in modern computing, powering everything from smartphones to edge devices and increasingly, servers and supercomputers. Their architecture is designed for energy efficiency and performance, making them ideal for deploying AI models in diverse environments. One of the key features that enable ARM processors to excel in AI workloads is their support for **vector instructions**, such as NEON and i8mm, which allow for parallel processing of multiple data points in a single instruction cycle. ARMv8-A is a member of the ARM architecture family, representing the 8th generation of ARM's advanced architecture with a focus on 64-bit computing, high performance, and scalable designs. It builds upon the energy-efficient foundation of earlier ARM architectures while introducing advanced features such as support for AArch64 (the 64-bit instruction set), enhanced vector processing capabilities, and improved cryptographic extensions. ARMv8-A is widely adopted in devices a range of mobile and embedded device including the **raspberry pi 4/5**\n",
    "\n",
    "### **Lab Objectives**\n",
    "\n",
    "The objectives of this lab are as follows: \n",
    "\n",
    "1. **Matrix Multiplication Optimization**:\n",
    "   - Analyze three C-based matrix multiplication implementations (`naive`, `fp32_neon`, and `int8_neon`) to understand their performance differences.\n",
    "   - Utilize ARMv8's NEON vector processing capability to accelerate matrix multiplications. The fundamental operation of AI workloads. \n",
    "\n",
    "2. **Benchmark PyTorch Operations**:\n",
    "   - Measure the performance of PyTorch's matrix multiplication operations in different precisions. \n",
    "   - Examine the generated assembly code to identify how pytorch accelerates matrix multiplications\n",
    "\n",
    "3. **Run Inference on a Language Model**:\n",
    "   - Load and run inference on the state-of-the-art small language model, Llama3.2-1B.\n",
    "   - Explore its computational graph to understand the underlying operations.\n",
    "\n",
    "4. **Eager vs. Graph Execution**:\n",
    "   - Compare PyTorch's Eager mode and Graph Execution.\n",
    "   - Use `torch.compile` to minimize overhead and improve runtime performance.\n",
    "\n",
    "5. **Apply Integer Quantization**:\n",
    "   - Learn how integer quantization can reduce model size and accelerate inference.\n",
    "   - Implement and evaluate quantization on matrix multiplication operations.\n",
    "   - Apply Integer Quantization to Lamma3.2-1B and record it's latency speed ups and memory reductions. \n",
    "\n",
    "\n",
    "\n",
    "### **What You Will Learn**\n",
    "\n",
    "- **Low-Level Optimizations**: Understand how ARM's NEON SIMD instructions can accelerate matrix computations.\n",
    "- **Performance Benchmarking**: Develop skills to benchmark operations effectively and analyze performance trade-offs.\n",
    "- **Framework Integration**: Explore how low-level optimizations can complement high-level frameworks like PyTorch.\n",
    "- **Quantization Techniques**: Learn to implement quantization for efficient AI workloads on resource-constrained devices.\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "To follow this lab, you should have a basic understanding of:\n",
    "\n",
    "- **C Programming**: Familiarity with C syntax and basic memory management.\n",
    "- **Python Programming**: Experience writing and running Python scripts.\n",
    "- **Matrix Operations**: A general understanding of matrix multiplication\n",
    "\n",
    "### **Getting Started**\n",
    "\n",
    "This lab begins with an exploration of matrix multiplication operators, the core computational components of AI workloads. Understanding and optimizing these operations can significantly enhance the inference performance of generative AI (GenAI) models. In this section, we will implement and compare three matrix multiplication approaches. While they are mathematically identical, their differing implementations result in vastly different performance outcomes. We will implement: \n",
    "\n",
    "1. **Naive Kernel (`src/c/kernels/naive.c`)**  \n",
    "   - A simple, baseline implementation of the matrix multiplication to provide a reference point for performance.\n",
    "\n",
    "2. **FP32 NEON Kernel (`src/c/kernels/fp32_neon.c`)**  \n",
    "   - A matrix multiplication optimized for single-precision floating-point operations using ARM NEON SIMD instructions to leverage vectorized computation.\n",
    "\n",
    "3. **INT8 NEON Kernel (`src/c/kernels/int8_neon.c`)**  \n",
    "   - A integer matrix multiplication tailored for 8-bit operations, utilizing NEON SIMD to maximize throughput for lower-precision workloads.\n",
    "\n",
    "By analyzing these implementations, you will gain insight into the performance trade-offs and benefits of hardware-specific optimizations that ARM can offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematics of Matrix Multiplication and Its Importance in Generative AI Models**\n",
    "\n",
    "#### **Basic Mathematics of Matrix Multiplication**\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra with wide-ranging applications in computer science, engineering, and especially in machine learning and AI. Given two matrices **A** and **B**, the product **C = A × B** is defined only if the number of columns in **A** matches the number of rows in **B**.\n",
    "\n",
    "Mathematically, if **A** is an *m × n* matrix and **B** is an *n × p* matrix, then their product **C** will be an *m × p* matrix. The element at position *(i, j)* in matrix **C** is computed as:\n",
    "\n",
    "$$C_{i,j} = \\sum_{k=1}^{n} A_{i,k} \\times B_{k,j}$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider matrices **A** (2×3) and **B** (3×2):\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "b_{31} & b_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Their product **C = A × B** (2×2) is:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{11} &= a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} \\\\\n",
    "c_{12} &= a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} \\\\\n",
    "c_{21} &= a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} \\\\\n",
    "c_{22} &= a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### **Computational Complexity**\n",
    "\n",
    "The naive matrix multiplication algorithm has a time complexity of **O(n³)**, which becomes computationally expensive for large matrices. Optimizations, such as those leveraging vector instructions, aim to reduce the constant factors and improve cache utilization, thereby enhancing performance without altering the theoretical complexity.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### **Implementation of Naive Matrix Multiplication Kernel**\n",
    "\n",
    "The naive implementation of matrix multiplication directly follows the mathematical definition. It uses three nested loops to compute the dot product of rows from matrix **A** and columns from matrix **B** for every element in the resulting matrix **C**. This is the most straightforward and intuitive approach but is computationally expensive due to its **O(n³)** time complexity.\n",
    "\n",
    "#### **Code Explanation**\n",
    "\n",
    "1. **Inputs**:\n",
    "   - **A**, **B**: Flattened 2D matrices (stored as 1D arrays in row-major order) to be multiplied.\n",
    "   - **C**: Flattened 2D matrix (1D array, also in row-major) to store the result.\n",
    "   - **N**: Size of the square matrices (number of rows/columns).\n",
    "\n",
    "2. **Procedure**:\n",
    "   - The outer two loops iterate over the rows **i** and columns **j** of the resulting matrix **C**.\n",
    "   - The innermost loop calculates the dot product for each element **C[i, j]** by summing the product of corresponding elements from row **i** of **A** and column **j** of **B**.\n",
    "\n",
    "3. **Performance**:\n",
    "   - This naive implementation is simple, performing one operation at a time but does not leverage advanced optimization techniques, such as blocking, vectorization, or parallelism.\n",
    "\n",
    "#### **Naive Kernel Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src/c/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/naive.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/naive.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "void matrix_multiply_naive(float* A, float* B, float* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            C[i * N + j] = 0;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i * N + j] += A[i * N + k] * B[k * N + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to NEON SIMD Instructions**\n",
    "\n",
    "NEON (ARM Advanced SIMD) is a set of SIMD (Single Instruction, Multiple Data) instructions designed to accelerate data-parallel operations on ARM processors. By leveraging NEON, computations on matrices, such as matrix multiplication, can be vectorized to process multiple data points simultaneously, thereby reducing execution time and increasing throughput.\n",
    "\n",
    "#### **FP32 Matrix Multiplication Using NEON Instructions**\n",
    "\n",
    "1. **Vectorized Processing**:\n",
    "   - In this implementation, the function processes four `float32` elements at a time, utilizing NEON's 128-bit registers (`float32x4_t`).\n",
    "   - The accumulation is performed using fused multiply-add operations, which minimize intermediate memory accesses.\n",
    "\n",
    "2. **Key NEON Instructions Used**:\n",
    "   - **`vld1q_f32`**: Loads four 32-bit floating-point elements into a vector register.\n",
    "   - **`vmlaq_f32`**: Performs a fused multiply-add operation on vectors.\n",
    "     \n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - The vector accumulator is reduced into a scalar using:\n",
    "     - **`vadd_f32`**: Adds low and high parts of the vector.\n",
    "     - **`vpadd_f32`**: Horizontally adds remaining elements for a final scalar result.\n",
    "     - **`vget_lane_f32`**: Extracts a specific element (lane) from a vector, used here to retrieve the final scalar value from the result of `vpadd_f32`.\n",
    "       \n",
    "       \n",
    "4. **Advantages Over Naive FP32 Implementation**:\n",
    "   - **Efficient Memory Access**: The naive implementation loads a single value at a time from memory, which can result in significant memory latency. The NEON implementation processes four elements simultaneously, reducing memory fetch overhead.\n",
    "   - **Reduced Loop Iterations**: By processing multiple elements in parallel, the NEON implementation reduces the number of loop iterations required in the inner loop, significantly improving performance for large matrices.\n",
    "   - **Optimized Accumulation**: NEON's fused multiply-add (`vmlaq_f32`) performs multiplication and addition in a single instruction, minimizing intermediate storage and computation overhead, unlike the naive implementation, which performs these operations sequentially.\n",
    "   - **Hardware Acceleration**: NEON leverages specialized SIMD hardware in the ARM processor, making it much faster than the general-purpose computation used in the naive implementation.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for `float32` matrices by iterating over the rows and columns of the input matrices, processing four elements at a time in the inner loop. This approach uses NEON to accelerate the computation by leveraging SIMD parallelism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/fp32_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/fp32_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_fp32_neon(float* A, float* B, float* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            float32x4_t acc = vdupq_n_f32(0.0f); // Accumulator\n",
    "            for (int k = 0; k < N; k += 4) { // Process 4 elements at a time\n",
    "                float32x4_t a_vec = vld1q_f32(&A[i * N + k]); // Load row of A\n",
    "                float32x4_t b_vec = vld1q_f32(&B[k * N + j]); // Load column of B\n",
    "                acc = vmlaq_f32(acc, a_vec, b_vec); // Multiply-accumulate\n",
    "            }\n",
    "            // Reduce acc to a single value and store in C\n",
    "            float32x2_t sum1 = vadd_f32(vget_low_f32(acc), vget_high_f32(acc));\n",
    "            float sum = vget_lane_f32(vpadd_f32(sum1, sum1), 0);\n",
    "            C[i * N + j] = sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantized (INT8) Matrix Multiplication Using NEON SIMD**\n",
    "\n",
    "Quantized matrix multiplication uses low-precision. In this case, 8-bit integer representations to reduce memory bandwidth, power consumption, and computational complexity. ARM processors with NEON (integer 8-bit matrix multiplication) instructions provide significant acceleration by increasing the level of vectorization and benefiting from the efficiency of integer arithmetic.\n",
    "\n",
    "#### **INT8 Matrix Multiplication Using NEON Instructions**\n",
    "\n",
    "1. **Increased Levels of Vectorization**:\n",
    "   - The implementation processes eight `int8_t` elements at a time, leveraging the higher data packing density of 8-bit integers compared to `float32` (four elements at a time). This doubles the level of parallelism compared to the FP32 implementation.\n",
    "\n",
    "2. **Key NEON Instructions Used**:\n",
    "   - **`vld1_s8`**: Loads eight signed 8-bit integers into a vector register.\n",
    "   - **`vmlal_s8`**: Multiplies two vectors of signed 8-bit integers and accumulates the results into 16-bit integers.\n",
    "\n",
    "3. **Reduction Step**:\n",
    "   - **`vaddvq_s16`**: Horizontally sums the elements of a 16-bit integer vector to produce a scalar result.\n",
    "\n",
    "4. **Advantages Over FP32 Implementation**:\n",
    "   - **Higher Vectorization**: Processes eight elements at a time versus four in the FP32 version.\n",
    "   - **Integer Arithmetic**: Integer operations are inherently faster than floating-point operations on most hardware due to simpler hardware requirements.\n",
    "   - **Lower Memory Usage**: `int8_t` data consumes four times less memory than `float32`, leading to reduced cache pressure and better memory bandwidth utilization.\n",
    "   - **Energy Efficiency**: Integer computations typically consume less power, making this approach ideal for energy-constrained environments.\n",
    "\n",
    "5. **Description**:\n",
    "   - The function performs matrix multiplication for quantized `int8` matrices by iterating over the rows and columns of the input matrices. In the inner loop, eight elements are processed simultaneously using NEON SIMD instructions. The 16-bit intermediate results are accumulated, and the final reduction produces a 32-bit scalar result for each element of the output matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/kernels/int8_neon.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/kernels/int8_neon.c\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void matmul_int8_neon(int8_t* A, int8_t* B, int32_t* C, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            int16x8_t acc = vdupq_n_s16(0); // Initialize 16-bit accumulator\n",
    "\n",
    "            for (int k = 0; k < N; k += 8) { // Process 8 elements at a time\n",
    "                // Load 8 int8 elements from row of A and column of B\n",
    "                int8x8_t a_vec = vld1_s8(&A[i * N + k]);\n",
    "                int8x8_t b_vec = vld1_s8(&B[k * N + j]);\n",
    "\n",
    "                // Perform element-wise multiplication and accumulate\n",
    "                acc = vmlal_s8(acc, a_vec, b_vec);\n",
    "            }\n",
    "\n",
    "            // Reduce the 16-bit accumulator into a 32-bit scalar\n",
    "            int32_t sum = vaddvq_s16(acc); // Horizontally sum all elements in the vector\n",
    "            C[i * N + j] = sum; // Store the result in C\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compiling Each Kernel to Inspect Assembly Using GCC**\n",
    "\n",
    "With the kernel implementations ready, we can compile them into assembly code to examine how the compiler leverages NEON hardware-level optimizations to enhance performance. This step provides insights into how SIMD instructions are utilized for accelerating computations, particularly for matrix operations.\n",
    "\n",
    "#### **Steps to Compile Each Kernel**\n",
    "\n",
    "To inspect the generated assembly code for each kernel, use the following GCC command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bin\n",
    "!gcc -O0 -S -march=armv8-a+simd src/c/kernels/naive.c -o bin/naive.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/fp32_neon.c -o bin/fp32_neon.s\n",
    "!gcc -O3 -S -march=armv8-a+simd src/c/kernels/int8_neon.c -o bin/int8_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. **Naive Implementation**\n",
    "   - Uses scalar instructions exclusively, meaning each operation processes a single pair of data values at a time.\n",
    "   - Relies on the following types of ARM assembly instructions:\n",
    "     - **`ldr` (Load Register):** Loads a single value from memory into a register.\n",
    "     - **`str` (Store Register):** Stores a single value from a register into memory.\n",
    "     - **`mul` (Multiply):** Multiplies two values in registers.\n",
    "     - **`add` (Add):** Adds two values in registers.\n",
    "   - Does not utilize SIMD (Single Instruction Multiple Data) capabilities, which can process multiple data values simultaneously in a single instruction.\n",
    "   - Experiences significant overhead in memory operations due to the frequent use of `ldr` and `str` instructions for each operation, as no batching or parallelism is applied.\n",
    "\n",
    "#### Observations:\n",
    "   - Computational units are underutilized because operations are performed serially, one at a time.\n",
    "   - Memory bandwidth becomes a bottleneck as frequent loads and stores slow down processing.\n",
    "   - Best suited for small matrices or architectures without support for vectorized operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".L5:\n",
      "\tldr\tw1, [sp, 44]\n",
      "\tldr\tw0, [sp, 4]\n",
      "\tmul\tw1, w1, w0\n",
      "\tldr\tw0, [sp, 40]\n",
      "\tadd\tw0, w1, w0\n",
      "\tsxtw\tx0, w0\n",
      "\tlsl\tx0, x0, 2\n",
      "\tldr\tx1, [sp, 8]\n",
      "\tadd\tx0, x1, x0\n",
      "\tldr\ts1, [x0]\n",
      "\tldr\tw1, [sp, 44]\n",
      "\tldr\tw0, [sp, 4]\n",
      "\tmul\tw1, w1, w0\n",
      "\tldr\tw0, [sp, 36]\n",
      "\tadd\tw0, w1, w0\n",
      "\tsxtw\tx0, w0\n"
     ]
    }
   ],
   "source": [
    "!sed -n '34,50p' bin/naive.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. **NEON Vectorization**\n",
    "   - Leverages NEON SIMD instructions to perform parallel computations on multiple data values simultaneously:\n",
    "     - **`movi` (Move Immediate):** Initializes all elements of a NEON vector register to a specified immediate value (e.g., zero).\n",
    "     - **`fmul` (Floating-Point Multiply):** Multiplies corresponding elements in two NEON vector registers.\n",
    "     - **`fadd` (Floating-Point Add):** Adds corresponding elements in two NEON vector registers.\n",
    "     - **`dup` (Duplicate):** Copies a scalar value into all elements of a vector register or duplicates one element of a vector across a scalar register.\n",
    "     - **`faddp` (Floating-Point Add Pair):** Adds adjacent pairs of elements within a vector register, effectively reducing the vector size.\n",
    "   - Processes 128-bit registers, enabling parallel computation of up to 4 single-precision floating-point numbers in a single instruction.\n",
    "   - Utilizes optimized memory access patterns to minimize latency and bottlenecks.\n",
    "\n",
    "#### Observations:\n",
    "   - The extracted assembly demonstrates the use of NEON vector registers (e.g., `v0`, `v1`, `v2`) and instructions for efficient parallel floating-point computations.\n",
    "   - While the exact addresses or register assignments may vary slightly due to compiler differences, the core operations and use of NEON SIMD instructions remain consistent.\n",
    "\n",
    "***NOTE***\n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/fp32_neon.s` file to view the full assembly. Regardless of compiler variations, you should see similar operations (e.g., `movi`, `fmul`, `fadd`, `dup`, `faddp`) utilizing NEON vector registers to perform SIMD optimizations.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv1.4s, 0               // Initialize all elements of v1 to zeros\n",
    "fmul\tv0.4s, v0.4s, v2.4s   // Multiply corresponding elements of v0 and v2, store result in v0\n",
    "fadd\tv1.4s, v1.4s, v0.4s   // Add corresponding elements of v1 and v0, store result in v1\n",
    "dup\td0, v1.d[1]            // Duplicate the second 64-bit element of v1 into scalar register d0\n",
    "fadd\tv0.2s, v0.2s, v1.2s   // Add the lower two elements of v0 and v1, store result in v0.2s\n",
    "faddp\tv0.2s, v0.2s, v0.2s   // Pairwise add elements of v0.2s, reducing it to one scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv1.4s, 0\n",
      "\tfmul\tv0.4s, v0.4s, v2.4s\n",
      "\tfadd\tv1.4s, v1.4s, v0.4s\n",
      "\tdup\td0, v1.d[1]\n",
      "\tfadd\tv0.2s, v0.2s, v1.2s\n",
      "\tfaddp\tv0.2s, v0.2s, v0.2s\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/fp32_neon.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **NEON Vectorization (Integer8-based)**\n",
    "\n",
    "This implementation leverages NEON SIMD instructions for efficient integer matrix multiplication. While it does **not** utilize i8mm-specific instructions, it achieves high performance with quantized data using the following NEON operations:\n",
    "\n",
    "#### Key NEON Instructions Observed:\n",
    "   - **`movi` (Move Immediate):** Initializes all elements of a NEON vector register to a specified immediate value, such as zero.\n",
    "   - **`smlal` (Signed Multiply-Add Long):** Multiplies pairs of 8-bit integers from two vector registers, producing 16-bit results, and accumulates them into a 16-bit vector register.\n",
    "   - **`addv` (Add Across Vector):** Horizontally sums all elements of a NEON vector register into a single scalar value.\n",
    "   - **`smov` (Scalar Move):** Moves the lowest element from a NEON vector register into a scalar general-purpose register.\n",
    "\n",
    "#### Characteristics:\n",
    "   - Processes **8 `int8_t` elements at a time** due to NEON's 128-bit vector registers, allowing for significantly higher throughput compared to `float32` implementations.\n",
    "   - Accumulates intermediate results in 16-bit registers (`int16`) to avoid overflow during the computation.\n",
    "   - Reduces the final 16-bit vector to a scalar using a horizontal sum (`addv`) followed by moving the scalar value to a general-purpose register (`smov`).\n",
    "\n",
    "#### Observations:\n",
    "   - **Performance Benefits**:\n",
    "     - Processes multiple `int8` elements per instruction, maximizing the benefits of vectorization.\n",
    "     - Smaller data types (`int8` vs. `float32`) reduce memory bandwidth requirements and improve efficiency.\n",
    "   - **Precision Limitations**:\n",
    "     - Integer arithmetic lacks the precision of floating-point operations, making it best suited for quantized workloads like neural network inference where reduced precision is acceptable.\n",
    "   - **Applications**:\n",
    "     - Ideal for embedded systems, mobile devices, and other environments requiring low-power, memory-efficient AI inference.\n",
    "\n",
    "***NOTE***  \n",
    "If the code block below does not show the exact instructions described above, you can open the `bin/int8_neon.s` file to view the assembly. Regardless of compiler or system variations, the assembly will use similar operations (e.g., `movi`, `smlal`, `addv`, `smov`) to optimize the `int8` matrix multiplication using NEON SIMD instructions.\n",
    "\n",
    "#### Example Assembly Output:\n",
    "```assembly\n",
    "movi\tv0.4s, 0               // Initialize v0 to zeros\n",
    "smlal\tv0.8h, v2.8b, v1.8b   // Multiply 8-bit integers from v2 and v1, accumulate into 16-bit vector v0\n",
    "addv\th0, v0.8h             // Horizontally sum all elements in v0.8h into scalar h0\n",
    "smov\tw0, v0.h[0]           // Move the lowest 16-bit value from v0 to scalar register w0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmovi\tv0.4s, 0\n",
      "\tsmlal\tv0.8h, v2.8b, v1.8b\n",
      "\taddv\th0, v0.8h\n",
      "\tsmov\tw0, v0.h[0]\n"
     ]
    }
   ],
   "source": [
    "!grep -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' bin/int8_neon.s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BenchMarking**\n",
    "\n",
    "Lets now compute the latency of results of these three operators across different matrix sizes top emprically measure their differences. We can set the matrix sizes used in the benchmark by writing them out to **src/c/sizes.c** as seen below. Feel free to adapt the sizes yourself to see how it can effect latency, Bear in mind however large matrix multiplications are compute intensive operations!\n",
    "\n",
    "***Note***\n",
    "This code is tested on raspberry pi 5 (Cortex-A76) with matmul size of 1024 can take upto 45s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/c/sizes.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/c/sizes.c\n",
    "\n",
    "int sizes[] = {32, 64, 128, 256, 512};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the naive implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Matrix Multiplication (Size 32): 0.000196 seconds\n",
      "Naive Matrix Multiplication (Size 64): 0.001563 seconds\n",
      "Naive Matrix Multiplication (Size 128): 0.013539 seconds\n",
      "Naive Matrix Multiplication (Size 256): 0.122234 seconds\n",
      "Naive Matrix Multiplication (Size 512): 1.037380 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compile the C code with optimization level 3 and NEON SIMD extensions, outputting the binary to bin/benchmark\n",
    "!gcc -O0 src/c/benchmark_naive.c -o bin/benchmark_naive -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the floating point SIMD implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 NEON Matrix Multiplication (Size 32): 0.000008 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 64): 0.000059 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 128): 0.000462 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 256): 0.008002 seconds\n",
      "FP32 NEON Matrix Multiplication (Size 512): 0.143779 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_fp32_neon.c -o bin/benchmark_fp32_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_fp32_neon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile and record latency with the integer-8 SIMD Impelmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 Neon Matrix Multiplication (Size 32): 0.000005 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 64): 0.000035 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 128): 0.000251 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 256): 0.001872 seconds\n",
      "Int8 Neon Matrix Multiplication (Size 512): 0.033696 seconds\n"
     ]
    }
   ],
   "source": [
    "!gcc -O3 -ffast-math src/c/benchmark_int8_neon.c -o bin/benchmark_int8_neon -march=armv8-a+simd -lm\n",
    "!./bin/benchmark_int8_neon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot the results** \n",
    "Great, now the operators have been run, lets plot how there latency scales with matrix size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data from CSV files\n",
    "naive_data = pd.read_csv('results/naive_latency_results.csv')\n",
    "fp32_neon_data = pd.read_csv('results/fp32_neon_latency_results.csv')\n",
    "int8_neon_data = pd.read_csv('results/int8_neon_latency_results.csv')\n",
    "\n",
    "# Extract sizes and times\n",
    "sizes = naive_data['Matrix Size']\n",
    "naive_times = naive_data['Latency (seconds)']\n",
    "fp32_neon_times = fp32_neon_data['Latency (seconds)']\n",
    "int8_neon_times = int8_neon_data['Latency (seconds)']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, naive_times, marker='o', linestyle='-', color='r', label='Naive fp32')\n",
    "plt.plot(sizes, fp32_neon_times, marker='o', linestyle='-', color='b', label='NEON SIMD fp32')\n",
    "plt.plot(sizes, int8_neon_times, marker='o', linestyle='-', color='g', label='NEON SIMD int8')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: fp32 Naive vs fp32 SIMD vs int8 SIMD Matrix Multiplication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph, the naive approach, which processes each matrix element individually, is the slowest. The SIMD floating-point implementation offers significant speedup, but the SIMD integer implementation is the fastest. This highlights the advantage of using lower-precision operations in AI network forward passes for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pytorch**\n",
    "Having explored how ARM's SIMD capabilities in C can accelerate AI workloads, we now transition to PyTorch which is a versatile, high-level framework that balances flexibility and ease of use. PyTorch streamlines AI model development by abstracting low-level operations, enabling developers to focus on model architecture and experimentation rather than intricate implementation details.\n",
    "\n",
    "Unlike the manual coding required for SIMD operations in C, PyTorch offers built-in support for tensor computations and hardware acceleration. It automatically utilizes processor optimizations, including SIMD and ARM's NEON instructions, to enhance performance without requiring developers to write low-level code.\n",
    "\n",
    "In the next section, we will examine how PyTorch utilizes the ARMv8-A's vectorization capability and benchmark Python-based matrix multiplication using both `int8` and `fp32` precision. Building on this foundation, we will implement and optimize the inference of a state-of-the-art small language model, **Llama 3.2-1B**, showcasing PyTorch's powerful capabilities for handling advanced AI workloads on ARMv8.\n",
    "\n",
    "Let's start by checking the build configuration of our installed pytorch package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with:\n",
      "  - GCC 10.2\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: NO AVX\n",
      "  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-10/root/usr/bin/c++, CXX_FLAGS=-ffunction-sections -fdata-sections -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=2.5.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__config__.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output, you can see the build configuration for this version of PyTorch. Some key flags indicate how PyTorch is optimized for performance on the your device:\n",
    "\n",
    "- **`USE_OPENMP=ON`**: This library is used to parallelize matrix multiplications across threads, providing speedups on the Raspberry Pi's quad-core processor.\n",
    "- **`BLAS_INFO=ON`** BLAS (Basic Linear Algebra Subprograms) are used to perform efficient linear algebra on ARM utilizing vector processing. \n",
    "- **`USE_NNPACK=ON`**: A low-level library of operators that utilizes vectorized instructions on ARM processors to accelerate operations.\n",
    "- **`USE_MKLDNN=ON`**: While primarily designed for x86 processors, this library also includes vectorized implementations of operators optimized for the AArch64 architecture, making it compatible with devices like the Raspberry Pi 4 and 5.\n",
    "\n",
    "These build configurations show that PyTorch is equipped to take advantage of ARM's vectorization and multi-threading capabilities. \n",
    "\n",
    "Next, let's validate that PyTorch is successfully utilizing these low-level libraries to vectorize its tensor operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analyzing PyTorch's Utilization of ARM Vector Processing**\n",
    "\n",
    "To confirm PyTorch's use of ARM's vector processing capabilities, we will investigate its behavior under the hood by writing two simple scripts for matrix multiplication. These scripts replicate what we implemented in C earlier but leverage PyTorch's abstractions. \n",
    "\n",
    "The scripts will perform matrix multiplication using both 32-bit floating-point (`fp32`) and 8-bit integer (`int8`) precisions. PyTorch simplifies these operations by providing built-in functionality that abstracts away low-level details, enabling developers to focus on high-level design while still benefiting from hardware optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/python/fp_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/fp_matmul.py\n",
    "\n",
    "import torch\n",
    "a = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "b = torch.randn(1024, 1024, dtype=torch.float32, requires_grad=False)\n",
    "c = torch.mm(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/python/int8_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/python/int8_matmul.py\n",
    "import torch\n",
    "\n",
    "# Generate random int8 tensors\n",
    "a = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "b = torch.randint(-128, 128, size=(1024, 1024), dtype=torch.int8)\n",
    "c = torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using `perf` to Inspect Assembly**\n",
    "\n",
    "The following code cells use the `perf` tool to extract the assembly instructions executed by the Python scripts above. By capturing this assembly, we can analyze whether the low-level libraries that `torch.mm` relies on for matrix multiplication are effectively utilizing ARM's vector processing capabilities, such as NEON instructions.\n",
    "\n",
    "We will write the extracted assembly code to a text file for further analysis. This process allows us to verify if PyTorch's operations are optimized to leverage hardware acceleration on ARM-based systems.\n",
    "\n",
    "**Note**: In the following code cells, a 10-second timeout is applied. This is sufficient to capture a representative sample of the assembly instructions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract assembly for the floating point matmul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 18 times to write data ]\n",
      "[ perf record: Captured and wrote 4.449 MB perf.data (10544 samples) ]\n",
      "============================== Completed! ==============================\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Execution timed out!\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "signal.alarm(10)  # Set the alarm for 60 seconds\n",
    "\n",
    "try:\n",
    "    # Your long-running code here\n",
    "    !rm -rf *perf*\n",
    "    !rm -rf fp_matmul.txt\n",
    "    !sudo perf record -e instructions:u -g $(which python) src/python/fp_matmul.py\n",
    "    !sudo perf annotate > results/fp_matmul_instructions.txt\n",
    "except TimeoutError as e:\n",
    "    !rm -rf *perf*\n",
    "    print(\"============================== Completed! ==============================\")\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract assembly for the int8 matmul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ perf record: Woken up 18 times to write data ]\n",
      "[ perf record: Captured and wrote 4.438 MB perf.data (10214 samples) ]\n",
      "============================== Completed! ==============================\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Execution timed out!\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "signal.alarm(10)  # Set the alarm for 60 seconds\n",
    "\n",
    "try:\n",
    "    # Your long-running code here\n",
    "    !rm -rf *perf*\n",
    "    !rm -rf int8_matmul.txt\n",
    "    !sudo perf record -e instructions:u -g $(which python) src/python/int8_matmul.py\n",
    "    !sudo perf annotate > results/int8_matmul_instructions.txt\n",
    "except TimeoutError as e:\n",
    "    !rm -rf *perf*\n",
    "    print(\"============================== Completed! ==============================\")\n",
    "finally:\n",
    "    signal.alarm(0)  # Disable the alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets have a look inside the instructions used by pytorch for the floating point matrix multiply script and print them below. We will search for vectorized instructions such as `fmul` and `fadd` to detect their presence and ensure they are using the vector registers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00 :   10b0ac: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   10b628: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   10b7a4: add     v0.2s, v0.2s, v1.2s\n",
      "    0.00 :   2af658: fmul    v16.4s, v0.4s, v8.s[0]\n",
      "    0.00 :   2af65c: fmul    v20.4s, v0.4s, v9.s[0]\n",
      "    0.00 :   2af664: fmul    v24.4s, v0.4s, v10.s[0]\n",
      "    0.00 :   2af668: fmul    v28.4s, v0.4s, v11.s[0]\n",
      "    0.00 :   2af670: fmul    v17.4s, v1.4s, v8.s[0]\n",
      "    0.00 :   2af674: fmul    v21.4s, v1.4s, v9.s[0]\n",
      "    0.41 :   2af67c: fmul    v25.4s, v1.4s, v10.s[0]\n",
      "    0.00 :   2af680: fmul    v29.4s, v1.4s, v11.s[0]\n",
      "    0.00 :   2af688: fmul    v18.4s, v2.4s, v8.s[0]\n",
      "    0.00 :   2af68c: fmul    v22.4s, v2.4s, v9.s[0]\n",
      "    0.00 :   2af694: fmul    v19.4s, v3.4s, v8.s[0]\n",
      "    0.00 :   2af698: fmul    v23.4s, v3.4s, v9.s[0]\n",
      "    0.00 :   2af6a0: fmul    v26.4s, v2.4s, v10.s[0]\n",
      "    0.00 :   2af6a4: fmul    v30.4s, v2.4s, v11.s[0]\n",
      "    0.00 :   2af6ac: fmul    v27.4s, v3.4s, v10.s[0]\n",
      "    0.00 :   2af6b0: fmul    v31.4s, v3.4s, v11.s[0]\n",
      "    0.00 :   2af6b8: fmla    v16.4s, v4.4s, v12.s[0]\n"
     ]
    }
   ],
   "source": [
    "!grep -m 20 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/fp_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see printed out above some assembly instructions that the ARM processor will run. the structure of each line written out by `perf annotate` has the following format\n",
    "\n",
    "***timestamp :    instruction_address: operation:   destination_registor, source_register1, source_register2***\n",
    "\n",
    "The printout should show operations making use of the vector registers. Examples of operations you might see include.   \n",
    "\n",
    "#### **Operation Definitions**\n",
    "- **`fmul`**: Performs a floating-point multiplication operation between two operands.\n",
    "- **`fmla`**: Performs a floating-point fused multiply-add operation, where the result of the multiplication is added to an accumulator in a single instruction.\n",
    "\n",
    "- **Vector Registers**: Operands starting with a `v` (e.g., `v16.4s`, `v0.4s`, `v8.s[0]`) indicate that vector registers are being used. These registers contain multiple data lanes, enabling the instruction to process multiple elements simultaneously, thereby increasing throughput.\n",
    "  - Example: `fmla v16.4s, v0.4s, v8.s[0]` performs the fused multiply-add operation on four single-precision floating-point elements in parallel (one for each lane of the vector register `v16`)..\n",
    "\n",
    "By examining the register types (vector vs. scalar), it becomes clear whether the instruction leverages SIMD for parallel processing or operates in a scalar manner and therefore whether torch is uses ARM's NEON SIMD processing capabilities to accelerate the matrix muliply operation. \n",
    "\n",
    "***Note*** Should no vector registers show up in your instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00 :   3740150:        dup     v2.16b, w0\n",
      "    0.00 :   3740160:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   3740194:        dup     v1.8b, w0\n",
      "    0.00 :   37401a8:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   3740518:        movi    v0.4s, #0x0\n",
      "    0.00 :   374053c:        mla     v0.16b, v2.16b, v1.16b\n",
      "    0.00 :   3740548:        addv    b0, v0.16b\n",
      "    0.00 :   3740554:        umov    w5, v0.b[0]\n",
      "    0.00 :   3740594:        mul     v0.8b, v0.8b, v1.8b\n",
      "    0.00 :   3740598:        addv    b0, v0.8b\n",
      "    0.00 :   374059c:        umov    w12, v0.b[0]\n",
      "    0.00 :   374082c:        dup     v2.16b, w0\n",
      "    0.00 :   3740840:        mla     v0.16b, v1.16b, v2.16b\n",
      "    0.00 :   3740878:        dup     v1.8b, w0\n",
      "    0.00 :   374088c:        mla     v0.8b, v2.8b, v1.8b\n",
      "    0.00 :   3740b34:        movi    v3.4s, #0x0\n",
      "    0.00 :   3740b44:        mov     v22.16b, v3.16b\n",
      "    0.00 :   3740b48:        mov     v21.16b, v3.16b\n",
      "    0.00 :   3740b50:        mov     v9.16b, v3.16b\n",
      "    0.00 :   3740b6c:        ld4     {v4.16b-v7.16b}, [x7], #64\n"
     ]
    }
   ],
   "source": [
    "!grep -m 20 -E '\\bv[0-9]+(\\.[a-z0-9]+)?\\b' results/int8_matmul_instructions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of instructions you might see in the output above are: \n",
    "- **`dup`**: Duplicates the value of a scalar register (e.g., `w0`) into all lanes of a SIMD vector register (e.g., `v2.16b`). This allows the same value to be broadcast across multiple lanes for parallel processing.\n",
    "- **`mla`**: Performs a fused multiply-accumulate operation. It multiplies corresponding elements from two SIMD vector registers and adds the results to the accumulator register. This operation is performed on all lanes in parallel.\n",
    "- **`addv`**: Adds all elements in a SIMD vector register and stores the resulting sum in a scalar register. This is typically used for reduction operations to aggregate data from multiple lanes.\n",
    "- **`umov`**: Extracts a specific lane from a SIMD vector register and moves it to a scalar register. This is useful for accessing individual elements after SIMD processing.\n",
    "\n",
    "These presence of these instructions shown above indicate that torch is utilizing ARMS SIMD capabilities. \n",
    "\n",
    "***Note*** Should no vector registers show up in the instruction list above, please look inside the file results/fp_matmul_instructions.txt to identify any vector register utilization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Benchmarking Pytorch Linear Layer**\n",
    "Great. Now you have verified that pytorch is utilizing the vector processing capability of ARM by defualt, we can benchmark it's operations and see the performance increase from floating point vectorization to integer vectorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a latency measuring function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "def benchmark(inputs, func, num_runs=10): \n",
    "    times = []\n",
    "    for _ in range(3): \n",
    "        func(inputs)\n",
    "\n",
    "    for _ in range(num_runs): \n",
    "        st = time.time()\n",
    "        func(inputs)\n",
    "        times.append(time.time() - st)\n",
    "\n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once again specify the size of matrices to evaluate**\n",
    "\n",
    "feel free to adjust the matrix sizes to identify how much computation large matrix multiply's require!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "sizes = [32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for a Full Precision Linear Layer**  \n",
    "The linear layer is a fundamental building block of generative AI models like transformers, performing dense matrix multiplications at its core. Given input $ X \\in \\mathbb{R}^{m \\times n} $ and weights $ W \\in \\mathbb{R}^{n \\times p} $, the output $ Y \\in \\mathbb{R}^{m \\times p} $ is computed as $ Y = XW + b $, where $ b $ is an optional bias term. Measuring the latency of these FP32 matrix multiplications provides a baseline for assessing performance and comparing with optimized or quantized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "float_times = []\n",
    "for size in sizes:\n",
    "    # floating point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    linear = nn.Linear(size, size, bias=False)\n",
    "    float_times.append(benchmark(x, linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Latencies for an INT8 Quantized Linear Layer**  \n",
    "In a quantized linear layer, the computation $ Y = XW + b $ is performed using INT8 precision for the operands $ X $ (input) and $ W $ (weights), while optionally adding a bias $ b $ in a higher precision (e.g., INT32 or FP32) to preserve accuracy. Quantization maps the original floating-point values to 8-bit integers using a scale $ S $ and zero-point $ Z $, such that $ x_\\text{quant} = \\text{round}(x / S) + Z $. This allows efficient matrix multiplications in reduced precision while maintaining a close approximation of the original computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.quantized as nnq\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "int8_times = []\n",
    "for size in sizes:\n",
    "    # floating point measurements \n",
    "    x = torch.randn(size, size, dtype=torch.float32, requires_grad=False)\n",
    "    x_quant = torch.quantize_per_tensor(x, scale=x.abs().max()/127, zero_point=0, dtype=torch.qint8)\n",
    "    qlinear = nnq.Linear(size, size, dtype=torch.qint8)\n",
    "    qlinear.set_weight_bias(x_quant, None)\n",
    "    int8_times.append(benchmark(x_quant, qlinear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the latency for each approach\n",
    "plt.plot(sizes, float_times, marker='o', linestyle='-', color='r', label='float precision')\n",
    "plt.plot(sizes, int8_times, marker='o', linestyle='-', color='b', label='int8 precision')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Matrix Size (N x N)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency Comparison: Naive vs NEON SIMD vs NEON I8MM Matrix Multiplication')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Lamma3.2-1B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to Download and Use Llama-3.2-1B from Hugging Face\n",
    "\n",
    "1. **Visit the Model Page**  \n",
    "   Navigate to the [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) page on Hugging Face.\n",
    "\n",
    "2. **Log In or Sign Up**  \n",
    "   If you don’t already have a Hugging Face account, create one. Otherwise, log in to your existing account.\n",
    "\n",
    "3. **Request Access**  \n",
    "   On the model's page, click the **\"Access repository\"** button. You’ll be prompted to review and agree to the terms of use.\n",
    "\n",
    "4. **Wait for Approval**  \n",
    "   After agreeing to the terms, access will be granted. This may take a few moments.\n",
    "\n",
    "5. **Proceed with the Script**  \n",
    "   Once you have access, you can use the provided script to download and use the model.\n",
    "\n",
    "To download the model, you may need to authenticate your Hugging Face account on your local machine. Run the following command in your terminal and follow the prompts to log in:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "After this authentication, you can download and use the model with the following script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets run a prompt through the model and see what it generates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM is a company that designs and manufactures microprocessors. ARM is the leader in the design of the CPU chips for mobile devices. ARM is also the leader in the design of the CPU chips for the servers and the data centers. ARM is also\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ARM is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets measure the models static memory consumption** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total static memory usage: 4714.2598876953125 MB\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(p.numel() * p.element_size() for p in model.buffers())\n",
    "print(f\"Total static memory usage: {(total_size + buffer_size) / (1024 ** 2)} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a significant amount of memory consumption—nearly 5GB! Many small embedded devices may struggle to handle this. Therefore, it's crucial to explore strategies for reducing the model's memory footprint, alongside improving inference speed. Doing so will expand the range of devices capable of running the model and, since memory access is energy-intensive, it will also reduce energy consumption and extend battery life. Next however, lets look at the latency bottlenecks and dynamic memory usage during inference with `torch.profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1127 15:48:15.828778000 CPUAllocator.cpp:249] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      model_inference         7.44%      23.723ms       100.00%     318.862ms     318.862ms      33.66 Mb    -273.66 Mb             1  \n",
      "                                         aten::linear         0.10%     311.614us        83.37%     265.834ms       2.353ms     117.53 Mb           0 b           113  \n",
      "                                         aten::matmul         0.46%       1.469ms        83.04%     264.799ms       2.323ms     117.54 Mb           0 b           114  \n",
      "                                             aten::mm        82.21%     262.140ms        82.22%     262.166ms       2.320ms     117.53 Mb     117.53 Mb           113  \n",
      "                                            aten::mul         2.39%       7.614ms         2.39%       7.623ms      51.505us      81.05 Mb      81.05 Mb           148  \n",
      "                   aten::scaled_dot_product_attention         0.03%     101.664us         1.05%       3.348ms     209.270us       7.63 Mb    -114.38 Kb            16  \n",
      "                                            aten::add         1.01%       3.220ms         1.05%       3.339ms      34.421us      24.79 Mb      24.79 Mb            97  \n",
      "                                           aten::silu         1.03%       3.286ms         1.03%       3.286ms     205.348us      30.50 Mb      30.50 Mb            16  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         0.97%       3.083ms         1.02%       3.247ms     202.916us       7.74 Mb      -1.98 Mb            16  \n",
      "                                           aten::mean         0.11%     362.422us         0.89%       2.843ms      86.156us       7.86 Kb       7.85 Kb            33  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 318.862ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "prompt = \"ARM is a company that designs and develops microprocessors and other microchips. It is also known for its ARM Cortex processor and the ARM Mali GPU. ARM has been a leader in the mobile computing industry since the early 2000s, and is now a major player in the AI and \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Profile the model\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "# Print a summary\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your printed profiler summary should look similar to this: \n",
    "\n",
    "| **Name**                                          | Self CPU % | Self CPU   | CPU total % | CPU total   | CPU time avg  | CPU Mem      | Self CPU Mem | # of Calls |\n",
    "|-----------------------------------------------|------------|------------|-------------|-------------|---------------|--------------|--------------|------------|\n",
    "| model_inference                               | 10.80%     | 39.397ms   | 100.00%     | 364.821ms   | 364.821ms     | 33.66 Mb     | -273.65 Mb   | 1          |\n",
    "| aten::linear                                  | 0.12%      | 425.168us  | 80.62%      | 294.129ms   | 2.603ms       | 117.53 Mb    | 0 b          | 113        |\n",
    "| aten::matmul                                  | 0.58%      | 2.127ms    | 80.24%      | 292.748ms   | 2.568ms       | 117.54 Mb    | 0 b          | 114        |\n",
    "| aten::mm                                      | 79.37%     | 289.571ms  | 79.39%      | 289.637ms   | 2.563ms       | 117.53 Mb    | 117.53 Mb    | 113        |\n",
    "| aten::mul                                     | 2.15%      | 7.859ms    | 2.20%       | 8.014ms     | 54.150us      | 81.05 Mb     | 81.05 Mb     | 148        |\n",
    "| aten::silu                                    | 1.63%      | 5.962ms    | 1.63%       | 5.962ms     | 372.654us     | 30.50 Mb     | 30.50 Mb     | 16         |\n",
    "| aten::add                                     | 0.89%      | 3.238ms    | 0.92%       | 3.353ms     | 34.562us      | 24.79 Mb     | 24.79 Mb     | 97         |\n",
    "| aten::scaled_dot_product_attention            | 0.03%      | 105.665us  | 0.84%       | 3.059ms     | 191.211us     | 7.62 Mb      | -122.00 Kb   | 16         |\n",
    "| aten::_scaled_dot_product_flash_attention_for_cpu | 0.76%   | 2.787ms    | 0.81%       | 2.954ms     | 184.607us     | 7.74 Mb      | -1.86 Mb     | 16         |\n",
    "| aten::mean                                    | 0.09%      | 324.214us  | 0.75%       | 2.740ms     | 83.028us      | 7.86 Kb      | 7.84 Kb      | 33         |\n",
    "\n",
    "**Self CPU time total:** 364.821ms\n",
    "\n",
    "### Key Takeaways from Profiling\n",
    "\n",
    "In this profile, the matrix multiplication (`aten::mm`) is the dominant computational operation, accounting for 79% of total CPU computation time. Similarly, `aten::linear` calls contribute 81%, reinforcing that dense layers, such as those in transformers' feedforward layers, are the main computational bottleneck. In contrast, scaled dot product attention (`_scaled_dot_product_flash_attention_for_cpu`) accounts for less than 1%, and element-wise operations like `aten::mul` and `aten::add` consume only ~4%, reflecting their relatively minor impact on overall performance.\n",
    "\n",
    "Memory profiling reveals that matrix multiplication (`aten::mm`) is also the most dynamic memory-intensive operation, allocating 117.53 MB. Dense layers rely heavily on this, which underscores the importance of optimizing these computations. Element-wise operations like `aten::mul` and `aten::add` consume 81.05 MB and 24.79 MB, respectively, while attention mechanisms are significantly more efficient, using less than 8 MB each and demonstrating minimal overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization Focus**\n",
    "\n",
    "**Matrix multiplications in dense layers are the most critical operations to optimize.** These dominate both computation and memory usage, making them the primary target for performance improvements. Techniques such as vecotrization, quantization, covered in earlier sections—can significantly enhance both efficiency and memory usage. Lets start by lowering the precision of the feedforward linear layers that rely heavily in matrix multiplications to the int8 datatype. This is a technique known as quantization. Specifically we will start with a method called **Integer Symmetric Weight-Only Quantization**. \n",
    "\n",
    "\n",
    "## Integer Per-TensorSymmetric Weight-Only Quantization\n",
    "\n",
    "Integer symmetric weight-only quantization reduces a neural network's weight precision from floating-point to integer values while keeping activations in floating-point format during inference. While the underlying matrix multiplication must still occur in floating point e.g. `torch.mm(W.dequantize(), x)`, the weight-only quantization can decrease the static memory consumption of the model by up to 4 times. Additionally in the case where the inference speed is limited by the memory bandwidth, reducing the weight precision can also reduce the memory read latencies, providing inference speedups. \n",
    "\n",
    "Given that the equation of the linear layer is\n",
    "\n",
    "$y = WX + b$\n",
    "\n",
    "To apply this technique, floating-point weights $ W $ are quantized to integer values $ Q_w $ using a scale factor $ S_w $. The scale factor can be computed based on the maximum absolute weight value:\n",
    "\n",
    "$\n",
    "S_w = \\frac{\\max(|W|)}{2^{b-1} - 1}\n",
    "$\n",
    "\n",
    "where $ b $ is the bit-width of the integer representation (e.g., $ b = 8 $ for 8-bit integers).\n",
    "\n",
    "The quantization and dequantization processes are defined by:\n",
    "\n",
    "- **Quantization**:\n",
    "  $\n",
    "  Q_w = \\text{round}\\left( \\frac{W}{S_w} \\right)\n",
    "  $\n",
    "\n",
    "- **Dequantization**:\n",
    "  $\n",
    "  \\hat{W} = Q_w \\cdot S_w\n",
    "  $\n",
    "\n",
    "During inference, the quantized weights $ Q_w $ are used with floating-point activations $ x $ to compute the output $ y $:\n",
    "\n",
    "$\n",
    "y = x \\cdot \\hat{W} = x \\cdot (Q_w \\cdot S_w)\n",
    "$\n",
    "\n",
    "**Lets collect a weight matrix and activation tensor from one of the feed forward layers in the Lamma3.2-1B model.** (we can do this using a functionality of pytorch called forward hooks)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected weight matrix has shape 8192x2048 this is the number of (input_channels x output_channels)\n",
      "The collected activation tensor has shape 1x61x8192 this is the number of (batch_size x sequence_length x token_dim)\n"
     ]
    }
   ],
   "source": [
    "# Lets grab a weight tensor and activation one of the feed forward layers in the Lamma3.2-1B model. \n",
    "# pick whatever transformer layer you like \n",
    "\n",
    "layer_idx = 7 # alter this to select the transformer block of your choosing. (the model has 16 transformer blocks)\n",
    "\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output\n",
    "    return hook\n",
    "\n",
    "# register a forward hook to collect the activations of the gate_proj layer in the transformer block. \n",
    "layer = model.model.layers[layer_idx].mlp.gate_proj\n",
    "hook_handle = layer.register_forward_hook(get_activation('gate_proj')) \n",
    "\n",
    "# run the model forward pass \n",
    "with torch.no_grad(): \n",
    "    model(**inputs) \n",
    "\n",
    "# extract the weight matrix and activation tensor from the hook. \n",
    "W = layer.weight.data.clone() \n",
    "X = activations['gate_proj']\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"The collected weight matrix has shape {W.shape[0]}x{W.shape[1]} this is the number of (input_channels x output_channels)\")\n",
    "print(f\"The collected activation tensor has shape {X.shape[0]}x{X.shape[1]}x{X.shape[2]} this is the number of (batch_size x sequence_length x token_dim)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!, now we have collected a weight matrix and activation tensor from the inside of the Lamma3.2-1B feed forward layers. We can now use these to test the accuracy of the quantization technique. Lets start by implementing the per-tensor quantization technque described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization to compare against our quantized version. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization (int8 quantization)\n",
    "S = W.abs().max() / (2**(b-1) - 1)  # Scale Factor Quantization\n",
    "Q = torch.round(W / S).clamp(-2**(b-1), 2**(b-1) - 1)  # Quantizing the weights (clamp is used to prevent overflow)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat = torch.matmul(X, (Q * S)) # Notice the Dequantzation step here (Q * S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quantization error to see if we have correctly approximated the full precision linear layer. This can be defined as the absolute difference between the full precision linear layer and our quantized version. Using our notation, where $y$ is the full precision output, $\\hat{y}$ is the quantized output, $Q$ is the quantized tensor, and $S$ is the scale factor, the quantization error can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Quantization Error} = \\left| y - \\hat{y} \\right| = \\left| y - W \\cdot (Q \\cdot S) \\right|\n",
    "$$\n",
    "\n",
    "Additionally, the relative quantization error as a percentage can be calculated to understand the error in relation to the magnitude of the full precision output:\n",
    "\n",
    "$$\n",
    "\\text{Relative Quantization Error (\\%)} = \\left( \\frac{\\left| y - \\hat{y} \\right|}{\\left| y \\right|} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "This relative error provides a normalized measure of the quantization error, making it easier to compare across different scales of output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 64646.39 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 5.89 (%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantization error. This is the difference between the full precision linear layer and our quantized version. \n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel.mean():.2f} (%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the mean and min relative quantization error are around 6% and 0% (for layer 8 of the Lamma3.2-1B model). The Max relative quantization error may however be huge. This is caused by outliers in the weight matrix distribution that skew the quantization error. There are anumber of techniques to mitigate this issue. We will not cover them here but you can learn more about them in the [LLM.int8() paper](https://arxiv.org/abs/2208.07339). \n",
    "\n",
    "This is a good start but we can quantize the weights, to give roughly a 5% relative quantization error. We however can do better. To do so lets try and understand what the distribution of the data in the weight matrix looks like. Below we will plot a boxplot of the 4 largest and 4 smallest channels in your chosen layer (set by the layer_idx variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the L2 norm of each channel along axis 1\n",
    "channel_magnitudes = np.linalg.norm(W.clone().numpy(), axis=1)\n",
    "\n",
    "num_channels_to_select = 5\n",
    "largest_channels_indices = np.argsort(channel_magnitudes)[-num_channels_to_select:]\n",
    "smallest_channels_indices = np.argsort(channel_magnitudes)[:num_channels_to_select]\n",
    "\n",
    "# Select the largest and smallest channels along axis 1\n",
    "largest_channels = W.numpy()[largest_channels_indices, :]\n",
    "smallest_channels = W.numpy()[smallest_channels_indices, :]\n",
    "\n",
    "# Combine the data for boxplot\n",
    "combined_data = np.concatenate((largest_channels, smallest_channels), axis=0)\n",
    "labels = ['Largest'] * num_channels_to_select + ['Smallest'] * num_channels_to_select\n",
    "\n",
    "combined_data = combined_data.T\n",
    "print(combined_data.shape)\n",
    "# Plot boxplot for the selected channels\n",
    "def plot_combined_boxplot(combined_data, labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=combined_data, palette=\"Set2\")\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels * (combined_data.shape[1] // len(labels)))\n",
    "    plt.title(f'BoxPlot Distributions of Largest and Smallest Channels in Lamma3.2-1B FeedForward Layer {layer_idx}')\n",
    "    plt.xlabel('Channel Type')\n",
    "    plt.ylabel('Weight Value')\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_boxplot(combined_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Per-Tensor Symmetric Weight-Only Quantization\n",
    "\n",
    "Looking at this plot, what you can see is that the larger channels have very wide distributions and large outliers. Additionally, the small channels have very tight distributions centered around 0. If we used per-tensor quantization with just a single scale value $S_w$ for the entire tensor, all the values in the small channels would be mapped to zero, causing large quantization errors.\n",
    "\n",
    "To overcome this, we can use a technique called **Per-Channel Weight-Only Quantization**. Instead of a single scale factor for the entire tensor, we compute a separate scale factor for each channel of the weight matrix. This allows us to more accurately quantize the weights in each channel, reducing the quantization error and preserving the accuracy of the model.\n",
    "\n",
    "The process for doing this is as follows:\n",
    "\n",
    "#### 1. Channel-Wise Scale Factor\n",
    "For each channel $i$ of $\\mathbf{W}$ (i.e., row $\\mathbf{W}_i$), compute a channel-specific scale factor:\n",
    "\n",
    "$\n",
    "S_{w_i} = \\frac{\\max(|\\mathbf{W}_i|)}{2^{b-1} - 1}.\n",
    "$\n",
    "\n",
    "#### 2. Channel-Wise Quantization and Dequantization\n",
    "1. **Quantization**: Convert the weights of each channel to integer values:\n",
    "   $\n",
    "   Q_{w_i} = \\text{round}\\left(\\frac{\\mathbf{W}_i}{S_{w_i}}\\right), \\quad Q_{w_i} \\in \\{-2^{b-1}, \\ldots, 2^{b-1} - 1\\}.\n",
    "   $\n",
    "\n",
    "2. **Dequantization**: Recover the approximate floating-point weights of each channel:\n",
    "   $\n",
    "   \\hat{\\mathbf{W}}_i = Q_{w_i} \\cdot S_{w_i}.\n",
    "   $\n",
    "\n",
    "The final quantized weight matrix $\\hat{\\mathbf{W}}$ is reconstructed by concatenating the channel-wise dequantized weights.\n",
    "\n",
    "#### 3. Inference\n",
    "During inference, the computation of $\\mathbf{y}$ remains similar but uses the per-channel dequantized weights:\n",
    "\n",
    "$\n",
    "\\mathbf{y} = \\mathbf{x} \\cdot \\hat{\\mathbf{W}} = \\mathbf{x} \\cdot \\left(\\sum_{i=1}^m Q_{w_i} \\cdot S_{w_i}\\right).\n",
    "$\n",
    "\n",
    "This approach ensures that both small and large channels are appropriately quantized, reducing overall quantization error. Lets implement this below. Your task is to complete the missing line in the code cell below that computs the per-channel scale factor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S_per_channel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# Bit-width for quantization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ------ Complete the missing line here ------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# you need to compute a different scale factor for each channel in the weight matrix. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# S = ... \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(W \u001b[38;5;241m/\u001b[39m \u001b[43mS_per_channel\u001b[49m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(b\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Q <- quantize(W)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Weight-only quantized linear layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m y_hat_per_channel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(X, (Q \u001b[38;5;241m*\u001b[39m S_per_channel)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'S_per_channel' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# you need to compute a different scale factor for each channel in the weight matrix. \n",
    "# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "# S_per_channel = ... \n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X, (Q * S_per_channel)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can run this test below to check your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative per channel quantization error: 6028.29 (%), per-tensor quantization error: 64646.39 (%)\n",
      "Min relative per channel quantization error: 0.00 (%), per-tensor rel quantization error: 0.00 (%)\n",
      "Mean relative per channel quantization error: 1.13 (%), per-tensor mean rel quantization error: 5.89 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: 1.13% and Per-Tensor quantization error 5.89%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Calculate quantization error\n",
    "residuals_per_channel = (y - y_hat_per_channel).abs()\n",
    "residuals_rel_per_channel = (residuals_per_channel / y.abs()) * 100\n",
    "print(f\"Max relative per channel quantization error: {residuals_rel_per_channel.max():.2f} (%), per-tensor quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative per channel quantization error: {residuals_rel_per_channel.min():.2f} (%), per-tensor rel quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative per channel quantization error: {residuals_rel_per_channel.mean():.2f} (%), per-tensor mean rel quantization error: {residuals_rel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel_per_channel.mean() < residuals_rel.mean(): \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Per-Channel quantization error: {residuals_rel_per_channel.mean():.2f}% and Per-Tensor quantization error {residuals_rel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel_per_channel.mean():.2f}%. Please try again.</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute the output of the linear layer before quantization. \n",
    "y = torch.matmul(X, W) \n",
    "\n",
    "# Quantize the layer weight 'W'\n",
    "b = 8  # Bit-width for quantization\n",
    "\n",
    "# ------ Complete the missing line here ------\n",
    "# you need to compute a different scale factor for each channel in the weight matrix. \n",
    "# this will result in a scale 'S' vector with the same number of output channels as the weight matrix. \n",
    "S_per_channel = W.abs().max(dim=1, keepdim=True)[0] / (2**(b-1) - 1)  # Scale factor\n",
    "# ---------------------------------------------\n",
    "Q = torch.round(W / S_per_channel).clamp(-2**(b-1), 2**(b-1) - 1)  # Q <- quantize(W)\n",
    "\n",
    "# Weight-only quantized linear layer\n",
    "y_hat_per_channel = torch.matmul(X, (Q * S_per_channel)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! you should see that the per-channel weight only quantizaton technqiue has a significantly lower quantization error than per-tensor quantization. Lets now use this approach to quantize Lamma3.2-1B! Specifically we will apply the to the feedfoward layers of Lamma3.2-1B transformer blocks as they quantize the best. Lets build a class to achieve this. **Below is a simple implementation of the Per-ChannelWeight-Only Integer Quantization technique that can be applied to the pytorch linear layer. It however is not completed, you will therefore need to fill in the missing line in the `quantize` member function, aswell as the linear multiplication in the `forward` function**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(out_features), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to compute a variable called `scale` that is used to quantize the `tensor` Look in the above exmple if you are unsure how to do this. \n",
    "        # scale = ...\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # you can use the `torch.nn.functional.linear` function to perform the linear layer operation. (Remember to dequantize the weights first)\n",
    "        # y = ...\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can test your implementation by running the following cell. The output relative error should be similar to the above example.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max relative quantization error: 8579.00 (%)\n",
      "Min relative quantization error: 0.00 (%)\n",
      "Mean relative quantization error: 1.70 (%)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: 1.70%</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# setup the full precision linear layer\n",
    "linear = nn.Linear(W.shape[0], W.shape[1], bias=False)\n",
    "linear.weight.data = W.t()\n",
    "\n",
    "# setup the quantized linear layer\n",
    "qlinear = WeightOnlyInt8Linear(W.shape[0], W.shape[1], bias=False)\n",
    "qlinear.quantize(W.t()) \n",
    "\n",
    "# run the forward pass through the linear layer and compare the output to the full precision linear layer. \n",
    "y = linear(X)\n",
    "y_hat = qlinear(X)\n",
    "\n",
    "# Calculate relative quantization error\n",
    "residuals = (y - y_hat).abs()\n",
    "residuals_rel = (residuals / y.abs()) * 100\n",
    "print(f\"Max relative quantization error: {residuals_rel.max():.2f} (%)\")\n",
    "print(f\"Min relative quantization error: {residuals_rel.min():.2f} (%)\")\n",
    "print(f\"Mean relative quantization error: {residuals_rel.mean():.2f} (%)\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if residuals_rel.mean() < 8: \n",
    "    display(HTML(f\"<div style='color: green; font-size: 20px;'>Success! Mean relative quantization error: {residuals_rel.mean():.2f}%</div>\"))\n",
    "else: \n",
    "    display(HTML(f\"<div style='color: red; font-size: 20px;'>Failed. Mean relative quantization error: {residuals_rel.mean():.2f}%. Please try again.</div>\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SOLUTION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightOnlyInt8Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features, bias=True): \n",
    "        super().__init__()\n",
    "        self.in_features = in_features # number of input channels \n",
    "        self.out_features = out_features # number of output channels \n",
    "        self.scale = nn.Parameter(torch.ones(1, dtype=torch.float32), requires_grad=False)  # scale factor used for quantizing weights\n",
    "        # Create a tensor with the desired dtype first, then wrap it with nn.Parameter\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.int8), requires_grad=False)  # quantized weights\n",
    "        if bias: \n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features)) # bias term\n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        b = 8\n",
    "        qmin = -2**(b-1)\n",
    "        qmax = 2**(b-1) - 1\n",
    "        \n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to compute a variable called `scale` that is used to quantize the `tensor` Look in the above exmple if you are unsure how to do this. \n",
    "        scale = torch.Tensor((tensor.abs().max(dim=1, keepdims=True)[0] / qmax))\n",
    "        # ---------------------------------------------\n",
    "        scale = scale.clamp(min=1e-8)\n",
    "        quantized_weights = torch.round(tensor / scale).clamp(qmin, qmax)\n",
    "        quantized_weights = quantized_weights.type(torch.int8)\n",
    "        self.weight.data = quantized_weights \n",
    "        self.scale.data = scale\n",
    "        assert self.weight.dtype == torch.int8\n",
    "        return None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ------ Complete the missing line here ------\n",
    "        # you will need to dequantize the weights `self.weight` using the scale factor `self.scale` and compute the output of the linear layer y. \n",
    "        # you can use the `torch.nn.functional.linear` function to perform the linear layer operation. \n",
    "        y = F.linear(x, self.weight * self.scale, self.bias)\n",
    "        # ---------------------------------------------\n",
    "        return y\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f\"WeightOnlyInt8Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets now use this to quantize the feedforward layers of Lamma3.2-1B.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): WeightOnlyInt8Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): WeightOnlyInt8Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): WeightOnlyInt8Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model.eval()\n",
    "\n",
    "def quantize_model(model): \n",
    "    for block in model.model.layers: \n",
    "        for name, module in block.mlp.named_modules(): \n",
    "            if isinstance(module, nn.Linear): \n",
    "                qlinear = WeightOnlyInt8Linear(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "                if module.bias is not None: \n",
    "                    qlinear.bias.data = module.bias.data \n",
    "                qlinear.quantize(module.weight.data.clone())\n",
    "                # Use setattr on the parent module to replace the linear layer\n",
    "                parent_module = block.mlp\n",
    "                setattr(parent_module, name, qlinear)\n",
    "    return model\n",
    "\n",
    "model = quantize_model(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets run a prompt through the model and make sure we haven't broken anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM is a company that designs and manufactures microprocessors and other microelectronic products. ARM chips are used in many devices such as smartphones, tablets, smartwatches, and more. ARM chips are widely used because they are fast, efficient,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ARM is a company that designs\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have confirmed that. Lets check the static memory consumption, to see what benefits weight only quantization has brought us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total static memory usage: 2410.2600708007812 MB\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(p.numel() * p.element_size() for p in model.model.parameters())\n",
    "buffer_size = sum(p.numel() * p.element_size() for p in model.model.buffers())\n",
    "print(f\"Total static memory usage: {(total_size + buffer_size) / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So we have reduced the static memory consumption from 4.7Gb to 2.3Gb. This is a significant reduction, and will make the model more portable to smaller devices. We can however do better. Lets explore graph optimization techniques to improve the inference speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of `torch.compile` for ARM CPUs in PyTorch\n",
    "\n",
    "`torch.compile` is a feature in PyTorch that optimizes model execution by compiling models into more efficient forms, specifically tailored for ARM CPUs. This process reduces the overhead of PyTorch's dynamic execution, leading to faster execution times and lower resource usage on ARM architectures. By analyzing the model's computation graph, `torch.compile` applies optimizations such as operator fusion and loop unrolling, which are particularly beneficial for ARM's architecture.\n",
    "\n",
    "ARM CPUs are known for their energy efficiency and performance, making them ideal for deploying AI models in diverse environments. `torch.compile` leverages ARM's specific capabilities, including its support for SIMD (Single Instruction, Multiple Data) instructions like NEON, to enhance model performance. This allows for parallel processing of multiple data points, significantly accelerating computationally intensive tasks.\n",
    "\n",
    "The feature is user-friendly, requiring minimal changes to existing code. Users can compile their models with `torch.compile` and immediately benefit from performance improvements on ARM CPUs. This makes it particularly useful for complex models or those running on ARM-based devices, offering significant speedups and efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency for Eager Execution: 519.14 ms\n"
     ]
    }
   ],
   "source": [
    "def benchmark_latency(inputs, model, num_runs=10):\n",
    "    import time\n",
    "    import torch\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Warm-up runs (to stabilize any initial overhead)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            model(**inputs)\n",
    "\n",
    "    # Measure latency\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            model(**inputs)\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "\n",
    "    # Calculate average latency\n",
    "    avg_latency = sum(times) / len(times)\n",
    "    return avg_latency\n",
    "\n",
    "\n",
    "lat = benchmark_latency(inputs, model)\n",
    "print(f\"Average latency for Eager Execution: {lat:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "compiled_model = torch.compile(model)\n",
    "lat = benchmark_latency(inputs, compiled_model)\n",
    "print(f\"Average latency for Torch Compile: {lat:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model for embedded inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([1, 7])\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.shape)\n",
    "print(inputs.attention_mask.shape)\n",
    "print(inputs.input_ids.dtype)\n",
    "print(inputs.attention_mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:97: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of BaseModelOutputWithPast(last_hidden_state=tensor([[[ 1.5326, -1.3593,  2.4385,  ..., -1.0574,  0.4010, -1.1256],\n         [-1.1032,  3.1969,  3.7620,  ..., -5.2792, -4.0093,  2.8944],\n         [ 1.5108,  1.3452, -0.2563,  ..., -5.3894, -6.9593, -0.1786],\n         ...,\n         [ 1.4442,  1.9402,  1.1227,  ..., -3.1643, -6.0105, -2.4108],\n         [ 3.6687,  4.9128,  1.4363,  ..., -3.1012, -4.0978, -0.2268],\n         [-0.4561,  4.2190,  2.2034,  ..., -2.9271, -3.0164, -0.9648]]],\n       grad_fn=<MulBackward0>), past_key_values=((tensor([[[[ 9.5610e-02,  1.7827e-01,  3.6755e-02,  ..., -1.7652e+00,\n            1.7963e+00,  7.9892e-03],\n          [-1.0441e+00, -1.9604e+00, -1.3867e+00,  ...,  6.3562e-01,\n           -2.3245e+00, -2.2302e+00],\n          [-4.0632e+00, -8.6298e-01, -3.0124e+00,  ...,  2.7366e+00,\n           -1.4887e+00, -2.8741e-01],\n          ...,\n          [-1.1385e+00, -3.3855e+00, -2.5555e+00,  ...,  1.7879e+00,\n           -1.7340e+00, -2.0997e+00],\n          [ 3.8586e+00, -3.0442e+00, -1.6802e+00,  ...,  2.6446e+00,\n           -1.3799e+00, -2.4386e+00],\n          [ 6.2070e+00, -9.6388e-02, -1.1381e+00,  ...,  1.5129e+00,\n           -1.4761e+00, -1.9522e+00]],\n\n         [[ 1.3574e-03, -6.1648e-03,  1.7492e-02,  ...,  1.3465e+00,\n           -3.6406e-01,  2.4415e-01],\n          [ 4.1891e+00, -2.6343e+00,  2.6864e+00,  ...,  1.2204e+00,\n            2.0671e+00, -2.0118e+00],\n          [ 4.8540e-01, -4.4151e-01,  4.9244e-01,  ..., -2.1734e-01,\n            1.3597e+00, -7.4817e-01],\n          ...,\n          [-2.6706e+00,  6.8963e-01,  2.3554e+00,  ...,  3.0610e-01,\n            1.7619e+00, -1.2003e+00],\n          [-2.0480e+00,  3.3310e-01,  1.2774e+00,  ..., -3.3540e-01,\n            2.1050e+00, -2.6796e+00],\n          [-1.8765e+00,  2.3173e+00,  6.0927e-01,  ..., -4.6700e-01,\n            1.8176e+00, -2.1536e+00]],\n\n         [[ 9.4520e-03, -1.5944e-02, -2.4548e-02,  ...,  1.3063e+00,\n            2.7019e+00, -1.7912e+00],\n          [ 8.7218e-01, -1.0578e+00, -1.5827e+00,  ...,  3.3310e-02,\n           -3.7639e-01,  4.6344e-01],\n          [-1.4165e-01, -3.0495e-01,  3.4495e-02,  ..., -3.5958e-01,\n           -1.0394e+00,  6.4655e-01],\n          ...,\n          [-5.6807e-01,  8.5252e-01, -7.0232e-01,  ..., -1.0918e-01,\n           -2.6005e+00,  7.6607e-01],\n          [-8.6511e-02,  3.2448e-01, -1.0111e+00,  ..., -1.8479e+00,\n            6.0253e-01,  1.4022e+00],\n          [ 1.2307e+00, -1.5465e-02,  7.3055e-01,  ..., -4.3984e-01,\n           -1.7431e+00, -2.3357e+00]],\n\n         ...,\n\n         [[ 1.8422e-01,  1.7773e-01,  2.1953e-02,  ...,  1.0377e+00,\n           -1.6470e+00,  1.0830e+00],\n          [-2.4002e+00,  1.6295e+00, -1.3280e+00,  ...,  4.8186e-02,\n            1.6535e+00, -4.3970e-01],\n          [ 3.0804e-01, -3.1984e-01, -3.4331e-01,  ...,  1.0190e+00,\n           -1.2367e+00, -1.4422e+00],\n          ...,\n          [ 5.7487e-01, -2.2988e-01, -5.5945e-02,  ..., -1.1135e+00,\n            4.8710e-01, -1.2589e+00],\n          [-9.0519e-01,  5.3804e-01,  8.3072e-01,  ..., -1.3570e+00,\n           -1.9944e-02, -1.3020e+00],\n          [-7.4401e-02, -2.7901e-01,  8.7861e-02,  ..., -1.0235e+00,\n            2.4862e-01, -9.2794e-01]],\n\n         [[-2.6273e-03,  2.6221e-03, -3.0507e-02,  ...,  1.1223e+00,\n            1.1003e+00,  1.8951e-01],\n          [ 4.7903e+00,  3.9686e+00,  1.0754e-01,  ...,  2.1310e-01,\n           -2.8108e-02, -8.7820e-01],\n          [ 1.0246e+00,  1.3096e+00,  1.4995e+00,  ..., -2.0850e+00,\n           -1.6414e-02, -1.2134e-01],\n          ...,\n          [-6.4768e+00, -1.2898e+00, -1.7894e+00,  ..., -3.3122e+00,\n           -6.5641e-01, -2.1036e+00],\n          [-8.0486e-01, -8.0386e-01, -6.5649e-01,  ..., -1.9635e+00,\n            3.5160e-02, -2.1034e-01],\n          [-3.8587e-01, -4.6915e+00, -1.9725e+00,  ..., -4.0251e+00,\n           -1.0439e+00, -2.0460e+00]],\n\n         [[-1.6762e-02,  9.5977e-02, -5.1725e-02,  ...,  1.3930e+00,\n            7.2637e-01, -1.7956e+00],\n          [-9.7841e-01,  3.2913e+00, -1.2614e+00,  ..., -1.5560e+00,\n           -1.1715e+00,  1.6520e+00],\n          [ 1.6378e-01, -3.6365e-01, -1.2429e+00,  ...,  3.4448e-01,\n            1.0371e+00, -4.1897e-01],\n          ...,\n          [-1.8854e-01, -5.7922e-01,  4.0452e-01,  ..., -7.6429e-01,\n           -6.2004e-01,  8.1454e-01],\n          [ 3.4380e-01, -1.2955e+00, -1.9310e+00,  ...,  2.8621e-02,\n            1.0520e+00,  5.6551e-01],\n          [ 1.8523e+00, -2.8961e+00,  1.2598e+00,  ..., -4.3885e-01,\n           -8.9879e-01,  7.6234e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.5795e-03,  1.3599e-02, -4.5031e-02,  ...,  3.4113e-04,\n           -2.8627e-03,  1.1434e-01],\n          [-9.9041e-02, -6.0634e-02, -2.1137e-02,  ...,  3.0084e-01,\n            2.7410e-02, -2.8304e-01],\n          [ 5.0372e-02, -7.0389e-03, -5.9483e-03,  ...,  1.2424e-02,\n            9.2066e-02, -1.9506e-02],\n          ...,\n          [ 9.5086e-02, -2.9229e-04,  5.0430e-03,  ...,  3.8130e-02,\n           -3.3565e-02, -8.2930e-03],\n          [ 2.4362e-02, -2.1318e-02, -3.0878e-02,  ..., -6.8150e-02,\n            8.7928e-02,  7.7262e-02],\n          [ 3.4411e-02, -1.3991e-03, -2.1157e-02,  ..., -3.8938e-02,\n            6.3289e-02, -5.3524e-03]],\n\n         [[ 4.5735e-04,  2.5294e-04, -4.7072e-05,  ..., -4.1792e-04,\n            5.8174e-05, -1.2571e-03],\n          [-6.8077e-02,  4.3200e-02, -1.6258e-01,  ...,  1.6611e-01,\n            1.8847e-01,  1.0549e-01],\n          [-1.6937e-03, -3.3735e-03, -7.3070e-03,  ...,  7.0302e-03,\n           -5.8456e-03,  4.7597e-03],\n          ...,\n          [ 2.0076e-01, -6.8954e-02,  1.4551e-02,  ..., -4.2534e-02,\n            1.5192e-01,  3.5590e-01],\n          [-6.8439e-02, -1.4327e-01,  2.9145e-03,  ...,  2.8026e-02,\n            8.6677e-02, -1.3608e-01],\n          [-1.7211e-01,  4.3724e-02, -4.5069e-02,  ..., -1.0670e-02,\n            4.1900e-02, -3.6702e-02]],\n\n         [[-4.7917e-03, -3.6565e-03,  3.6446e-04,  ..., -3.5328e-04,\n           -6.5906e-04, -9.0832e-04],\n          [-9.4917e-02, -2.8329e-02,  4.3898e-02,  ...,  3.1837e-02,\n           -3.1751e-02, -6.8372e-02],\n          [-1.9278e-02, -3.1923e-02, -7.9513e-03,  ..., -4.0204e-02,\n            1.9414e-03, -1.5249e-02],\n          ...,\n          [ 2.0179e-01, -1.4612e-02,  3.0915e-02,  ...,  7.6138e-03,\n           -8.9133e-02,  4.7802e-02],\n          [-5.1400e-02,  5.7345e-02,  7.4082e-02,  ..., -2.9667e-02,\n           -1.5746e-02,  4.2932e-02],\n          [ 1.4780e-01,  4.6155e-03,  4.9988e-02,  ...,  6.1138e-02,\n            9.1901e-03,  1.6400e-02]],\n\n         ...,\n\n         [[-3.7790e-03,  6.0383e-04, -3.2364e-03,  ..., -5.6062e-04,\n           -1.4380e-04,  4.3967e-04],\n          [ 1.7510e-02, -3.3539e-02, -3.3570e-02,  ..., -1.7791e-02,\n           -4.1592e-03,  1.0626e-02],\n          [-1.4188e-01, -2.0370e-02, -3.2847e-02,  ...,  1.2244e-02,\n            6.5973e-03,  2.4519e-02],\n          ...,\n          [ 2.2899e-02, -7.2733e-02,  5.6826e-02,  ...,  2.8298e-02,\n            2.9018e-02, -1.1067e-03],\n          [ 3.3121e-03,  2.2296e-02, -5.5816e-02,  ...,  2.6916e-02,\n           -3.7854e-02, -2.1149e-02],\n          [ 6.6373e-02, -1.4837e-02, -1.9332e-03,  ..., -4.1016e-02,\n           -2.2104e-02,  1.5020e-02]],\n\n         [[-5.6312e-04, -1.6511e-04,  4.0350e-03,  ...,  8.4011e-05,\n           -1.4644e-03, -8.6666e-04],\n          [ 1.0925e-01,  5.0212e-02,  1.4440e-01,  ...,  6.7587e-03,\n           -1.4874e-02, -1.4358e-01],\n          [-7.5719e-03,  5.1073e-03,  1.8936e-02,  ...,  1.7475e-03,\n           -3.7511e-03,  1.0725e-02],\n          ...,\n          [-6.6414e-02,  5.5474e-02, -1.0623e-01,  ..., -7.9432e-02,\n            4.9748e-02, -1.3237e-01],\n          [ 5.7485e-03,  4.3640e-03, -2.1794e-02,  ...,  6.7570e-03,\n            3.4369e-03,  7.0808e-03],\n          [ 1.9519e-01,  4.5070e-02,  1.5320e-02,  ...,  3.4810e-02,\n           -5.0381e-02, -1.9583e-01]],\n\n         [[ 5.6741e-03,  2.7614e-03,  1.6247e-03,  ...,  2.1552e-05,\n            5.5974e-04, -1.0744e-03],\n          [ 3.1769e-02, -1.8088e-01,  7.9264e-03,  ...,  3.3389e-02,\n           -7.1930e-02,  6.4582e-02],\n          [ 7.2849e-03,  2.0841e-03, -1.0420e-03,  ...,  1.5843e-02,\n           -1.1953e-02,  9.6781e-03],\n          ...,\n          [-1.5549e-01,  1.2683e-01, -1.9559e-02,  ..., -5.6759e-02,\n            3.0501e-04,  7.7877e-02],\n          [ 1.7493e-03,  7.1972e-02,  4.0724e-02,  ..., -1.5790e-01,\n           -8.9693e-02, -8.0710e-02],\n          [-1.0907e-02, -1.9946e-01, -1.1967e-01,  ..., -5.7081e-02,\n           -4.5260e-02,  4.0982e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.7967e-02, -1.5538e-01, -3.5137e-02,  ..., -4.5287e-01,\n           -9.7894e-02,  9.9902e-01],\n          [ 2.1435e+00, -1.1052e+00,  2.9003e-01,  ...,  3.4048e-01,\n           -1.8103e-01, -4.0469e+00],\n          [-1.1528e+00,  8.0551e-02, -3.3165e+00,  ..., -4.6863e-01,\n            1.6778e-02, -3.6705e+00],\n          ...,\n          [-2.0375e+00,  2.7392e+00, -9.9266e-01,  ...,  3.6336e-01,\n            3.0317e-01, -3.8284e+00],\n          [-1.0166e+00,  2.8167e+00, -1.3171e+00,  ..., -1.0825e-01,\n           -2.5986e-01, -3.9744e+00],\n          [ 2.1676e+00,  1.0250e+00,  1.3725e-01,  ..., -1.1101e+00,\n            4.4459e-01, -4.0472e+00]],\n\n         [[ 2.0526e-01, -1.0120e-01,  2.8459e-02,  ...,  8.7780e-01,\n           -3.6544e-01, -9.6916e-01],\n          [ 1.4555e+00, -5.0022e+00, -2.4639e+00,  ..., -1.7220e+00,\n            1.2669e+00,  2.8843e+00],\n          [-1.7542e+00, -3.8467e+00, -3.0021e+00,  ..., -1.2341e+00,\n           -2.3595e+00,  3.0494e+00],\n          ...,\n          [-2.5242e+00, -5.1299e-01, -2.8127e+00,  ..., -3.1357e+00,\n            6.3081e-01,  2.5451e+00],\n          [ 3.9007e+00,  1.7564e+00, -1.6855e+00,  ..., -1.6215e+00,\n            1.2152e-01,  3.5846e+00],\n          [ 3.4854e+00,  3.3406e+00, -2.1783e+00,  ..., -2.6598e+00,\n           -1.6483e+00,  2.4277e+00]],\n\n         [[-2.4163e-02, -1.3793e-02, -1.5605e-02,  ..., -3.3829e-01,\n           -2.1568e-01, -2.7395e-01],\n          [-1.6232e+00, -9.9561e-01, -2.0859e+00,  ...,  7.0393e-01,\n           -1.0576e+00, -9.0824e-01],\n          [-7.1267e-02, -2.0515e+00, -1.4744e+00,  ...,  5.6114e-01,\n            5.6179e-01,  4.5942e-01],\n          ...,\n          [ 2.2636e+00, -3.5167e-03, -1.3527e+00,  ...,  6.2748e-01,\n           -4.1023e-01,  6.6814e-01],\n          [ 7.4718e-01,  8.5385e-01, -5.1241e-01,  ...,  6.5449e-01,\n           -2.8959e-01,  1.1189e+00],\n          [-5.5142e-01,  2.2874e+00,  1.1139e-03,  ..., -6.2647e-01,\n           -1.2708e+00,  1.7367e-01]],\n\n         ...,\n\n         [[ 1.8175e-01, -6.8501e-02,  9.8065e-02,  ...,  5.6903e-01,\n            6.9947e-01,  3.4793e-01],\n          [ 1.2064e+00, -1.8463e-01,  2.7027e+00,  ..., -5.7690e+00,\n           -3.5259e+00, -2.6167e+00],\n          [-4.0773e+00,  1.5611e+00,  1.7085e+00,  ..., -1.9952e+00,\n           -4.0612e+00, -4.2319e+00],\n          ...,\n          [-2.2839e+00,  3.9910e+00, -5.4121e-01,  ..., -4.6933e+00,\n           -5.7405e+00, -2.9257e+00],\n          [ 2.5067e+00,  1.7773e+00, -1.2392e+00,  ..., -6.0192e+00,\n           -5.4825e+00, -4.3812e+00],\n          [ 5.1151e+00, -6.7553e-02, -2.5948e+00,  ..., -3.0317e+00,\n           -4.5868e+00,  2.0757e+00]],\n\n         [[ 2.4641e-01, -7.5120e-04,  1.4931e-01,  ...,  4.2755e-02,\n            2.9609e-01, -5.9467e-01],\n          [ 2.2968e+00, -1.0072e+00,  2.1115e+00,  ..., -2.2388e+00,\n            1.3980e+00,  2.3438e+00],\n          [-2.3381e+00, -3.5296e+00,  1.1245e+00,  ...,  1.0777e+00,\n            8.0039e-02,  3.0540e+00],\n          ...,\n          [-4.5341e+00, -1.4650e+00, -1.5961e+00,  ...,  1.7733e-01,\n           -1.4686e+00,  4.1432e+00],\n          [ 7.0741e-01,  1.2569e+00, -2.8351e+00,  ..., -1.0399e+00,\n           -7.8148e-01,  3.2712e+00],\n          [ 6.3685e+00,  2.0896e+00, -4.8673e+00,  ...,  7.3676e-01,\n           -1.1852e+00,  4.6863e+00]],\n\n         [[ 6.5294e-02,  5.4152e-03,  9.5116e-02,  ..., -2.6992e-01,\n            1.3247e-01, -1.0223e+00],\n          [ 1.0522e+00,  1.3931e+00,  1.9404e+00,  ..., -1.5272e+00,\n            1.4662e+00,  1.2632e+00],\n          [ 1.4162e-01,  8.8900e-01,  3.2562e-01,  ...,  4.7265e-01,\n            1.1191e+00,  1.5841e+00],\n          ...,\n          [-9.9284e-01,  5.0924e-01, -1.1408e+00,  ...,  2.1557e+00,\n           -1.8703e-01,  7.6806e-01],\n          [ 1.0320e-01,  1.1855e-01, -1.0493e+00,  ...,  2.0507e+00,\n            1.0512e+00,  7.7381e-01],\n          [ 1.9356e+00, -2.4486e+00, -6.1142e-01,  ..., -7.0353e-01,\n           -8.7412e-01,  8.5476e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-3.2377e-03,  3.6782e-03, -2.0835e-03,  ...,  2.1884e-03,\n           -1.7065e-03, -1.4043e-02],\n          [-6.8249e-02, -9.0891e-02,  7.4523e-02,  ...,  4.0324e-02,\n            1.1355e-01,  8.2261e-02],\n          [-1.2549e-01,  8.1891e-02, -1.0486e-01,  ..., -2.0578e-02,\n           -8.8899e-02,  1.4754e-01],\n          ...,\n          [ 3.4477e-01,  2.5028e-02, -1.5197e-02,  ..., -1.4269e-01,\n            2.6218e-01,  6.2984e-02],\n          [-1.9109e-01, -1.9266e-02,  1.1804e-01,  ..., -3.0260e-01,\n            2.0018e-01,  3.0816e-01],\n          [-1.1212e-01, -8.4712e-02, -8.4004e-03,  ..., -1.1759e-01,\n            6.7740e-02,  6.6850e-02]],\n\n         [[ 4.1326e-04,  1.7973e-03, -3.2491e-03,  ..., -1.8416e-03,\n           -2.1346e-03, -5.6098e-03],\n          [ 7.9583e-02, -1.7815e-01, -5.7443e-03,  ...,  2.3471e-01,\n            1.9975e-01,  7.2629e-02],\n          [ 2.4855e-02,  1.8370e-01, -4.9052e-02,  ..., -9.4083e-02,\n           -9.4262e-03, -1.7727e-02],\n          ...,\n          [ 2.2283e-01,  5.1064e-02,  8.6225e-02,  ..., -5.5915e-02,\n            3.2762e-01,  1.3147e-01],\n          [-6.8345e-02,  6.0235e-02,  1.0391e-01,  ...,  4.2214e-02,\n            2.4259e-02, -2.2577e-01],\n          [-1.5042e-02,  8.3515e-02, -3.7921e-02,  ..., -2.3938e-01,\n            1.6055e-01, -5.0694e-02]],\n\n         [[ 7.5014e-03, -3.1590e-03,  1.1861e-02,  ...,  9.1303e-03,\n            8.1564e-03, -1.5911e-01],\n          [ 6.1164e-02, -7.1458e-03, -6.6609e-02,  ...,  7.1957e-02,\n           -2.3087e-02,  5.9988e-01],\n          [-2.5318e-01, -1.0410e-01, -5.7920e-02,  ...,  2.6127e-01,\n            3.5043e-02,  8.3340e-01],\n          ...,\n          [-3.3453e-02,  1.8735e-01,  6.7241e-02,  ...,  2.0223e-01,\n            1.4901e-01,  5.9556e-01],\n          [ 1.0063e-01,  1.1752e-01,  4.5007e-01,  ...,  6.4598e-02,\n            1.2316e-01,  3.3600e-01],\n          [ 7.8331e-02, -7.1940e-02, -2.5699e-01,  ...,  1.2051e-02,\n            2.1061e-01,  4.6554e-01]],\n\n         ...,\n\n         [[-8.6541e-03, -3.8981e-03, -2.6316e-02,  ...,  1.9426e-02,\n            1.4738e-03,  3.2282e-03],\n          [-5.9135e-02, -9.9246e-02, -3.6900e-01,  ...,  1.0190e-01,\n            2.3477e-01, -3.7322e-03],\n          [ 6.5881e-02, -9.7573e-02, -9.5627e-02,  ...,  9.0878e-02,\n           -1.9417e-02, -1.0951e-02],\n          ...,\n          [-1.9661e-01,  1.1670e-01, -2.6479e-02,  ...,  1.9701e-02,\n            2.5399e-02, -9.7587e-02],\n          [-1.2173e-01, -1.6699e-01,  9.9858e-02,  ...,  5.7304e-02,\n           -4.9886e-02, -4.7641e-02],\n          [ 1.9774e-01,  1.5135e-02, -5.3215e-02,  ...,  8.8475e-02,\n            1.5693e-01, -9.6357e-02]],\n\n         [[ 5.1806e-03, -5.2374e-03, -2.2343e-03,  ..., -1.5930e-03,\n            3.5463e-03, -4.3412e-03],\n          [-5.9287e-02,  7.1173e-02, -2.3751e-02,  ..., -2.0564e-01,\n            2.2858e-01,  3.2094e-01],\n          [ 8.5236e-02,  6.9737e-02,  6.1274e-02,  ..., -1.8012e-01,\n           -3.3260e-02,  2.3315e-02],\n          ...,\n          [ 1.1825e-02, -5.9067e-02, -1.0706e-01,  ..., -1.6356e-01,\n           -3.4370e-01,  2.4248e-02],\n          [ 4.9969e-02,  1.4826e-01, -3.9919e-02,  ..., -4.0458e-02,\n            6.6567e-02, -8.3417e-02],\n          [ 1.0430e-01, -3.2007e-02, -1.4930e-01,  ...,  3.6746e-01,\n            1.4962e-01,  1.6755e-01]],\n\n         [[-5.6988e-04, -8.3401e-03, -8.2436e-03,  ..., -1.5176e-02,\n            7.5245e-03, -3.7639e-03],\n          [ 1.8856e-01,  2.7176e-01, -1.5545e-01,  ..., -1.4014e-02,\n           -3.9068e-02,  1.3135e-01],\n          [-5.3303e-02,  1.8317e-02,  3.4295e-02,  ..., -9.3917e-02,\n            6.7584e-02,  3.9990e-02],\n          ...,\n          [-1.3570e-01,  1.3095e-01,  1.1496e-01,  ..., -5.9814e-01,\n            9.0253e-02,  1.7751e-01],\n          [ 3.8223e-02,  3.8875e-02, -1.8843e-01,  ..., -9.1675e-02,\n            5.0060e-02, -2.1880e-02],\n          [-4.1986e-01,  2.1757e-01, -1.4327e-01,  ...,  7.7314e-02,\n            9.4747e-02,  8.5389e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7701e-01,  1.2814e-01, -5.6786e-02,  ...,  1.3507e-01,\n           -1.4585e+00, -6.9457e-01],\n          [ 2.6764e-01,  2.9066e+00, -3.2170e+00,  ...,  8.7905e-02,\n            4.8198e+00,  2.8236e+00],\n          [ 9.2695e-01,  1.4156e+00, -7.8957e-01,  ...,  1.7659e-01,\n            3.0773e+00,  9.0030e-01],\n          ...,\n          [-2.9926e-01,  3.5457e-02, -1.4828e+00,  ..., -1.5730e-01,\n            4.5621e+00,  9.8181e-01],\n          [-6.7563e-01, -2.2247e+00, -1.5221e+00,  ...,  5.3594e-01,\n            4.2234e+00,  1.0064e+00],\n          [-2.3657e+00, -1.6536e+00, -1.2066e+00,  ..., -4.5254e-01,\n            5.0909e+00,  1.6524e+00]],\n\n         [[-6.3640e-02, -1.6707e-02,  1.4471e-02,  ...,  4.7435e-01,\n           -3.9084e-01, -2.5429e-01],\n          [ 1.1823e+00,  5.1502e-01, -4.1835e-01,  ...,  1.1009e-01,\n           -2.9313e-01, -8.9718e-01],\n          [ 1.3956e+00,  1.6913e+00, -8.9101e-01,  ...,  8.5906e-02,\n            7.4851e-01, -8.5068e-01],\n          ...,\n          [-1.0720e+00,  1.5869e+00, -1.4292e+00,  ...,  4.5084e-01,\n           -3.9096e-01, -2.0741e+00],\n          [-1.3015e+00, -2.1746e-01, -1.5711e+00,  ...,  8.3230e-01,\n            9.6939e-01, -7.3736e-01],\n          [-1.5033e+00, -8.2268e-01, -7.3206e-01,  ...,  1.1917e+00,\n           -1.2171e+00, -1.7185e+00]],\n\n         [[ 8.8415e-02, -7.7824e-02,  5.8160e-02,  ...,  3.3606e-01,\n           -1.3101e+00, -1.1014e+00],\n          [-2.3669e+00, -3.6616e+00, -1.2765e+00,  ...,  2.0766e+00,\n            3.3702e+00,  5.0247e+00],\n          [-3.1251e+00, -3.1575e+00, -7.4505e-01,  ...,  4.9128e-01,\n            1.8747e+00,  4.4647e+00],\n          ...,\n          [ 2.0000e+00, -6.1031e-01, -1.6045e+00,  ...,  7.2175e-01,\n            3.4087e+00,  3.7380e+00],\n          [ 3.2028e+00,  2.2939e+00, -4.2439e+00,  ...,  4.8598e-01,\n            2.9832e+00,  3.1867e+00],\n          [ 2.3772e+00,  2.5177e+00, -1.3139e+00,  ...,  1.4361e+00,\n            3.7860e+00,  4.9523e+00]],\n\n         ...,\n\n         [[-2.5395e-01, -1.4475e-01,  1.7906e-01,  ..., -1.9359e-01,\n           -3.9308e-01,  8.8821e-01],\n          [-2.6796e+00, -2.0157e+00,  3.1024e+00,  ...,  4.2189e-01,\n           -1.3306e+00, -4.9016e+00],\n          [ 1.6348e+00, -5.3055e-02,  1.9740e+00,  ..., -1.4344e+00,\n           -2.4934e-01, -3.7893e+00],\n          ...,\n          [ 2.8807e+00,  1.4479e+00, -1.4614e-03,  ..., -1.0776e+00,\n            2.9119e-01, -3.3255e+00],\n          [-1.1927e+00,  1.2245e+00, -1.0029e+00,  ...,  5.8190e-01,\n            1.8443e+00, -3.6302e+00],\n          [-3.7693e+00,  2.5642e+00, -1.8331e+00,  ..., -1.2635e+00,\n           -5.5469e-02, -3.2170e+00]],\n\n         [[ 1.0696e-02, -8.8757e-02, -1.3791e-01,  ..., -5.1921e-01,\n            2.0680e-01, -9.1169e-02],\n          [ 6.0183e-01, -8.7852e-01, -2.0071e+00,  ...,  3.2347e-01,\n            6.0076e-01, -6.7956e-01],\n          [ 1.1107e+00, -1.4100e-01, -1.3417e+00,  ...,  2.6395e+00,\n            1.8479e+00,  1.0224e+00],\n          ...,\n          [-1.5660e+00,  8.7282e-01,  9.3701e-01,  ...,  1.8857e+00,\n            8.2834e-01,  2.4639e-01],\n          [-1.4213e+00,  1.9002e+00,  7.0173e-01,  ...,  1.7888e+00,\n            1.6171e+00, -8.0769e-01],\n          [-1.2590e+00,  1.6183e+00,  1.4074e+00,  ...,  8.6553e-01,\n           -4.7060e-01,  5.8349e-01]],\n\n         [[ 7.1884e-02,  1.4028e-01,  6.8085e-02,  ...,  2.4312e-01,\n           -1.7492e-01,  2.5649e-01],\n          [ 5.1536e+00,  3.8639e+00,  2.2726e+00,  ...,  4.6500e-01,\n           -2.1876e+00, -8.3482e-01],\n          [ 2.9126e+00,  3.1976e+00,  3.2291e+00,  ...,  7.0219e-01,\n           -1.1357e-01,  1.1334e+00],\n          ...,\n          [-4.1209e+00,  2.7656e-01,  2.6617e+00,  ..., -3.8955e-01,\n           -2.3895e-01,  3.2122e+00],\n          [-2.8110e+00,  3.8436e-01,  1.9234e+00,  ..., -1.6735e+00,\n            7.6658e-01,  1.5494e+00],\n          [-1.6525e+00, -1.5763e+00,  6.6924e-01,  ..., -1.4195e+00,\n           -1.4014e+00,  4.7321e-02]]]], grad_fn=<AddBackward0>), tensor([[[[-6.1820e-03,  5.4064e-03,  8.8012e-03,  ...,  1.4157e-02,\n            5.4133e-03, -9.3104e-03],\n          [ 2.9941e-01,  6.7903e-01, -1.6327e-01,  ...,  8.3898e-02,\n            2.1157e-01, -7.8838e-02],\n          [ 3.1184e-01,  7.6078e-02,  3.5302e-01,  ..., -2.8107e-01,\n           -1.5467e-01,  4.1138e-02],\n          ...,\n          [-9.6306e-02,  6.9173e-02,  6.5667e-02,  ...,  2.6577e-01,\n           -6.4739e-02, -3.4857e-02],\n          [-2.4713e-01, -3.5561e-01, -1.4759e-01,  ...,  1.7052e-01,\n            7.5728e-02, -1.5410e-01],\n          [-2.3465e-01,  1.5675e-01,  2.8819e-01,  ...,  1.2776e-01,\n            6.8719e-02, -2.5834e-01]],\n\n         [[-1.2713e-02,  1.2328e-01,  2.5158e-03,  ..., -2.7015e-03,\n            9.6137e-03,  1.6953e-03],\n          [ 7.1951e-01, -1.6914e-01, -1.1247e-01,  ..., -4.8129e-01,\n            1.4451e-01, -6.2263e-02],\n          [-2.4158e-01, -5.0414e-02,  1.1081e-01,  ...,  4.6947e-02,\n           -1.9938e-01,  7.7349e-02],\n          ...,\n          [-1.9011e-01, -1.5957e-01,  1.9725e-01,  ...,  2.3459e-01,\n            1.6987e-01,  1.8029e-01],\n          [-3.1155e-01, -3.1968e-01, -2.1609e-01,  ...,  2.5294e-01,\n            6.0543e-02,  1.5184e-01],\n          [ 1.5025e-01,  1.5711e-01, -1.4395e-02,  ..., -1.5603e-01,\n            6.3068e-02,  3.9409e-01]],\n\n         [[ 7.1219e-03,  3.1417e-03, -1.0653e-02,  ...,  1.6981e-02,\n           -7.8259e-03,  4.3196e-03],\n          [-1.9960e-01,  2.7524e-01,  1.9772e-02,  ..., -1.6835e-01,\n            8.5904e-02, -2.8022e-01],\n          [-1.0467e-01,  3.5048e-02,  1.1350e-02,  ...,  5.4866e-02,\n           -1.1864e-01,  2.1198e-01],\n          ...,\n          [-1.8213e-01,  9.0273e-03, -1.1572e-01,  ..., -9.9041e-02,\n            8.4770e-02,  4.1519e-01],\n          [-1.2092e-01, -3.8780e-01, -3.6987e-01,  ..., -3.8170e-01,\n           -2.6693e-02,  3.6690e-01],\n          [-5.4785e-02, -3.9241e-01, -1.1562e-01,  ...,  4.1272e-01,\n            3.9609e-01,  1.8474e-01]],\n\n         ...,\n\n         [[ 3.3089e-02,  1.1034e-03, -6.1658e-03,  ..., -2.2012e-01,\n            2.6411e-03, -9.0748e-04],\n          [ 2.0938e-01, -6.8443e-01, -3.5323e-01,  ...,  1.0073e+00,\n            2.5970e-01,  2.7422e-01],\n          [-9.7218e-01, -6.1478e-01,  2.2379e-01,  ...,  1.2794e+00,\n            1.3615e-01, -8.1561e-01],\n          ...,\n          [-3.2407e-01, -5.6607e-01,  1.0611e-01,  ...,  1.0359e+00,\n           -1.1281e-01, -1.8238e-01],\n          [-2.6283e-01,  2.3867e-02, -5.1711e-01,  ...,  1.1837e+00,\n            1.3125e-01,  1.7411e-01],\n          [-3.9711e-01, -2.7209e-01, -1.6776e-01,  ...,  8.4565e-01,\n           -7.8791e-02,  3.2866e-02]],\n\n         [[ 5.5222e-03,  5.5187e-03, -2.4976e-03,  ..., -1.4439e-02,\n            2.8718e-03, -1.2763e-02],\n          [-9.6075e-02, -8.6414e-02,  7.9327e-02,  ...,  3.3177e-02,\n            5.1969e-02, -1.4187e-01],\n          [-7.6938e-02, -2.0906e-01,  1.4066e-01,  ..., -1.1291e-01,\n            1.5014e-01, -7.1458e-02],\n          ...,\n          [ 1.4898e-02, -6.8294e-02, -7.0356e-02,  ...,  2.5958e-02,\n           -4.2350e-02,  7.3146e-02],\n          [-1.7195e-01, -3.7946e-01, -6.4187e-02,  ...,  1.7663e-02,\n           -1.0420e-02,  1.2907e-01],\n          [-1.8034e-01,  2.7801e-03,  4.4255e-01,  ..., -2.3100e-01,\n           -4.6818e-02,  2.5140e-01]],\n\n         [[ 9.3314e-03, -1.0290e-02, -8.7502e-03,  ...,  1.6052e-03,\n            4.6391e-03, -5.5440e-04],\n          [ 3.4664e-01, -8.4540e-01,  1.8082e-01,  ..., -1.9415e-01,\n            1.3139e-01,  3.4814e-02],\n          [ 6.2430e-01,  1.2965e-01,  3.7513e-02,  ..., -2.3402e-02,\n            5.5743e-02,  9.9857e-02],\n          ...,\n          [ 1.9298e-01, -3.6481e-01,  2.9809e-02,  ...,  2.0951e-02,\n            6.1376e-02, -8.9263e-02],\n          [-1.6985e-01,  3.1712e-01, -2.7905e-01,  ..., -2.4076e-01,\n            5.8419e-02, -1.1336e-01],\n          [-1.5146e-01, -2.7222e-01,  1.6510e-03,  ..., -6.8705e-03,\n           -4.0594e-01,  1.8751e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.2282e-02, -1.0165e-01, -1.1627e-01,  ..., -1.1198e+00,\n            1.2367e+00,  9.2278e-01],\n          [ 2.6947e+00, -1.1263e+00, -2.5378e+00,  ...,  3.8508e-01,\n            7.6976e-01, -9.4456e-01],\n          [ 1.3896e+00, -5.4602e-01, -1.3699e+00,  ..., -3.5502e-01,\n            9.4520e-01, -1.5680e+00],\n          ...,\n          [-1.7908e+00,  9.5140e-01, -1.4063e+00,  ..., -2.5337e-01,\n            6.5877e-02, -5.4208e-01],\n          [-7.0496e-01,  1.6231e-01, -5.4211e-01,  ..., -6.9233e-01,\n            1.3365e+00, -2.8431e+00],\n          [-2.0937e-01, -1.1077e-01, -5.4180e-01,  ..., -4.4429e-01,\n            6.7587e-01, -9.7499e-01]],\n\n         [[ 2.0206e-01,  1.7876e-01,  2.1073e-01,  ..., -1.2444e-01,\n            6.5449e-01,  1.8805e-01],\n          [ 5.5494e+00,  2.7431e-01,  2.8697e+00,  ...,  7.7296e-01,\n           -4.0696e-01,  8.3838e-01],\n          [ 1.1737e+00,  3.4204e-03,  2.3668e+00,  ...,  3.9985e-01,\n            9.8103e-01, -7.5665e-01],\n          ...,\n          [-3.2539e+00, -3.5224e+00,  2.2123e+00,  ..., -1.0414e+00,\n           -5.9234e-01,  1.4053e+00],\n          [-8.2012e-01, -4.5371e+00,  3.9510e-01,  ...,  1.8237e+00,\n            1.3257e+00,  1.9354e+00],\n          [ 1.8927e+00, -2.3753e+00, -4.3274e-01,  ..., -7.4800e-01,\n           -4.4143e-01,  1.1267e+00]],\n\n         [[ 2.1973e-02,  7.3497e-02, -5.5480e-02,  ..., -1.8062e-01,\n            1.7539e+00, -1.5784e+00],\n          [ 3.3127e+00,  2.5819e+00, -2.4716e+00,  ...,  1.6049e+00,\n           -5.4206e+00,  6.3410e+00],\n          [ 2.7562e+00,  2.5899e+00, -3.3643e+00,  ..., -1.7875e-01,\n           -4.6014e+00,  6.0351e+00],\n          ...,\n          [-2.7383e+00,  8.6641e-01, -4.0272e+00,  ...,  6.0972e-01,\n           -5.1756e+00,  5.1306e+00],\n          [-2.2211e+00,  1.1909e+00, -2.4088e+00,  ...,  3.1355e-01,\n           -5.1743e+00,  4.1928e+00],\n          [-1.0697e+00, -1.9966e+00, -1.2744e+00,  ..., -1.0002e-01,\n           -5.1887e+00,  5.8612e+00]],\n\n         ...,\n\n         [[ 1.1266e-01, -1.1705e-01,  7.7527e-02,  ..., -5.3134e-01,\n           -1.2289e+00,  2.8861e-01],\n          [ 3.4821e-01, -2.3385e+00,  1.6136e+00,  ..., -1.4544e+00,\n            4.0061e-01, -3.1225e-01],\n          [-1.1731e+00, -4.6898e-02,  1.0132e+00,  ..., -1.2839e+00,\n           -1.2630e-01,  9.5137e-01],\n          ...,\n          [-2.0203e-01,  1.2111e+00, -1.6005e-02,  ..., -5.1681e-01,\n           -3.3450e-01, -2.1627e-01],\n          [ 1.1841e+00, -2.7654e-01,  1.8011e-01,  ..., -5.9574e-01,\n           -6.3698e-01,  3.0269e-01],\n          [ 1.0746e+00,  7.7070e-01, -9.6412e-01,  ..., -1.0072e+00,\n           -7.1167e-02, -1.6784e-01]],\n\n         [[-2.3820e-01, -1.1915e-01, -1.3883e-01,  ...,  1.2986e+00,\n           -1.2263e+00,  1.8063e+00],\n          [-1.6037e+00, -1.0995e+00, -2.6342e+00,  ..., -1.7757e+00,\n           -1.0154e+00, -2.1865e+00],\n          [ 1.9105e+00,  8.1207e-01, -7.8228e-01,  ..., -1.4002e+00,\n           -1.6378e+00, -2.3000e+00],\n          ...,\n          [ 9.5449e-01,  1.6893e+00,  1.3387e+00,  ..., -9.9793e-01,\n            3.8434e-01, -8.7498e-01],\n          [-1.4753e+00,  2.2700e+00,  1.4921e+00,  ...,  2.1434e+00,\n            1.2297e+00, -9.2150e-01],\n          [-2.7791e+00,  3.6354e-01,  2.2254e+00,  ..., -2.0619e+00,\n           -1.6319e+00, -2.4281e+00]],\n\n         [[ 1.3673e-01,  1.1688e-01,  1.1799e-01,  ..., -7.8946e-01,\n           -2.1710e+00,  1.2962e+00],\n          [ 1.4964e+00,  3.5646e+00,  6.2662e-01,  ..., -6.0104e-01,\n            4.9614e+00, -1.0345e-01],\n          [-9.5273e-01,  8.1729e-01,  6.4827e-01,  ...,  9.7054e-02,\n            7.1365e+00,  5.5422e-01],\n          ...,\n          [-7.5994e-01, -3.1559e-01,  3.6393e-01,  ..., -2.4653e-01,\n            5.8504e+00, -1.6054e-01],\n          [ 6.1265e-01, -4.1892e-01,  2.2466e-01,  ..., -3.9562e-01,\n            6.2402e+00,  2.3792e-01],\n          [ 1.0237e+00, -1.5125e+00, -1.0386e-02,  ...,  2.2863e-01,\n            5.2406e+00, -2.2171e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.4321e-03, -8.7532e-03, -4.7245e-03,  ...,  8.1068e-03,\n            8.7612e-04,  6.9124e-03],\n          [-1.0467e-01, -1.9611e-02,  2.4791e-02,  ..., -4.3568e-02,\n           -5.7890e-02,  2.3274e-01],\n          [-9.9035e-02, -3.5960e-01,  1.4438e-01,  ..., -2.2842e-02,\n            1.6327e-01,  3.1524e-01],\n          ...,\n          [-3.0338e-01, -7.6661e-03,  4.5093e-01,  ...,  1.9181e-01,\n            2.8696e-01, -9.1073e-02],\n          [-3.0866e-02, -6.2382e-02,  2.9084e-02,  ..., -1.5676e-01,\n           -1.7092e-01,  1.1898e-01],\n          [-2.6783e-01, -1.7185e-01,  1.7530e-01,  ...,  1.3997e-01,\n            9.0545e-02, -3.8462e-01]],\n\n         [[ 7.0619e-04, -3.2205e-03,  8.7327e-03,  ...,  1.4436e-03,\n            2.0123e-02,  9.0113e-04],\n          [ 1.2734e-02, -4.0825e-02,  4.3852e-03,  ..., -1.2760e-02,\n           -5.4517e-01, -2.6769e-01],\n          [-1.6712e-01, -1.6344e-01,  9.6333e-02,  ..., -6.8344e-03,\n           -2.1898e-02, -3.1479e-01],\n          ...,\n          [ 3.0523e-01,  2.7330e-01, -6.8375e-03,  ..., -1.5098e-01,\n           -1.0993e-01,  3.0583e-01],\n          [ 3.4018e-01, -7.7130e-02, -1.4879e-01,  ..., -3.0749e-02,\n            5.6093e-01,  1.9593e-01],\n          [ 1.0211e-01,  1.3157e-01, -1.3349e-01,  ...,  1.2502e-01,\n           -6.5941e-02, -1.1809e-01]],\n\n         [[-2.7866e-03,  1.0993e-02,  6.1155e-03,  ...,  1.5371e-02,\n            2.8720e-03, -2.3152e-02],\n          [-6.4073e-02,  1.5430e-02,  2.6343e-01,  ...,  8.0253e-02,\n            1.7867e-01,  1.2398e+00],\n          [ 5.1941e-02,  1.3583e-01, -1.6573e-01,  ..., -8.3535e-02,\n            1.3651e-01,  1.5384e-01],\n          ...,\n          [-1.2322e-01,  1.8373e-01, -8.2243e-02,  ...,  1.0742e-01,\n            3.1176e-01,  3.3107e-01],\n          [-1.7566e-02,  1.9896e-01, -1.6854e-02,  ...,  7.8815e-02,\n            2.5707e-01,  1.6920e-02],\n          [ 7.9629e-03,  1.1664e-02, -1.9343e-01,  ..., -2.2412e-02,\n            2.4916e-01,  3.5274e-01]],\n\n         ...,\n\n         [[-1.4118e-02, -3.8184e-03,  1.3744e-02,  ..., -4.0826e-04,\n           -4.0499e-03,  8.0336e-03],\n          [ 5.4029e-02, -1.0008e-01, -3.6924e-01,  ..., -5.0542e-01,\n           -1.2258e-01, -1.0312e-01],\n          [ 2.2759e-02, -7.9112e-02, -9.1752e-02,  ..., -3.1552e-01,\n           -1.4367e-01,  7.4229e-02],\n          ...,\n          [-2.3685e-01, -5.3028e-01, -7.6086e-02,  ..., -9.8673e-02,\n           -1.3463e-02,  2.3224e-01],\n          [-4.5401e-01,  1.1683e-01, -2.2798e-02,  ..., -6.0475e-01,\n           -3.5990e-01, -1.3324e-01],\n          [-1.8999e-01,  7.5736e-02,  2.8255e-02,  ..., -1.2506e-01,\n           -1.9876e-01,  2.6780e-01]],\n\n         [[-3.5600e-03,  2.4092e-04, -9.2962e-03,  ..., -1.4673e-02,\n            4.7458e-03, -1.1192e-02],\n          [-1.5482e-02,  1.1980e-01,  2.1448e-01,  ...,  6.4581e-02,\n           -1.7597e-01,  1.1077e-01],\n          [ 2.6037e-02,  7.4305e-02, -1.0224e-01,  ..., -6.9476e-02,\n           -9.4906e-02, -2.6536e-01],\n          ...,\n          [ 1.0273e-01,  8.0308e-02,  1.1856e-01,  ...,  1.3968e-01,\n            1.5410e-01, -1.1429e-01],\n          [ 1.1800e-01,  5.7852e-02,  1.4395e-01,  ...,  1.1984e-01,\n            1.3060e-01, -9.2328e-02],\n          [-1.5998e-01, -8.2837e-02,  8.2714e-02,  ..., -3.5956e-02,\n           -9.6886e-02, -8.8186e-02]],\n\n         [[ 5.8911e-03, -8.2265e-03,  5.3291e-03,  ...,  8.7417e-04,\n           -1.3503e-02, -1.0993e-02],\n          [ 2.8653e-03,  4.0489e-03, -2.4093e-01,  ..., -1.3937e-01,\n            2.8739e-01,  5.5181e-01],\n          [ 1.2024e-01, -1.2629e-02,  2.7387e-02,  ...,  1.7477e-01,\n            1.2676e-01,  1.6036e-01],\n          ...,\n          [ 5.3377e-02,  2.0845e-01,  4.0209e-03,  ...,  7.6788e-02,\n            5.8192e-01,  2.6301e-01],\n          [ 3.5664e-01, -8.8840e-02, -1.1054e-01,  ...,  7.6085e-03,\n            6.6291e-01, -1.2942e-01],\n          [ 2.7074e-02,  1.2920e-01, -1.4235e-01,  ...,  3.5492e-01,\n            7.7556e-02,  6.1752e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0673, -0.0380,  0.0587,  ..., -0.2401,  1.2458, -0.5890],\n          [-2.6230, -1.4096,  1.0336,  ..., -1.0997,  0.7870, -2.2670],\n          [ 0.0684, -0.8596, -0.4222,  ...,  0.0686,  1.8364, -1.7942],\n          ...,\n          [ 0.4360,  0.0993, -0.3778,  ...,  0.2330,  0.4170, -1.8798],\n          [-0.4057,  0.0715, -0.6293,  ...,  0.2380,  1.6907, -3.2166],\n          [-0.4575,  0.9622, -0.8712,  ...,  0.7507,  0.7713, -1.9774]],\n\n         [[-0.1482,  0.1215, -0.1029,  ..., -1.0353, -1.0146,  0.8210],\n          [-2.7964,  1.4028, -2.8288,  ..., -0.8176, -0.8423,  1.3311],\n          [ 1.2582,  0.1732, -1.6765,  ..., -0.2601, -0.9366,  1.1228],\n          ...,\n          [ 1.3964, -1.2859,  0.7055,  ...,  0.5220, -1.2482, -0.1308],\n          [-1.7408, -1.5983,  0.9078,  ...,  0.0471, -0.5481,  0.2924],\n          [-1.7288, -1.0642,  1.4368,  ...,  0.5570, -1.4549, -0.8904]],\n\n         [[ 0.0888, -0.0451, -0.0820,  ...,  1.2527, -1.6093,  0.8153],\n          [ 1.1792, -1.2164, -3.3174,  ...,  1.8395,  2.3515, -0.6136],\n          [-1.5866,  0.1771, -1.9733,  ...,  1.0520,  2.0242, -0.3927],\n          ...,\n          [-1.0689,  1.1365, -0.1952,  ...,  0.5337,  1.3976, -0.5168],\n          [ 0.3324,  1.2364, -0.5633,  ...,  0.5687,  2.1237, -0.3122],\n          [ 2.0921,  0.1781,  0.9043,  ...,  1.3126,  1.3001,  0.6655]],\n\n         ...,\n\n         [[ 0.1052, -0.0565,  0.0683,  ..., -0.1388,  0.3983,  1.4133],\n          [ 3.4661, -1.0068,  1.4672,  ..., -0.7612, -0.6297, -3.9924],\n          [ 0.2750,  0.7824,  0.9173,  ..., -0.4139,  0.6396, -3.5821],\n          ...,\n          [-1.0323,  0.2570, -0.7744,  ..., -0.0266,  0.3943, -1.8256],\n          [ 0.6397,  0.2168, -0.2871,  ..., -0.5229,  0.9866, -2.3851],\n          [ 0.4325,  0.0755, -0.7504,  ..., -0.3195,  1.3672, -1.1089]],\n\n         [[ 0.1472,  0.0827,  0.0479,  ...,  0.3101,  0.0898, -0.3013],\n          [ 1.7290,  1.5935,  0.6793,  ...,  0.7582,  1.3635,  0.5977],\n          [-0.9900,  0.1284,  1.2406,  ...,  0.2236, -0.4528, -1.5582],\n          ...,\n          [-0.7661, -0.5840,  2.1097,  ..., -0.0846,  0.3760,  0.1993],\n          [ 0.5308, -1.5368,  1.5667,  ..., -0.7289, -0.2877, -1.1430],\n          [ 1.6712, -0.8101,  1.7596,  ...,  1.8957,  0.6416,  0.1783]],\n\n         [[-0.1321,  0.1387, -0.0576,  ..., -0.8415,  1.0974,  0.2400],\n          [ 0.5177,  3.2533, -1.6478,  ..., -0.1231, -0.1318,  1.0980],\n          [ 2.1462,  0.3267, -2.0895,  ..., -0.5722,  0.1243,  0.8788],\n          ...,\n          [ 0.0263, -0.7484, -1.8913,  ..., -1.8921, -0.5946,  1.4830],\n          [-1.3986, -0.6276, -1.3345,  ..., -1.0882,  1.3606,  1.5931],\n          [-1.7844, -1.3445, -0.5183,  ..., -0.7938, -0.0828,  1.6731]]]],\n       grad_fn=<AddBackward0>), tensor([[[[ 7.0184e-03, -2.6607e-01,  8.9918e-03,  ...,  1.0008e-02,\n            9.8950e-03,  2.1153e-02],\n          [-1.7189e-02,  7.4637e-01,  7.3324e-02,  ...,  1.7711e-01,\n            1.8271e-01, -1.9652e-01],\n          [ 4.3036e-01,  4.6241e-01, -2.1777e-02,  ..., -5.7014e-02,\n           -4.1061e-01, -3.6297e-01],\n          ...,\n          [ 1.1786e-01,  3.7584e-01, -1.6500e-01,  ...,  4.3060e-01,\n            1.1172e-01, -3.4991e-01],\n          [ 1.5505e-01,  6.0234e-01, -2.4335e-03,  ..., -1.4892e-01,\n            1.3472e-01, -1.7865e-01],\n          [ 8.8841e-02,  5.1718e-01, -3.3933e-01,  ...,  2.1001e-01,\n           -2.9975e-02, -1.9431e-01]],\n\n         [[-5.5726e-04, -7.7484e-03, -5.9076e-03,  ..., -2.0097e-02,\n           -3.3195e-03, -1.2772e-02],\n          [-1.1337e-01,  2.4335e-02,  1.2576e-01,  ..., -2.8663e-02,\n            7.0448e-01,  1.6985e-01],\n          [-4.6211e-01,  5.2771e-02,  1.5753e-01,  ..., -2.2525e-01,\n            1.1754e-01, -1.0439e-01],\n          ...,\n          [-5.2622e-01,  3.3897e-01, -3.6210e-03,  ..., -1.6833e-01,\n            8.1798e-03, -3.5989e-02],\n          [-1.7898e-01,  1.4902e-01,  4.3760e-02,  ..., -6.7576e-02,\n            9.4174e-02, -1.0657e-01],\n          [ 1.6964e-01, -3.3308e-01, -1.3923e-01,  ..., -3.0010e-01,\n           -2.0117e-01,  1.6435e-02]],\n\n         [[-3.5693e-02, -7.9178e-03,  1.0561e-02,  ...,  7.9003e-03,\n            1.1250e-02,  1.1793e-02],\n          [-2.1157e-01, -9.3278e-01,  1.2789e-01,  ..., -2.2399e-02,\n           -3.4019e-01, -3.9702e-01],\n          [ 1.5232e-01, -6.8308e-01, -1.2667e-02,  ..., -4.4224e-02,\n            1.6209e-01, -4.1417e-01],\n          ...,\n          [ 1.5063e-01, -2.0814e-01,  2.1461e-01,  ..., -9.8562e-02,\n           -3.6636e-02, -3.1627e-01],\n          [ 3.9176e-01, -1.8605e-01,  2.8391e-02,  ..., -9.8264e-02,\n           -9.5677e-03, -3.8438e-01],\n          [-5.0047e-02, -4.1200e-01,  2.9480e-01,  ...,  3.6376e-01,\n           -8.1028e-02, -4.4138e-01]],\n\n         ...,\n\n         [[-1.7020e-02,  2.0514e-02, -1.3219e-02,  ..., -2.7396e-03,\n           -2.3856e-03,  1.1714e-02],\n          [-4.4627e-02, -9.1744e-02, -9.9211e-02,  ...,  7.2449e-02,\n            9.8154e-02, -3.2808e-02],\n          [ 7.0124e-02,  1.2697e-01,  1.8704e-01,  ..., -1.5831e-01,\n           -1.7343e-02,  1.6386e-01],\n          ...,\n          [-2.0358e-01, -1.5230e-01,  3.7804e-02,  ...,  7.0350e-02,\n           -4.1890e-01,  5.0514e-02],\n          [-1.2607e-02,  2.3928e-01, -3.0951e-02,  ...,  1.8131e-01,\n           -3.2832e-01, -2.9254e-01],\n          [-7.5559e-02, -2.4791e-02, -7.3714e-02,  ...,  2.6189e-01,\n           -1.7322e-01, -8.1548e-02]],\n\n         [[ 2.9759e-03,  9.9851e-03,  1.2614e-02,  ...,  4.7422e-02,\n            1.3567e-02, -3.1465e-03],\n          [ 1.0880e-01,  1.3510e+00,  1.3634e-01,  ...,  2.3499e-02,\n           -1.7103e-01,  1.1104e-01],\n          [ 1.5093e-01,  5.5053e-01,  1.4742e-01,  ..., -5.6128e-02,\n            8.8363e-02,  1.1552e-01],\n          ...,\n          [-8.5937e-02,  1.9849e-01, -5.3613e-02,  ..., -2.0514e-01,\n           -2.6807e-02, -6.9927e-03],\n          [-3.7672e-01, -3.1404e-01, -1.6301e-01,  ..., -1.0568e-01,\n           -2.0208e-01,  3.4364e-01],\n          [ 1.1627e-01,  4.4837e-01, -6.7039e-01,  ...,  2.4221e-01,\n           -4.9293e-01,  2.4366e-01]],\n\n         [[-9.0115e-03,  5.8156e-03,  5.7625e-03,  ...,  1.5618e-03,\n           -8.1958e-03, -1.0273e-02],\n          [ 3.4790e-01, -3.6076e-01,  2.4894e-01,  ..., -3.2158e-01,\n           -5.9973e-02, -1.2560e-01],\n          [ 2.7315e-01, -8.5368e-02,  1.3310e-01,  ...,  1.5860e-01,\n           -1.6372e-01, -8.2068e-03],\n          ...,\n          [-1.0695e-01, -2.5185e-03,  1.4617e-01,  ..., -4.5404e-02,\n           -1.9578e-01, -2.9609e-02],\n          [ 1.1120e-01,  9.2414e-02,  3.8157e-02,  ..., -4.1317e-02,\n           -1.5420e-01, -2.1623e-01],\n          [-1.6257e-01, -1.2010e-02,  6.4325e-02,  ...,  2.4202e-01,\n           -1.9851e-01, -1.4417e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.6317e-02,  1.3466e-01,  5.4830e-02,  ..., -9.1992e-01,\n           -1.4019e+00,  1.4818e+00],\n          [ 1.2326e+00,  3.1613e+00,  1.6568e+00,  ...,  1.8509e-01,\n           -1.3722e+00, -6.6491e+00],\n          [ 1.9328e+00,  1.8872e+00,  2.0568e+00,  ...,  3.9007e-01,\n           -7.1264e-01, -6.8318e+00],\n          ...,\n          [-1.2359e+00, -1.2981e+00,  2.6715e+00,  ..., -5.8398e-01,\n            1.1117e-01, -7.2999e+00],\n          [-1.6475e+00, -1.7938e+00,  9.2559e-02,  ..., -3.8479e-02,\n           -1.5146e-01, -6.6211e+00],\n          [-1.8923e+00, -2.7682e+00,  4.9398e-01,  ..., -9.5186e-01,\n           -5.1755e-01, -7.0788e+00]],\n\n         [[-5.3628e-02,  8.0139e-02, -8.0991e-02,  ..., -1.3485e+00,\n           -1.5041e+00,  6.0794e-01],\n          [-1.0587e+00,  1.2573e+00, -1.7250e+00,  ...,  2.2708e-01,\n            5.9032e-01,  5.7291e-01],\n          [-2.1097e-01,  4.3254e-01, -1.0558e+00,  ...,  1.4978e+00,\n            1.1671e+00,  1.4344e+00],\n          ...,\n          [ 1.2665e+00, -7.3746e-01, -6.9133e-01,  ...,  5.8056e+00,\n            1.6770e+00,  1.7636e+00],\n          [-6.8971e-01, -3.6889e-01,  6.9248e-01,  ...,  6.6919e+00,\n            4.2297e+00,  1.0080e+00],\n          [-4.3376e-01, -1.0968e+00,  5.8477e-01,  ...,  5.3802e+00,\n            6.0880e-01,  1.7648e+00]],\n\n         [[-5.5008e-05, -5.0495e-02, -5.0035e-02,  ..., -3.3727e-01,\n            2.7300e-01, -4.0619e-01],\n          [ 6.1092e-01, -4.9943e-01, -5.3615e-01,  ...,  4.8645e-01,\n           -1.3054e+00,  7.6184e-01],\n          [ 8.0350e-01, -2.8751e-01, -1.0037e+00,  ...,  5.0757e+00,\n           -3.8508e+00,  2.2675e+00],\n          ...,\n          [-3.5077e-01,  3.9110e-01, -7.7770e-02,  ...,  6.1296e+00,\n           -2.7130e+00,  1.4375e+00],\n          [-2.9203e-01,  4.3968e-01, -1.1092e-01,  ...,  5.5444e+00,\n           -1.5172e+00,  3.0202e+00],\n          [-2.7358e-02, -3.0619e-02,  3.5029e-01,  ...,  5.5981e+00,\n           -5.3268e-01,  3.4868e+00]],\n\n         ...,\n\n         [[ 1.0437e-01, -3.3548e-02,  1.2506e-01,  ...,  1.6803e+00,\n           -1.1270e+00, -1.2944e-01],\n          [ 1.7417e+00, -2.8875e-01,  2.3643e+00,  ..., -2.9597e+00,\n           -1.9080e+00,  1.2661e+00],\n          [ 2.9395e-02,  7.7660e-01,  4.5813e-01,  ..., -5.1198e+00,\n            2.3659e-02, -2.0262e-01],\n          ...,\n          [-1.4781e+00,  1.3819e+00, -1.3117e+00,  ..., -3.8288e+00,\n           -1.2726e+00, -1.0869e+00],\n          [ 3.4382e-01,  7.7357e-01, -2.1937e+00,  ..., -3.9219e+00,\n            3.8463e-01, -1.7025e+00],\n          [ 5.3140e-01, -1.3983e-01, -1.8373e+00,  ..., -4.2518e+00,\n           -1.2880e-02, -1.2317e+00]],\n\n         [[-6.8644e-02, -8.1956e-02, -4.0383e-02,  ...,  8.9328e-01,\n            1.8409e+00,  1.4446e+00],\n          [-1.1454e+00, -1.9503e+00, -9.7478e-01,  ...,  3.1692e-01,\n           -3.3628e+00, -1.6152e-01],\n          [ 4.7400e-01, -8.7426e-01, -1.1007e+00,  ...,  3.3010e-01,\n           -3.5354e+00, -2.1006e-01],\n          ...,\n          [ 1.1901e+00,  1.3551e+00, -4.0182e-01,  ..., -9.0979e-02,\n           -3.8868e+00,  6.3973e-01],\n          [ 1.3428e-01,  7.1527e-01, -3.9368e-03,  ...,  7.7908e-02,\n           -3.9934e+00,  3.0507e-01],\n          [-1.0753e+00,  9.4585e-01,  1.7405e-01,  ...,  1.5428e-01,\n           -4.0549e+00, -2.9338e-01]],\n\n         [[-4.6730e-02,  3.6169e-02,  1.8060e-01,  ..., -1.3124e+00,\n            3.0565e-01, -9.7563e-01],\n          [-4.1502e+00,  1.2285e+00,  3.0599e+00,  ..., -1.2249e+00,\n            1.0287e+00, -3.6803e-01],\n          [-1.2745e+00,  9.7856e-01,  1.4054e+00,  ..., -1.7665e+00,\n            7.9501e-01, -9.4531e-01],\n          ...,\n          [ 2.1988e+00,  2.1710e-01, -1.2996e+00,  ..., -7.9220e-01,\n           -2.9255e-01, -7.1018e-01],\n          [ 1.0463e-01, -9.0516e-01, -6.1460e-01,  ..., -1.5183e+00,\n           -2.8626e-01, -1.4258e-01],\n          [ 3.0811e-01, -2.4144e+00, -2.5698e+00,  ...,  7.1495e-01,\n           -7.5689e-01,  3.9869e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5741e-02,  1.2704e-02,  1.0975e-02,  ..., -1.9629e-02,\n           -4.1710e-02,  1.3925e-02],\n          [ 1.0290e-01, -8.6253e-02, -7.6577e-02,  ..., -3.2430e-02,\n            4.8261e-01, -1.7361e-01],\n          [ 7.6148e-02,  1.4310e-01, -2.5367e-01,  ..., -1.9399e-01,\n            5.5266e-01, -2.5191e-01],\n          ...,\n          [-1.1625e-01,  1.5059e-01, -1.2364e-02,  ...,  3.2351e-02,\n            5.4142e-01,  1.3567e-01],\n          [-3.2112e-01,  1.6964e-01, -1.5878e-02,  ..., -8.1791e-02,\n            4.7819e-01,  5.1879e-02],\n          [ 1.2166e-01, -2.2661e-02,  1.4029e-01,  ...,  2.2474e-01,\n            6.1302e-01,  7.6070e-02]],\n\n         [[-7.9166e-04,  2.3437e-03,  8.0485e-03,  ...,  7.7399e-03,\n            9.5045e-03,  4.6631e-03],\n          [ 4.2380e-02, -8.7425e-02, -1.3761e-01,  ...,  7.3599e-02,\n           -2.6021e-01, -2.5217e-01],\n          [-1.7206e-01,  1.6248e-01, -2.3086e-01,  ..., -3.4205e-01,\n           -2.5810e-01, -3.5532e-01],\n          ...,\n          [-9.6249e-02,  2.2249e-01, -6.6095e-01,  ..., -2.5532e-01,\n           -2.2615e-01, -2.3028e-01],\n          [ 9.3356e-02,  2.3495e-01, -5.7557e-01,  ..., -4.1006e-01,\n           -3.8410e-01,  2.0350e-02],\n          [ 1.2238e-01,  2.7590e-01, -1.5102e-01,  ..., -1.0727e-01,\n           -3.4931e-01, -3.5034e-01]],\n\n         [[ 3.9210e-02,  1.9393e-04,  3.3262e-03,  ...,  8.1781e-03,\n           -9.4381e-03, -7.9167e-03],\n          [-9.4547e-01,  1.1973e-01, -2.8968e-01,  ..., -1.8359e-01,\n            1.4042e-01,  8.3514e-03],\n          [-8.3745e-01,  2.7255e-01, -3.6366e-01,  ...,  3.1175e-01,\n           -9.0469e-01,  3.0255e-02],\n          ...,\n          [-8.4997e-01,  2.1602e-01,  7.3421e-02,  ...,  1.1707e-02,\n           -1.1145e-01, -2.7753e-02],\n          [-8.5703e-01, -2.1940e-02, -5.1277e-02,  ..., -1.4274e-01,\n            1.5582e-01,  1.0579e-01],\n          [-8.0754e-01, -1.9184e-02,  1.6539e-01,  ...,  6.7397e-02,\n            3.5809e-02, -1.3506e-01]],\n\n         ...,\n\n         [[ 1.9551e-02,  1.0297e-02,  1.5773e-03,  ..., -1.1819e-02,\n           -1.2796e-03, -1.1804e-02],\n          [ 1.8765e-01, -2.8786e-02, -2.9936e-02,  ..., -3.0006e-02,\n           -2.2696e-01, -1.0051e-01],\n          [-6.9160e-02,  2.4853e-01, -1.6608e-01,  ..., -4.4749e-01,\n            2.3618e-02,  1.7608e-01],\n          ...,\n          [ 1.6023e-01,  2.2103e-01, -3.2907e-01,  ..., -1.1427e-01,\n            1.2678e-01,  1.8312e-02],\n          [-2.5502e-02,  4.3437e-02, -8.4752e-02,  ..., -1.3916e-01,\n            1.1505e-02, -9.1755e-02],\n          [ 9.1606e-02,  1.1718e-01, -2.1248e-01,  ...,  1.8931e-01,\n            1.1512e-01, -1.6510e-01]],\n\n         [[ 2.2793e-02,  2.3541e-02, -2.8061e-02,  ..., -1.2769e-01,\n           -7.2716e-03,  6.0188e-04],\n          [-3.0313e-01,  2.7208e-02, -1.7947e-01,  ...,  3.1877e-01,\n            8.2309e-03, -1.9664e-01],\n          [-1.6683e-01,  2.7815e-01,  1.8993e-02,  ...,  1.8573e-01,\n            4.8218e-02, -3.9488e-02],\n          ...,\n          [-2.5782e-02, -2.7187e-01,  9.3562e-02,  ...,  4.0017e-02,\n           -1.2506e-01, -2.9782e-02],\n          [ 1.4955e-01,  1.9041e-01,  2.7136e-01,  ...,  4.8256e-02,\n           -3.2006e-01,  1.2765e-01],\n          [-7.3341e-02, -8.2576e-02,  1.6007e-02,  ...,  4.9811e-02,\n           -5.3329e-02, -6.1567e-02]],\n\n         [[-2.4003e-02,  1.4413e-03, -9.7189e-03,  ..., -1.0038e-02,\n           -4.0050e-03, -7.8792e-05],\n          [ 4.3207e-02, -1.1919e-01, -5.4457e-01,  ..., -1.4651e-01,\n            1.0372e-01,  2.7977e-01],\n          [-1.0743e-01, -2.7909e-01, -5.8523e-01,  ..., -1.0493e-01,\n           -1.3021e-01, -2.6057e-01],\n          ...,\n          [ 2.4492e-01,  4.2667e-01, -2.2949e-01,  ...,  1.7911e-01,\n            1.6487e-01,  9.0538e-02],\n          [-1.0448e-01,  1.6701e-01, -4.7813e-01,  ..., -1.7286e-02,\n            1.5522e-01,  9.7537e-02],\n          [-1.7028e-02,  8.0929e-01,  9.4702e-01,  ..., -3.3726e-02,\n           -1.0608e-01,  4.0858e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.7858e-02,  1.7457e-01, -2.2763e-02,  ..., -1.1239e+00,\n            9.3052e-01,  9.3458e-01],\n          [ 4.1589e+00,  3.1935e+00, -2.4707e+00,  ..., -1.0910e+00,\n            2.0046e+00,  2.3571e+00],\n          [ 4.1812e+00,  7.1243e-01, -4.3240e+00,  ..., -5.7747e-01,\n            1.6257e+00,  1.6119e+00],\n          ...,\n          [-2.7209e+00, -4.7509e+00, -4.3103e+00,  ..., -2.4002e+00,\n            1.1299e-01,  1.4929e+00],\n          [-3.3909e+00, -3.7879e+00, -2.5160e+00,  ..., -1.4558e+00,\n            6.1843e-01,  2.3745e+00],\n          [-2.5503e+00, -2.7173e+00, -1.4323e+00,  ..., -1.3430e+00,\n           -6.9395e-01,  1.7852e+00]],\n\n         [[ 5.7449e-02, -3.3637e-03, -1.6319e-02,  ..., -4.2206e-01,\n            1.2233e+00,  3.4318e-01],\n          [ 3.2573e+00, -1.4257e+00, -1.1359e+00,  ..., -6.8138e-01,\n            1.1662e+00, -1.3537e-01],\n          [-4.8836e-01, -1.0289e+00, -1.0035e-03,  ..., -5.0336e-01,\n            4.7493e-01,  2.7646e-01],\n          ...,\n          [-1.5530e+00, -1.6113e+00, -3.6268e-01,  ..., -4.6333e-01,\n           -7.9367e-01,  8.5537e-01],\n          [ 9.4621e-01,  1.2660e+00, -6.1222e-01,  ...,  4.1511e-01,\n           -2.5061e-01,  1.0104e+00],\n          [ 2.2199e+00,  2.2013e+00, -1.1727e+00,  ...,  5.7435e-01,\n           -7.1837e-01,  1.9721e-01]],\n\n         [[ 8.8649e-05, -1.0856e-02,  6.7287e-03,  ..., -1.1209e+00,\n           -8.4819e-01,  1.1060e+00],\n          [ 3.0503e-02,  3.2406e-01,  3.5938e-01,  ...,  9.3989e+00,\n            4.5584e+00, -9.7403e+00],\n          [ 6.1180e-03, -4.0424e-02,  1.3981e-01,  ...,  8.2844e+00,\n            9.3161e-01, -9.1253e+00],\n          ...,\n          [-3.2833e-01, -3.9624e-03,  3.8422e-02,  ...,  1.5234e+00,\n            3.0882e+00, -8.6050e+00],\n          [-7.1255e-01, -1.5154e-01, -4.9607e-02,  ...,  5.0184e-01,\n            3.0396e+00, -6.9793e+00],\n          [-4.2383e-02, -2.2849e-01, -5.1579e-01,  ..., -1.3567e+00,\n            8.6881e+00, -4.9598e+00]],\n\n         ...,\n\n         [[ 4.7751e-03,  2.4765e-02,  6.5427e-02,  ...,  1.1498e+00,\n           -5.5660e-01, -2.3739e+00],\n          [-2.7048e-01,  1.9240e+00,  9.2797e-01,  ...,  6.1590e-01,\n           -5.3501e-02,  3.5105e+00],\n          [-1.4208e+00,  7.9953e-01,  7.2649e-01,  ...,  1.4090e+00,\n           -1.0943e-01,  4.6916e+00],\n          ...,\n          [ 1.4458e+00, -1.2390e+00, -1.6505e-01,  ...,  1.6354e+00,\n           -5.8472e-01,  4.2420e+00],\n          [ 1.2140e+00, -1.6530e+00, -7.6998e-01,  ...,  1.3969e+00,\n           -6.5632e-01,  4.7695e+00],\n          [ 4.2530e-01, -1.0632e+00, -1.5249e+00,  ...,  9.5883e-01,\n           -1.4171e+00,  4.5755e+00]],\n\n         [[ 4.9551e-03,  4.8713e-03, -6.7961e-02,  ..., -9.5603e-01,\n           -2.0773e+00, -1.4528e+00],\n          [-2.4987e+00,  4.8178e-01, -8.5864e-01,  ..., -1.1032e+00,\n           -4.5302e-01, -2.4382e+00],\n          [-1.3630e+00,  1.4044e+00, -7.8732e-01,  ..., -3.2960e-01,\n           -6.6378e-01, -1.6802e+00],\n          ...,\n          [ 1.0112e+00,  6.0710e-01,  3.2059e-01,  ..., -1.3903e-01,\n           -1.1601e+00, -1.1141e+00],\n          [ 5.1350e-01, -1.1856e+00, -1.4531e-01,  ...,  3.1771e-01,\n           -4.8929e-02, -3.4188e-01],\n          [ 5.9443e-01, -1.3259e+00,  2.0242e-02,  ..., -1.4919e+00,\n           -5.6953e-01, -7.6237e-01]],\n\n         [[-7.9350e-02, -5.4308e-02, -9.5349e-03,  ...,  9.4083e-01,\n            2.5170e+00, -3.2786e+00],\n          [-2.1497e+00, -1.7756e+00,  8.0995e-01,  ..., -3.3403e+00,\n           -1.8374e-01,  4.8019e+00],\n          [ 1.6129e+00,  5.4331e-01,  6.1357e-01,  ..., -3.7763e+00,\n            1.2750e+00,  7.3814e+00],\n          ...,\n          [ 9.7731e-01,  1.4124e+00,  3.8733e-01,  ..., -2.2185e+00,\n            1.4152e+00,  8.3236e+00],\n          [-1.5950e+00, -1.5879e-01,  4.6176e-01,  ..., -1.2807e+00,\n            1.1276e+00,  8.4439e+00],\n          [-7.3651e-01, -4.5895e-03, -7.2717e-02,  ..., -3.6994e+00,\n           -1.4185e+00,  6.0466e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 9.3590e-04,  2.8529e-02,  5.6978e-03,  ...,  1.2661e-02,\n            8.9264e-03, -8.7175e-03],\n          [ 3.0078e-01,  2.6767e-01,  1.9271e-01,  ...,  1.4708e-01,\n            7.1249e-02,  1.2822e-01],\n          [-8.4698e-02,  5.7607e-01, -3.0367e-01,  ...,  1.0784e-01,\n           -2.7994e-02,  4.0537e-01],\n          ...,\n          [ 1.2736e-01, -7.2686e-02,  3.0685e-01,  ..., -1.6118e-02,\n           -5.6757e-01, -3.7781e-01],\n          [ 1.8854e-01,  4.3236e-01,  2.4099e-01,  ..., -4.3005e-02,\n           -9.0187e-01,  1.1908e-01],\n          [ 1.0048e-01, -7.0765e-03,  4.2359e-02,  ..., -6.1907e-01,\n           -1.0317e-02,  2.9996e-01]],\n\n         [[ 4.2125e-02, -1.4949e-02,  6.8945e-03,  ...,  6.5633e-03,\n           -1.9918e-03, -1.2263e-02],\n          [-8.6123e-01,  2.1418e-02, -4.6317e-02,  ..., -4.7484e-01,\n           -2.6559e-01, -2.0109e-01],\n          [-3.3328e-01,  3.3497e-02, -1.6863e-01,  ..., -2.9473e-01,\n           -3.0380e-01,  8.4597e-02],\n          ...,\n          [-3.1169e-01,  1.6245e-01, -5.2212e-02,  ..., -2.4400e-01,\n           -3.6954e-01, -2.5663e-02],\n          [-9.2145e-02,  4.4051e-01,  4.6001e-02,  ..., -1.2436e-03,\n           -9.6444e-02, -4.9024e-01],\n          [ 2.0358e-01,  1.3464e-01, -2.1163e-01,  ..., -1.3686e-01,\n           -1.3435e-01,  1.6847e-01]],\n\n         [[ 4.8812e-03,  6.3428e-03, -1.5907e-02,  ..., -7.9164e-03,\n            1.9445e-03,  9.0020e-03],\n          [-9.8095e-02, -8.6381e-02, -3.6844e-01,  ..., -5.7589e-02,\n            2.1959e-01, -7.0982e-02],\n          [ 3.0170e-01, -1.0434e-01, -3.8194e-03,  ...,  6.2394e-02,\n            1.2737e-01, -1.1202e-01],\n          ...,\n          [ 2.2344e-01,  9.3603e-03, -2.2775e-01,  ..., -7.9702e-03,\n            2.8950e-02,  1.9388e-01],\n          [ 3.4498e-01,  8.0839e-02, -7.5499e-02,  ...,  5.3918e-02,\n           -1.1760e-01,  1.5815e-01],\n          [ 1.9497e-01, -1.1156e-01, -2.7446e-01,  ..., -6.2212e-02,\n            1.6979e-01,  5.9713e-02]],\n\n         ...,\n\n         [[-2.1932e-03,  6.7271e-03,  1.6275e-02,  ..., -1.2165e-02,\n            6.1163e-03, -3.1671e-02],\n          [ 9.7741e-02,  9.4036e-02,  2.4281e-02,  ...,  1.0980e-01,\n           -1.0405e-01,  1.0067e-01],\n          [-3.2719e-02,  5.3270e-02, -6.5072e-02,  ...,  1.3246e-01,\n           -4.2380e-01,  1.8715e-01],\n          ...,\n          [ 4.3528e-01,  1.0122e-01,  1.3358e-01,  ...,  4.0793e-01,\n           -2.2985e-01, -1.7101e-01],\n          [ 3.1636e-02,  1.8576e-02,  2.6358e-01,  ...,  3.2357e-01,\n           -3.8288e-01, -1.5802e-01],\n          [ 1.3856e-01, -7.9769e-02,  8.5927e-02,  ...,  4.6440e-01,\n           -3.4063e-02, -3.4477e-01]],\n\n         [[ 1.3362e-02,  9.6465e-03, -3.3372e-03,  ..., -2.1057e-02,\n           -1.5878e-02,  1.7561e-02],\n          [-1.7414e-01,  1.1742e-01,  1.4036e-01,  ...,  2.4521e-01,\n            9.3921e-02,  1.9606e-01],\n          [ 8.1271e-02,  4.4230e-01,  2.0403e-02,  ..., -1.2735e-01,\n           -1.8625e-01,  1.1084e-01],\n          ...,\n          [-2.4271e-01,  1.0439e-01, -1.1056e-01,  ...,  3.6153e-01,\n           -3.2894e-01,  2.8341e-02],\n          [ 5.9896e-02,  1.3758e-01, -2.8612e-01,  ...,  2.2576e-01,\n           -4.9372e-01,  5.5741e-02],\n          [-3.2227e-01,  4.0608e-01, -5.1128e-01,  ...,  4.2920e-01,\n           -1.1371e-01,  7.4510e-01]],\n\n         [[ 1.2613e-02,  4.4390e-03, -2.8977e-02,  ...,  1.5352e-02,\n           -3.1253e-03,  1.0260e-02],\n          [ 2.6141e-02,  4.4875e-01,  4.2975e-01,  ...,  3.2282e-01,\n           -2.1937e-01, -2.1450e-01],\n          [-1.0085e-01,  1.5843e-01,  9.9052e-01,  ...,  1.2787e-01,\n           -2.0459e-01, -5.0992e-01],\n          ...,\n          [-1.1244e-01,  6.0204e-02,  4.3369e-02,  ...,  1.0944e-01,\n            3.6751e-03, -9.8643e-02],\n          [-6.7898e-02, -1.1371e-01,  2.6787e-01,  ..., -1.7534e-01,\n            2.3881e-01, -4.7611e-02],\n          [-1.0737e-01, -1.8074e-01,  1.1761e-01,  ...,  5.3748e-03,\n           -3.4666e-01, -4.4969e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.2986e-02, -5.6758e-02,  4.2181e-02,  ...,  6.8194e-01,\n           -2.4365e-01, -2.6245e-01],\n          [-2.0277e+00,  3.6989e-01,  1.0358e+00,  ...,  1.5145e+00,\n           -2.0044e+00,  8.4235e-01],\n          [-1.3090e+00,  8.2125e-01,  5.5625e-01,  ..., -1.8567e-01,\n           -1.1091e+00,  5.4527e-01],\n          ...,\n          [ 1.5006e+00,  4.5542e-01,  4.3176e-01,  ...,  7.1436e-01,\n           -8.7396e-01, -4.8069e-01],\n          [ 1.0901e+00,  3.9606e-01, -4.7152e-01,  ...,  3.2983e-01,\n           -5.0065e-01,  5.0166e-01],\n          [ 1.2840e-01, -8.3067e-01, -8.1293e-01,  ..., -4.0872e-01,\n           -1.9316e+00, -1.9433e+00]],\n\n         [[-6.8734e-02,  8.0266e-02, -9.0677e-03,  ...,  7.3990e-01,\n            8.9014e-02, -5.7725e-01],\n          [-2.7317e+00,  2.1390e+00,  1.1494e+00,  ..., -3.4212e+00,\n           -3.1490e-01, -1.6709e+00],\n          [-2.9074e-01, -5.3336e-02, -5.1275e-01,  ..., -2.8581e+00,\n            4.7676e-02, -2.8024e-01],\n          ...,\n          [ 1.6968e+00, -1.9554e+00,  1.7006e+00,  ..., -4.3257e+00,\n           -2.2115e-01,  6.2416e-01],\n          [-9.2230e-01, -9.7474e-01,  3.9166e-01,  ..., -4.3167e+00,\n           -2.2765e-01, -4.8433e-01],\n          [-6.0461e-01, -1.0264e+00,  3.7290e-01,  ..., -3.7289e+00,\n           -6.2502e-02,  5.3949e-01]],\n\n         [[ 1.7127e-03, -3.6500e-02, -9.9987e-03,  ...,  3.8029e-01,\n            7.0632e-01, -9.5575e-02],\n          [-1.7413e-01, -2.3737e-01, -9.7911e-01,  ...,  1.6357e+00,\n           -9.5725e-02,  5.3678e-01],\n          [-8.8745e-01,  3.5649e-02, -1.8132e-01,  ...,  3.8635e-01,\n           -3.3454e-01,  3.4820e-01],\n          ...,\n          [ 4.0956e-01,  9.8702e-01, -1.6676e+00,  ..., -6.2349e-02,\n           -3.2785e-01,  3.2371e-01],\n          [ 1.3126e+00, -1.0274e-01, -5.2804e-01,  ...,  3.1959e-01,\n           -1.4227e-01,  1.1870e-01],\n          [ 7.6431e-01,  1.3596e-01, -2.6380e-01,  ...,  5.5026e-02,\n            4.8367e-02,  1.2360e+00]],\n\n         ...,\n\n         [[ 4.9501e-02, -3.4285e-02,  4.6124e-02,  ..., -3.8699e-01,\n            1.4646e+00, -1.7441e-01],\n          [ 1.2642e+00, -1.4507e+00,  1.6048e+00,  ...,  2.4330e+00,\n           -4.2889e+00, -5.5782e-01],\n          [-1.4796e-01,  3.3218e-01,  3.1144e-01,  ...,  2.9231e+00,\n           -3.6052e+00, -1.9999e+00],\n          ...,\n          [-1.2134e+00,  1.0892e+00, -3.7682e-01,  ...,  1.4461e+00,\n           -4.1793e+00, -2.2762e+00],\n          [ 7.4900e-01,  1.1938e-01, -4.1763e-01,  ...,  1.6771e+00,\n           -4.0144e+00, -2.6294e+00],\n          [ 4.3123e-01,  6.7885e-01, -1.1292e+00,  ...,  1.6564e+00,\n           -2.8780e+00, -1.3151e+00]],\n\n         [[-4.2510e-02,  3.7889e-02,  6.0747e-02,  ..., -5.7372e-02,\n           -1.2709e+00, -7.8387e-01],\n          [-5.0359e-01,  8.3469e-01,  1.2501e+00,  ...,  2.3276e-02,\n           -1.6576e+00, -4.7826e+00],\n          [ 3.5352e-01, -6.4913e-02,  5.3792e-01,  ..., -1.1122e-01,\n           -1.0168e+00, -5.8695e+00],\n          ...,\n          [ 5.3220e-02, -7.1725e-01,  4.9906e-01,  ..., -9.5418e-01,\n           -3.0370e+00, -2.7932e+00],\n          [-4.3403e-03, -3.6534e-01, -2.3779e-02,  ..., -1.4896e+00,\n           -2.6524e+00, -4.4556e+00],\n          [-3.7900e-01, -5.6974e-01, -3.5380e-02,  ..., -2.3949e+00,\n           -1.5284e+00, -4.1054e+00]],\n\n         [[ 5.9208e-02, -7.4041e-02, -3.8141e-02,  ...,  4.1372e-01,\n            1.9973e+00,  1.8169e+00],\n          [-3.5135e-01, -1.5177e+00, -1.0858e+00,  ...,  2.8560e-01,\n           -6.1455e-01, -2.1975e+00],\n          [-1.7711e+00, -5.5017e-01, -8.7693e-01,  ...,  1.6907e-01,\n           -5.6715e-02, -2.5102e+00],\n          ...,\n          [ 1.6425e-01,  2.4681e+00, -1.4838e+00,  ...,  4.7841e-01,\n           -1.6130e+00, -4.9014e+00],\n          [ 1.0952e+00,  1.1468e+00,  6.9954e-01,  ..., -7.0833e-01,\n           -1.6066e+00, -4.8652e+00],\n          [ 1.0568e+00,  6.1719e-01,  4.0996e-01,  ..., -2.6305e-02,\n           -3.2821e+00, -4.5679e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.5075e-03, -4.3818e-02,  3.2011e-02,  ...,  3.3098e-02,\n           -1.9107e-02, -3.6052e-02],\n          [-2.1269e-03,  1.6216e-01, -1.5261e-01,  ..., -2.4489e-01,\n           -1.2112e-01,  1.9746e-01],\n          [-2.4059e-01,  3.1051e-01,  5.7210e-02,  ..., -2.6637e-01,\n           -6.1936e-02,  2.2837e-01],\n          ...,\n          [-1.1204e-01,  2.9352e-01,  2.7793e-01,  ..., -1.0572e-01,\n            2.2897e-02,  5.1392e-01],\n          [-1.3869e-01,  1.0130e-01, -6.4869e-02,  ...,  4.4783e-02,\n            4.8176e-03,  2.2737e-01],\n          [-4.1347e-03,  2.8174e-02, -1.6069e-01,  ...,  2.7191e-01,\n            2.8911e-04,  4.5144e-01]],\n\n         [[-1.2707e-02,  7.7928e-03, -2.4912e-02,  ...,  4.3719e-04,\n            1.3313e-02,  3.7711e-03],\n          [ 1.4313e-01, -3.8084e-01,  4.3599e-01,  ..., -1.1904e-01,\n            2.5198e-01,  2.1550e-01],\n          [-5.0879e-03, -3.7911e-02,  2.5485e-01,  ..., -2.7624e-01,\n           -1.2211e-01,  2.4520e-01],\n          ...,\n          [-1.3643e-01, -3.5685e-01,  4.4867e-01,  ...,  1.2322e-01,\n            6.0183e-01,  2.4804e-01],\n          [-2.0965e-01, -3.1830e-01,  3.1960e-01,  ...,  3.6249e-02,\n            3.7764e-01,  6.0210e-03],\n          [-8.3376e-02, -2.1156e-01,  1.5272e-01,  ..., -6.9034e-02,\n            2.5154e-01, -1.3800e-01]],\n\n         [[ 4.9766e-03,  1.6422e-02, -9.3895e-04,  ..., -1.0992e-02,\n            2.7222e-02, -3.3118e-03],\n          [ 7.0952e-02,  1.4994e-01, -1.4358e-01,  ...,  1.3321e-01,\n           -9.9199e-01, -5.0039e-02],\n          [ 2.2315e-03,  1.3689e-02,  5.4795e-01,  ...,  4.3925e-01,\n           -3.8316e-01,  1.2275e-01],\n          ...,\n          [ 1.4568e-01,  3.6804e-01,  1.8879e-01,  ...,  6.3231e-01,\n           -6.8411e-01,  8.1384e-02],\n          [ 1.6347e-01,  5.0384e-02,  1.6249e-01,  ...,  2.5391e-01,\n            4.1089e-02,  8.6733e-02],\n          [ 1.0754e-01,  2.0364e-03,  3.1858e-02,  ...,  1.2100e-01,\n           -6.8371e-01, -2.3201e-02]],\n\n         ...,\n\n         [[-6.8232e-03, -3.4541e-03,  3.0257e-03,  ..., -1.1610e-02,\n            1.4196e-02, -4.2651e-03],\n          [-1.9102e-01, -9.6374e-02, -3.8347e-01,  ..., -6.8594e-02,\n            1.5969e-01, -2.9198e-01],\n          [-1.1796e-01, -3.7198e-01, -1.7730e-01,  ...,  3.0197e-01,\n            4.4423e-01, -3.5287e-01],\n          ...,\n          [ 2.5382e-01,  1.8584e-02, -1.4926e-01,  ...,  1.1287e-01,\n            1.5608e-01,  1.8151e-01],\n          [ 5.1760e-02,  1.7294e-02, -1.8609e-01,  ..., -1.3589e-01,\n            6.0654e-02, -8.9075e-02],\n          [-3.4706e-01, -2.0907e-01, -1.7131e-01,  ..., -2.4358e-01,\n           -1.2957e-01, -2.4586e-01]],\n\n         [[ 1.8800e-02, -2.2288e-02, -1.8583e-02,  ...,  2.8067e-02,\n            2.0518e-02,  7.2470e-03],\n          [-2.2676e-01,  2.8546e-01,  1.0846e-01,  ...,  7.3832e-02,\n           -2.8026e-02,  2.3904e-06],\n          [-2.1163e-02,  7.5956e-01,  1.8756e-01,  ...,  3.1446e-01,\n            5.6013e-01, -2.0784e-01],\n          ...,\n          [ 1.9506e-01,  1.2693e-01, -2.0888e-01,  ...,  1.7235e-01,\n            6.0297e-02, -8.2514e-02],\n          [ 4.7009e-01,  4.0924e-01, -2.4973e-01,  ..., -1.2039e-01,\n            1.1012e-01,  5.1201e-02],\n          [ 2.7584e-01,  3.3358e-01, -1.8232e-01,  ...,  2.0032e-01,\n            2.2118e-01,  5.0746e-02]],\n\n         [[ 1.1655e-02, -1.1191e-02, -2.4126e-02,  ...,  1.7493e-02,\n           -1.3610e-02, -2.2237e-02],\n          [-1.6543e-01, -2.5003e-01,  8.6125e-02,  ...,  5.6394e-02,\n            3.4471e-02,  9.6381e-02],\n          [-1.6387e-01, -1.1931e-01,  5.5219e-01,  ...,  5.0510e-01,\n           -1.5565e-01,  8.6988e-02],\n          ...,\n          [-3.0309e-01, -2.2586e-02,  1.5457e-01,  ...,  1.6099e-01,\n           -5.5603e-01, -2.0773e-01],\n          [-4.4897e-01, -2.6678e-02, -3.1037e-01,  ...,  1.3030e-01,\n           -9.0136e-01,  5.7729e-02],\n          [ 2.0204e-01,  3.6599e-02,  1.9856e-01,  ...,  7.2303e-02,\n           -2.2883e-01, -1.2853e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4815e-02,  3.8855e-03, -6.5161e-02,  ...,  3.6477e-01,\n           -7.6282e-01,  3.9310e-01],\n          [ 1.7472e-01,  8.0178e-01, -5.7563e-01,  ..., -1.6855e+00,\n           -3.8904e+00, -1.9497e+00],\n          [-9.5987e-01,  5.8723e-01, -7.1325e-01,  ..., -1.8820e+00,\n           -1.9519e+00, -3.5142e+00],\n          ...,\n          [ 2.9018e-01, -2.2332e-01, -2.2718e-01,  ..., -1.3363e+00,\n           -1.4313e+00, -2.5188e+00],\n          [ 3.7244e-01, -1.9810e-01, -1.9881e-01,  ..., -6.5588e-01,\n           -1.1756e+00, -4.1595e+00],\n          [ 6.5614e-01, -1.0736e+00,  8.7181e-01,  ..., -9.0678e-01,\n           -7.1075e-01, -3.9002e+00]],\n\n         [[-5.2089e-03,  1.4205e-02, -1.2689e-03,  ..., -5.0026e-01,\n           -5.8205e-01,  5.3615e-01],\n          [-1.5271e+00,  8.8098e-01, -5.7725e-02,  ...,  4.1800e+00,\n           -1.7192e+00,  1.1539e+00],\n          [ 3.9878e-01, -2.5377e-01, -2.3752e-01,  ...,  4.6732e+00,\n           -8.8458e-01, -5.0242e-01],\n          ...,\n          [ 7.0503e-01, -1.1254e+00,  2.2898e-01,  ...,  3.6139e+00,\n           -7.5675e-02,  9.0863e-01],\n          [-3.5906e-01, -1.2928e-01,  1.5099e-01,  ...,  2.8251e+00,\n           -3.7178e-01, -2.4456e+00],\n          [-7.6586e-01, -1.8641e-01,  8.9007e-01,  ...,  7.6406e-01,\n           -8.1659e-01, -3.2165e-01]],\n\n         [[-4.6029e-02, -7.7140e-02,  7.5468e-02,  ..., -7.1893e-02,\n            1.2510e+00, -1.8972e-01],\n          [-1.7925e+00, -1.6732e+00,  2.4657e+00,  ..., -3.3280e-01,\n            1.4177e+00, -1.9065e-01],\n          [-1.3771e-01, -1.2481e-01,  1.1394e+00,  ...,  2.8269e-03,\n            1.3836e+00,  4.2271e-01],\n          ...,\n          [ 1.1215e+00,  1.7611e+00, -2.1272e+00,  ...,  4.1990e-02,\n            1.2248e+00,  4.4655e-01],\n          [-4.5170e-03,  1.8393e+00, -5.3411e-01,  ..., -2.2830e-05,\n            5.9196e-01,  8.4567e-01],\n          [-1.4081e+00,  2.5823e-01, -2.0935e+00,  ...,  8.9313e-02,\n           -9.1095e-03, -7.3424e-01]],\n\n         ...,\n\n         [[ 3.8313e-02,  3.1387e-02, -4.9776e-02,  ...,  1.4955e+00,\n            1.4901e+00,  3.7326e-01],\n          [ 4.7177e-01, -1.9497e-01, -9.5550e-01,  ..., -1.4786e+00,\n           -5.3615e+00, -4.5147e+00],\n          [-2.1454e-01, -3.4196e-01,  1.6850e-01,  ..., -2.6179e+00,\n           -6.5332e+00, -5.7047e+00],\n          ...,\n          [-5.5921e-01,  7.6688e-01,  5.1687e-01,  ..., -2.3091e+00,\n           -6.1471e+00, -5.2471e+00],\n          [ 5.3142e-01, -1.4104e-02,  2.5696e-01,  ..., -2.2951e+00,\n           -6.7571e+00, -5.0329e+00],\n          [ 6.1794e-01, -4.1871e-01,  8.8736e-01,  ..., -9.6804e-01,\n           -6.1409e+00, -5.1368e+00]],\n\n         [[ 1.6163e-01,  7.1201e-02,  7.7224e-02,  ..., -9.5703e-01,\n            5.5020e-01,  6.5097e-01],\n          [ 6.2034e-01,  1.2192e+00,  2.8485e+00,  ..., -4.1860e+00,\n            2.0176e+00, -1.7881e+00],\n          [-3.9736e+00, -1.6081e+00,  2.0517e+00,  ..., -4.9265e+00,\n            2.1788e+00,  1.6002e-01],\n          ...,\n          [-1.7394e+00, -3.7851e+00, -5.4124e-01,  ..., -4.0162e+00,\n            2.0071e+00, -5.5163e-01],\n          [ 3.5902e+00, -2.1066e+00, -1.7955e+00,  ..., -2.4751e+00,\n            3.4249e+00, -7.4834e-03],\n          [ 5.7654e+00, -5.9770e-01, -2.7081e+00,  ..., -4.5666e+00,\n            3.4483e+00, -4.5146e-01]],\n\n         [[ 9.2309e-03, -1.3726e-02, -6.7525e-02,  ...,  5.8759e-01,\n            1.1254e+00,  8.7347e-01],\n          [ 6.7081e-01,  1.3083e+00, -2.8793e-01,  ..., -1.8503e+00,\n            1.0474e+00, -1.1659e+00],\n          [-1.9691e+00,  6.1067e-02, -4.9139e-02,  ..., -2.0803e+00,\n            1.0640e+00, -1.6873e+00],\n          ...,\n          [-1.9309e-01, -8.1366e-01, -1.0745e+00,  ..., -1.1412e+00,\n            1.2140e+00, -2.7790e+00],\n          [ 1.2697e+00, -4.0457e-01,  5.4169e-01,  ..., -2.0708e+00,\n           -3.0930e-01, -3.3942e+00],\n          [ 1.1935e+00, -3.6645e-01,  1.2198e+00,  ..., -2.5969e+00,\n            3.4379e-02, -4.6407e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3331e-02, -4.6928e-03, -7.3019e-03,  ..., -1.9747e-02,\n           -3.0523e-03,  2.9128e-02],\n          [ 1.2601e-01, -5.6824e-02,  1.2379e-01,  ...,  1.2374e-01,\n           -3.3303e-02, -3.7515e-01],\n          [ 1.9364e-01, -1.2125e-01,  2.8124e-01,  ..., -1.0768e-01,\n            6.3876e-02, -6.0966e-01],\n          ...,\n          [-4.2872e-02, -1.5920e-01,  3.1075e-01,  ...,  7.3740e-02,\n           -1.0305e-01, -3.5542e-01],\n          [ 9.1024e-02, -1.0297e-01,  4.4406e-01,  ...,  3.4327e-01,\n           -1.5741e-01, -6.0071e-01],\n          [ 1.5528e-01, -2.7940e-01,  3.7453e-01,  ..., -7.5135e-02,\n           -3.9484e-02, -1.8948e-01]],\n\n         [[-4.6705e-03,  2.1753e-03,  2.7855e-02,  ..., -1.3684e-03,\n            6.2462e-02,  2.0781e-03],\n          [-1.2567e-03, -1.0440e-01, -1.3493e-02,  ..., -1.8711e-01,\n           -9.1693e-02,  6.7549e-02],\n          [ 2.9302e-01,  2.4074e-01, -4.7760e-02,  ..., -3.8108e-01,\n            2.2722e-02,  2.2241e-01],\n          ...,\n          [ 6.7676e-02,  3.6794e-01,  2.8198e-01,  ..., -1.3390e-01,\n            5.1727e-01,  2.9591e-01],\n          [ 5.2915e-02,  3.9826e-01,  1.7824e-01,  ..., -8.1494e-03,\n            1.1654e-01,  4.1005e-01],\n          [ 6.6410e-02,  2.2054e-01,  2.0727e-01,  ...,  6.0972e-02,\n            2.6168e-01,  1.0454e-01]],\n\n         [[-6.5995e-03, -3.0940e-02,  3.0484e-02,  ...,  5.9859e-04,\n            1.1084e-02, -1.7138e-01],\n          [ 1.1578e-01,  2.1733e-03, -2.2893e-01,  ..., -1.7371e-01,\n           -2.2726e-03, -5.9387e-02],\n          [ 2.0877e-01,  2.2600e-01, -1.2645e-01,  ..., -3.3830e-01,\n            6.7676e-02,  1.6034e-01],\n          ...,\n          [ 2.2702e-01,  3.6602e-02, -2.1516e-01,  ..., -9.3808e-01,\n            2.2783e-01, -1.7914e-01],\n          [ 4.6318e-01,  1.4795e-02,  1.8363e-02,  ..., -5.6283e-01,\n            1.2641e-01,  4.9621e-01],\n          [-1.6959e-01, -2.2225e-03,  2.8095e-01,  ..., -4.9871e-01,\n            1.7634e-01,  7.5611e-01]],\n\n         ...,\n\n         [[ 4.8023e-03,  4.0914e-01, -1.4497e-02,  ...,  7.0290e-03,\n            1.2445e-02, -2.1458e-02],\n          [ 1.3051e-02, -9.3685e-01,  1.1798e-01,  ..., -1.4003e-01,\n           -1.2717e-01,  8.9570e-02],\n          [-1.6168e-01, -1.6029e+00,  2.3556e-02,  ...,  4.6295e-02,\n            6.0343e-02,  1.0125e-01],\n          ...,\n          [-1.6702e-01, -1.0712e+00,  2.1793e-01,  ...,  1.0436e-01,\n           -4.7951e-02, -1.9403e-01],\n          [-1.1241e-01, -1.2524e+00,  2.6625e-01,  ..., -2.1663e-02,\n           -7.1329e-02, -2.4839e-01],\n          [-1.3651e-01, -6.7578e-01, -2.5005e-02,  ...,  6.8521e-02,\n           -2.7406e-01, -2.5056e-01]],\n\n         [[-1.6400e-02, -9.9874e-03, -1.3735e-02,  ..., -2.2474e-02,\n            2.0281e-02,  2.2781e-01],\n          [ 4.7074e-01, -4.9741e-01, -3.5659e-01,  ...,  9.1268e-02,\n            1.1694e-01, -2.1805e-01],\n          [ 2.0684e-01,  4.0959e-01, -5.0544e-02,  ...,  9.8892e-02,\n            2.3886e-01,  1.7650e-01],\n          ...,\n          [ 2.1865e-01,  2.6039e-01, -1.8954e-02,  ..., -1.6628e-01,\n           -2.1735e-02, -5.7966e-01],\n          [ 1.3507e-01,  1.4209e-01,  4.9038e-02,  ...,  1.2472e-01,\n           -8.2507e-02,  2.5974e-01],\n          [ 2.4348e-01, -3.2005e-02, -2.9278e-01,  ..., -3.8628e-02,\n            1.9323e-01, -4.1019e-01]],\n\n         [[-5.9746e-03,  1.0048e-02, -4.2604e-03,  ..., -1.0930e-01,\n            3.4326e-02,  8.4342e-02],\n          [ 1.0278e-01, -3.1082e-02,  5.8721e-02,  ...,  6.1986e-01,\n           -2.0531e-01, -6.2644e-01],\n          [ 1.1943e-01, -5.8391e-02,  5.6865e-02,  ...,  7.2164e-01,\n           -1.2859e-01, -6.6454e-01],\n          ...,\n          [-3.3965e-01, -1.6304e-01,  5.2154e-01,  ...,  7.4580e-01,\n           -5.2450e-02, -6.4519e-01],\n          [-2.0817e-01,  2.0681e-01,  3.1963e-01,  ...,  7.1918e-01,\n           -3.6935e-02, -6.8810e-01],\n          [-3.5905e-01,  3.3189e-01,  2.7067e-01,  ...,  2.4084e-01,\n           -5.4943e-02, -6.5760e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7947e-02,  7.9241e-03,  5.4580e-02,  ...,  7.3166e-01,\n            7.5432e-01, -5.3334e-01],\n          [-1.4227e+00, -3.1328e-01,  7.6773e-01,  ..., -1.4749e+00,\n            1.3679e+00,  1.1102e+00],\n          [ 2.1317e-01, -3.5648e-01, -4.1964e-01,  ...,  1.3892e-01,\n            8.3991e-01, -9.4518e-01],\n          ...,\n          [ 2.0481e-01,  6.3052e-01,  5.3492e-01,  ...,  1.6286e+00,\n            7.7440e-01, -2.1364e+00],\n          [-9.4441e-01,  1.3181e+00, -9.8881e-01,  ...,  9.1665e-01,\n            8.6446e-01, -2.0505e+00],\n          [-9.4097e-01,  1.0915e+00, -3.3495e-02,  ..., -2.5536e-01,\n            1.0748e+00, -1.3074e+00]],\n\n         [[-2.9458e-02,  1.3561e-02, -1.8575e-02,  ..., -1.3401e+00,\n           -1.5849e+00,  8.7977e-01],\n          [-2.5556e+00,  6.0933e-01, -1.5862e+00,  ..., -2.3096e+00,\n           -2.1301e+00,  1.4235e-01],\n          [-1.0071e+00,  4.9559e-02, -6.3062e-01,  ..., -3.2193e+00,\n           -1.8662e+00, -1.0737e-02],\n          ...,\n          [ 6.9421e-01, -4.1986e-01, -9.7129e-01,  ..., -2.5432e+00,\n           -1.3412e+00,  3.5665e-01],\n          [ 1.0528e+00, -1.8289e+00, -3.9715e-01,  ..., -2.5711e+00,\n           -1.0718e+00, -2.1864e-01],\n          [ 1.1092e+00, -1.4605e+00, -1.4248e+00,  ..., -1.9361e+00,\n            7.9981e-01,  4.2452e-01]],\n\n         [[ 4.9120e-02,  7.5759e-02,  2.0578e-02,  ...,  5.4245e-01,\n            3.2780e-01, -1.0823e+00],\n          [ 1.9434e+00,  1.4119e+00,  8.9912e-01,  ..., -2.4708e+00,\n           -5.8531e-01,  1.9458e+00],\n          [ 1.3223e+00,  6.1247e-01,  1.3897e+00,  ..., -2.6598e+00,\n           -2.1375e+00,  9.3404e-01],\n          ...,\n          [-1.0583e+00, -1.9213e+00,  1.4193e+00,  ..., -1.9858e+00,\n           -2.4528e+00,  1.8812e+00],\n          [-1.6972e+00, -1.7027e+00,  1.3032e-01,  ..., -1.7238e+00,\n           -3.0209e+00,  1.1120e+00],\n          [ 6.4561e-02, -9.5799e-01,  4.1644e-01,  ..., -2.1372e+00,\n           -2.3745e+00,  6.9682e-01]],\n\n         ...,\n\n         [[ 3.2195e-02,  5.6718e-02,  3.6552e-02,  ..., -1.1915e+00,\n            2.4083e+00, -8.5296e-01],\n          [ 1.9427e+00,  8.3545e-01,  1.8341e+00,  ..., -2.2661e+00,\n           -6.1698e+00, -2.3955e+00],\n          [ 1.3729e+00, -1.2718e+00,  9.0652e-01,  ..., -2.7209e+00,\n           -4.1600e+00, -2.1662e+00],\n          ...,\n          [-1.8673e+00, -1.3913e+00, -1.8973e+00,  ..., -2.7562e+00,\n           -3.9062e+00, -3.6888e+00],\n          [-1.5251e+00, -1.6881e+00, -7.6363e-01,  ..., -1.6459e+00,\n           -3.8735e+00, -1.6431e+00],\n          [-1.5214e-01, -6.2781e-01, -2.1714e+00,  ..., -1.4013e+00,\n           -4.1542e+00, -2.9377e+00]],\n\n         [[-3.5472e-02, -2.0856e-02,  8.1324e-03,  ..., -5.8972e-02,\n           -1.6879e+00, -5.6755e-01],\n          [-9.1058e-01, -1.8759e+00,  1.3397e+00,  ..., -6.5463e-01,\n            2.4758e+00,  1.8893e+00],\n          [ 7.3099e-01, -1.5181e+00,  3.8525e-01,  ...,  9.1481e-02,\n            2.7941e+00,  7.3755e-01],\n          ...,\n          [ 5.8331e-01, -2.1469e-01,  6.6975e-01,  ...,  9.8887e-03,\n            3.0421e+00,  9.7136e-01],\n          [-9.5213e-01,  1.7801e+00, -4.6554e-01,  ..., -1.5791e+00,\n            3.7799e+00,  1.1569e+00],\n          [-2.1433e+00,  2.5012e+00, -1.9069e-01,  ..., -8.2747e-01,\n            3.1538e+00,  4.1107e-01]],\n\n         [[ 6.5458e-03,  4.6341e-03,  1.3568e-01,  ...,  6.9160e-01,\n            6.4990e-01,  5.2006e-01],\n          [ 4.8618e+00,  4.0616e+00,  3.7445e+00,  ...,  1.9428e+00,\n            1.0039e+00,  1.3118e+00],\n          [ 3.8549e+00,  1.3905e+00,  2.3375e+00,  ...,  7.9430e-01,\n           -8.6311e-01,  2.3621e+00],\n          ...,\n          [-3.1003e+00,  8.0585e-01, -1.8532e+00,  ..., -3.3204e-01,\n           -1.2169e+00,  1.0019e+00],\n          [-3.9758e+00,  5.6328e-01, -3.1805e+00,  ..., -5.5188e-01,\n           -1.2724e+00,  3.1984e+00],\n          [-6.5815e-01, -5.3763e-01, -4.4679e+00,  ...,  1.2719e+00,\n           -1.3666e+00,  7.4437e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.1082e-04,  3.5931e-03,  1.1365e-02,  ...,  2.7242e-02,\n           -1.2164e-02,  4.3938e-03],\n          [ 7.1937e-01, -3.0655e-01, -7.0021e-02,  ...,  7.0112e-02,\n            6.7781e-02, -5.4168e-01],\n          [ 7.3877e-01,  2.1703e-02, -1.0473e-01,  ..., -2.1154e-02,\n            2.0039e-01, -5.4422e-01],\n          ...,\n          [ 6.1225e-01, -1.2619e-01, -3.8412e-01,  ..., -1.9074e-01,\n            3.5200e-01, -5.5218e-01],\n          [ 5.8258e-01,  1.7563e-02, -1.8405e-01,  ..., -3.2325e-02,\n            1.4457e-01, -4.9312e-01],\n          [ 6.1921e-01, -1.5764e-01, -2.0414e-01,  ...,  8.6097e-02,\n           -6.4259e-02, -8.1969e-01]],\n\n         [[-1.2056e-02, -5.5574e-03,  3.8054e-02,  ..., -1.7010e-01,\n            3.2242e-02, -1.3090e-02],\n          [-1.2494e-01,  1.3325e-01,  6.3530e-02,  ...,  3.1447e-02,\n            9.3956e-02,  1.3487e-01],\n          [-2.0624e-01,  2.0004e-01,  1.4182e-01,  ..., -3.0684e-02,\n           -2.0348e-01,  2.2170e-01],\n          ...,\n          [ 8.2619e-02, -4.3028e-02,  5.5799e-02,  ..., -3.4660e-01,\n           -3.2581e-01,  5.0147e-01],\n          [ 8.1035e-02,  2.4790e-01,  1.1859e-01,  ..., -1.9436e-02,\n           -1.4303e-01,  1.4264e-01],\n          [-1.4075e-01,  2.3101e-01, -2.6854e-01,  ...,  1.1051e-01,\n           -3.3257e-01,  5.5553e-02]],\n\n         [[ 3.2441e-02,  7.3343e-03, -7.6267e-03,  ..., -6.4834e-03,\n           -1.7868e-02, -1.3072e-02],\n          [ 3.0421e-01,  8.5115e-02, -7.7033e-02,  ..., -1.5478e-01,\n            1.5215e-01, -6.6995e-02],\n          [ 3.4015e-01,  3.5630e-02, -1.5668e-02,  ..., -1.7520e-01,\n            2.5132e-01,  1.6824e-01],\n          ...,\n          [ 1.7575e-01, -3.0521e-01,  4.1171e-01,  ...,  7.2286e-02,\n           -2.5222e-01, -1.4530e-01],\n          [ 1.4619e-01,  1.0188e-01, -2.4380e-01,  ..., -1.6767e-01,\n           -3.9960e-01,  1.6171e-01],\n          [-2.3076e-01,  2.3107e-02,  4.7495e-03,  ...,  7.6523e-02,\n            3.8540e-02, -1.1297e-01]],\n\n         ...,\n\n         [[-7.2353e-03,  1.2753e-02,  5.2595e-03,  ..., -1.2895e-02,\n           -5.8279e-03, -7.6916e-03],\n          [ 2.4093e-01, -6.4406e-02,  1.2542e-01,  ...,  1.0988e-01,\n            3.0498e-01,  1.3181e-01],\n          [-1.0176e-01,  1.6784e-02, -1.7389e-01,  ..., -1.2551e-01,\n            1.2583e-01, -1.7929e-02],\n          ...,\n          [-5.6606e-02,  4.1440e-02, -2.0578e-01,  ...,  3.2717e-01,\n            1.3850e-03, -8.0089e-02],\n          [ 1.3123e-01, -6.5005e-02, -1.2321e-02,  ...,  1.7689e-02,\n            1.9605e-01, -9.2188e-02],\n          [-3.0912e-02, -2.2582e-01,  2.3516e-01,  ...,  1.8610e-01,\n            3.1048e-01, -4.0471e-02]],\n\n         [[ 2.8551e-03,  4.7691e-02, -3.8349e-02,  ...,  4.4290e-02,\n           -2.6177e-04, -3.4950e-01],\n          [ 4.8099e-01, -3.4549e-01,  1.2902e-01,  ...,  7.0070e-01,\n            3.1902e-01,  1.7167e-01],\n          [ 2.5321e-01, -3.6839e-02, -1.0152e-01,  ...,  3.4972e-01,\n            2.1394e-01,  1.3799e-01],\n          ...,\n          [ 1.2337e-01,  1.2475e-01, -2.7210e-01,  ...,  3.3952e-01,\n           -2.4348e-01, -2.5896e-01],\n          [ 1.2148e-01, -5.1883e-02, -2.5086e-01,  ...,  5.8157e-01,\n            3.1913e-03, -1.3361e-02],\n          [ 5.2966e-01, -4.1191e-02, -3.2714e-01,  ...,  2.6910e-01,\n           -2.4594e-01,  1.3633e-01]],\n\n         [[-8.1304e-03,  1.4941e-02, -1.0689e-02,  ...,  7.0810e-03,\n            5.8582e-03, -1.3757e-02],\n          [-2.2038e-01,  2.3056e-01,  2.9953e-02,  ..., -4.9608e-02,\n           -1.3103e-01, -1.4165e-01],\n          [-5.8053e-01,  9.6231e-03,  2.1723e-01,  ...,  2.3857e-01,\n            1.9165e-01,  2.1279e-02],\n          ...,\n          [-2.3823e-02,  1.5250e-01, -4.2803e-01,  ..., -1.1787e-01,\n            4.9306e-01, -3.7298e-01],\n          [-1.1632e-03,  2.6019e-03, -1.5353e-01,  ..., -2.3680e-01,\n            2.4624e-01, -7.7515e-02],\n          [-3.3636e-01, -1.6598e-01,  2.2549e-01,  ..., -7.8078e-02,\n            2.5051e-01,  2.2947e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2312e-01,  8.0175e-04,  6.5801e-02,  ...,  3.4184e-01,\n            7.8137e-01, -1.7893e-01],\n          [-2.0273e+00,  2.5088e-01,  2.2136e+00,  ...,  2.1602e+00,\n           -3.6832e+00,  5.5925e-02],\n          [ 5.8843e-01, -2.2986e+00,  2.9580e+00,  ...,  2.8248e+00,\n           -4.8020e+00, -7.1777e-01],\n          ...,\n          [ 3.2741e+00, -1.0181e+00,  8.7457e-01,  ...,  2.4720e+00,\n           -6.6660e+00, -1.2930e+00],\n          [ 3.3239e-01,  1.3234e+00,  1.0918e+00,  ...,  3.1143e+00,\n           -6.8487e+00, -1.3001e+00],\n          [-1.6789e+00,  1.6649e+00,  6.4973e-01,  ...,  4.9136e+00,\n           -5.9759e+00, -1.3086e+00]],\n\n         [[-4.0276e-02, -6.4417e-02, -5.3369e-02,  ...,  4.9013e-01,\n            9.7800e-02, -3.2408e-01],\n          [-2.2293e+00, -1.7687e+00, -1.3600e+00,  ..., -1.2043e+00,\n           -3.0898e-01, -8.6754e-01],\n          [-1.8947e+00, -4.6684e-01, -4.1457e-01,  ..., -2.0658e+00,\n           -5.5696e-02,  2.4413e-01],\n          ...,\n          [ 7.9429e-01,  2.2565e+00,  9.3412e-01,  ..., -8.6943e-01,\n           -6.8542e-01, -3.4692e-01],\n          [ 1.1933e+00,  7.4079e-01,  1.6663e+00,  ..., -9.9109e-01,\n           -1.0008e+00,  5.6319e-01],\n          [ 5.9951e-01,  1.5651e+00,  2.5191e+00,  ..., -3.8812e-02,\n           -6.5149e-01, -1.8521e-02]],\n\n         [[ 3.3561e-02,  8.4039e-02, -2.8396e-02,  ..., -4.7250e-01,\n            1.9230e+00, -2.1323e-01],\n          [ 5.7200e-01,  2.2279e+00,  1.5472e-01,  ..., -6.2765e-01,\n            8.7105e-01, -2.3120e-01],\n          [-1.0779e+00,  2.0015e-01,  1.5254e+00,  ...,  1.1903e+00,\n           -1.7934e-01,  2.8011e-02],\n          ...,\n          [-5.1735e-01, -1.3757e+00,  1.9088e+00,  ...,  4.7389e+00,\n           -1.3282e+00, -6.7751e-01],\n          [ 8.7289e-01, -1.1860e+00,  1.6457e+00,  ...,  1.5667e+00,\n           -1.8226e+00,  1.0986e-01],\n          [ 1.0428e+00, -1.9213e+00,  2.9362e-01,  ..., -5.2215e-01,\n           -1.1983e+00,  2.4624e-01]],\n\n         ...,\n\n         [[ 3.5742e-03, -1.2710e-02, -1.4841e-02,  ...,  9.3606e-03,\n           -1.9389e-01,  7.3719e-02],\n          [-4.5218e-01,  6.1590e-02,  2.1453e-01,  ...,  1.5978e+00,\n            4.0628e+00, -3.0448e-01],\n          [-2.9151e-03,  2.6232e-01,  3.1400e-01,  ...,  4.0447e+00,\n            5.4559e+00, -3.1808e+00],\n          ...,\n          [ 4.8316e-01,  1.1920e-02,  8.1735e-01,  ...,  4.6892e+00,\n            4.6682e+00, -1.2101e+00],\n          [ 6.1053e-01, -5.4636e-01,  1.2391e+00,  ...,  4.1324e+00,\n            6.1381e+00, -2.8616e+00],\n          [ 7.9651e-01,  2.1781e-01,  8.0535e-01,  ...,  4.9993e+00,\n            4.2333e+00, -2.6715e+00]],\n\n         [[ 3.2944e-02, -4.3002e-03,  7.3976e-03,  ..., -7.6260e-01,\n            7.9404e-01, -7.5428e-01],\n          [-5.1165e-01,  3.3352e-01,  1.6246e-01,  ...,  1.1631e+00,\n            1.7283e+00,  3.4625e-01],\n          [ 1.5882e-01, -2.4296e-02, -2.3312e-01,  ...,  1.3329e+00,\n            8.8855e-01, -4.3772e-02],\n          ...,\n          [ 3.1424e-01, -2.7989e-01, -6.4704e-01,  ...,  2.0611e+00,\n            8.6480e-01,  1.1318e+00],\n          [ 1.9290e-02,  2.0101e-01, -5.4484e-01,  ...,  1.2569e+00,\n            5.4770e-01,  2.0927e+00],\n          [ 1.4652e-01,  7.5938e-01, -9.8990e-02,  ...,  1.3441e+00,\n            1.2334e+00,  3.1849e+00]],\n\n         [[-2.4856e-02,  5.0377e-02, -2.0058e-02,  ..., -7.2910e-02,\n            1.7153e+00,  6.1020e-01],\n          [ 1.2633e+00,  1.7550e+00, -1.0946e+00,  ..., -4.0205e-01,\n           -3.2863e+00,  1.8942e+00],\n          [ 1.4678e+00,  9.7763e-01, -9.2193e-01,  ..., -1.7589e-01,\n           -3.5836e+00,  3.3629e+00],\n          ...,\n          [-4.3405e-01,  4.0004e-01, -1.2932e+00,  ..., -8.0985e-01,\n           -2.9375e+00,  3.4383e+00],\n          [-1.2732e+00,  6.0986e-02, -6.0903e-01,  ..., -6.1697e-01,\n           -3.2768e+00,  3.0371e+00],\n          [-6.4617e-01, -2.6350e-01, -7.1758e-01,  ..., -1.4704e+00,\n           -2.5376e+00,  4.9321e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.8676e-03,  1.1584e-03, -2.5280e-03,  ...,  1.0427e-03,\n           -1.0717e-02, -7.4770e-03],\n          [-7.7805e-01, -9.1382e-02, -8.1707e-02,  ..., -4.8083e-01,\n           -7.2615e-01,  4.8550e-01],\n          [-1.7969e-01,  5.2268e-03, -3.1901e-01,  ..., -2.2387e-01,\n           -2.4940e-01,  4.2651e-02],\n          ...,\n          [ 4.2639e-01, -4.6097e-01,  9.1851e-02,  ...,  2.3704e-01,\n           -1.4049e-01,  4.5012e-01],\n          [ 5.4303e-02,  2.1139e-01, -2.1290e-01,  ..., -2.3935e-01,\n            1.9690e-01, -1.4556e-01],\n          [ 6.2408e-01,  1.3162e-01,  5.5379e-02,  ..., -1.1796e-01,\n            1.4404e-01, -4.0336e-01]],\n\n         [[ 2.7190e-01,  1.4988e-02, -2.0839e-03,  ...,  3.9396e-03,\n           -1.3605e-02, -1.2758e-03],\n          [-2.7506e-01,  9.9242e-02,  5.8154e-03,  ..., -7.6369e-02,\n            3.0214e-01,  5.2639e-02],\n          [-7.1955e-01,  1.5563e-01, -2.2284e-01,  ...,  2.7428e-02,\n           -1.4784e-02,  1.6643e-01],\n          ...,\n          [-8.5632e-01, -1.5496e-01, -6.9391e-03,  ..., -3.6154e-01,\n           -1.1608e-01,  7.1647e-02],\n          [-4.2170e-01,  4.5486e-02,  1.2847e-01,  ..., -1.3755e-01,\n           -4.6646e-01,  1.7543e-01],\n          [-9.4145e-01, -2.0402e-02, -8.6336e-02,  ..., -6.6149e-02,\n           -3.6288e-01,  3.5880e-01]],\n\n         [[ 3.8737e-02,  1.7042e-02, -4.3715e-03,  ...,  3.2801e-02,\n            3.5758e-03, -2.1997e-01],\n          [ 6.4525e-02, -1.3032e-01, -7.0709e-02,  ..., -1.1451e-01,\n           -1.4236e-01, -5.1637e-01],\n          [ 1.2331e-01,  7.8746e-02, -9.9049e-03,  ..., -1.1019e-01,\n           -2.0571e-01, -1.6622e-01],\n          ...,\n          [-5.6203e-02, -3.0834e-02,  8.1040e-02,  ...,  2.3187e-01,\n           -3.1130e-01, -1.1976e-01],\n          [ 5.7486e-02, -1.6409e-01,  9.7397e-02,  ...,  8.6678e-02,\n           -1.8303e-01, -2.1524e-01],\n          [ 9.5699e-02, -9.4756e-02,  1.3789e-01,  ..., -1.6573e-01,\n           -5.1339e-02, -5.8899e-01]],\n\n         ...,\n\n         [[ 1.0134e-02,  8.3906e-03, -1.4678e-02,  ...,  2.0061e-02,\n            8.9872e-03, -4.0096e-03],\n          [-6.4184e-01,  2.4045e-01,  9.8672e-02,  ...,  4.4117e-02,\n            9.3468e-02, -7.7275e-01],\n          [-7.0650e-01, -4.5859e-01, -1.3923e-01,  ..., -4.0585e-01,\n           -2.4234e-02,  5.8095e-01],\n          ...,\n          [-7.0595e-01,  4.2824e-02, -1.0454e-01,  ..., -1.7282e-01,\n            3.1412e-03,  4.0021e-02],\n          [-1.8547e-01, -5.4997e-01,  5.4040e-01,  ...,  8.5589e-01,\n           -6.9312e-01, -6.8143e-02],\n          [-4.1477e-02, -3.4422e-01, -1.2055e-01,  ..., -5.7615e-01,\n           -6.1381e-01,  3.2405e-01]],\n\n         [[ 4.6773e-03, -6.1752e-03, -1.3456e-02,  ...,  2.2459e-03,\n           -1.4624e-02,  1.7275e-02],\n          [ 1.0933e-01, -2.4646e-01,  4.2456e-01,  ..., -5.0560e-02,\n           -5.1515e-01,  7.0203e-02],\n          [-5.8753e-01, -2.8683e-01,  5.6241e-01,  ...,  8.7894e-02,\n           -5.3783e-01, -1.3549e-01],\n          ...,\n          [-3.6709e-01,  1.0368e-02, -2.6492e-01,  ...,  3.0226e-01,\n            1.7847e-01, -3.9443e-01],\n          [-1.8174e-01, -1.5944e-01, -1.2444e-01,  ...,  1.7185e-01,\n           -1.4663e-01, -3.7696e-01],\n          [ 8.7055e-02, -4.1232e-02,  1.1925e-02,  ..., -4.3146e-02,\n           -3.0805e-01, -1.8966e-01]],\n\n         [[ 9.5875e-03,  1.9877e-03,  5.5560e-04,  ...,  1.6537e-02,\n            1.5532e-03,  1.2090e-02],\n          [-2.8249e-02,  3.7383e-02,  3.8696e-01,  ..., -5.9451e-02,\n            1.7590e-01, -3.6832e-02],\n          [-2.0332e-03, -1.6847e-01,  1.1968e-02,  ...,  6.5603e-02,\n           -1.3205e-01,  2.7796e-01],\n          ...,\n          [-5.3895e-01, -9.4709e-02, -2.4603e-01,  ..., -9.8091e-02,\n            1.9656e-01, -9.3505e-02],\n          [-1.3160e-01,  2.0801e-01, -1.3776e-01,  ..., -5.2161e-02,\n           -6.2677e-02,  3.4716e-02],\n          [-1.6502e-01,  1.7395e-01,  9.3074e-02,  ..., -6.8535e-02,\n           -1.7071e-01,  6.4562e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.5351e-02, -8.4757e-02, -6.0248e-02,  ...,  6.7227e-01,\n            1.3881e+00, -7.4223e-01],\n          [ 1.2616e+00, -1.2370e+00, -7.4736e-01,  ...,  1.2731e+00,\n           -1.0633e+00, -4.8718e-01],\n          [ 1.1614e-01,  1.6054e-01, -4.4539e-01,  ...,  4.4137e-01,\n           -1.3666e+00,  1.7992e-01],\n          ...,\n          [-5.4168e-01, -2.6959e-01,  6.0514e-01,  ..., -2.5940e-01,\n           -5.2639e-01, -1.0426e+00],\n          [-5.8534e-01, -1.8146e-01,  1.1400e+00,  ..., -2.3092e-01,\n           -1.5982e+00, -1.6913e-01],\n          [ 7.7766e-01,  1.6303e+00,  2.2796e+00,  ..., -5.4150e-01,\n           -1.6284e+00, -1.7295e+00]],\n\n         [[-1.8946e-02, -2.2157e-02,  2.3029e-02,  ...,  3.3776e-01,\n            1.0489e-01, -1.6274e+00],\n          [-2.2000e-01, -1.2279e-01,  4.6447e-01,  ...,  4.0435e-01,\n           -1.0897e+00,  5.0793e+00],\n          [ 3.8183e-01, -1.3501e-01,  9.4150e-01,  ...,  3.5868e+00,\n           -2.5323e+00,  3.0625e+00],\n          ...,\n          [ 7.1628e-01,  2.1930e-01,  6.7505e-01,  ...,  2.6556e+00,\n           -5.0718e-01,  4.5530e+00],\n          [-5.0165e-01, -7.0535e-01, -3.4468e-01,  ...,  3.8036e+00,\n           -1.9527e+00,  3.5220e+00],\n          [-6.0048e-01, -9.2246e-01, -7.6930e-01,  ...,  4.0905e+00,\n           -1.6349e+00,  3.0920e+00]],\n\n         [[ 2.1701e-02, -9.1714e-03,  9.7642e-02,  ...,  9.4590e-01,\n           -2.8308e-01, -5.0261e-01],\n          [ 6.7626e+00, -2.7791e+00,  3.9439e+00,  ...,  2.1430e+00,\n           -6.6616e-01,  9.2521e-02],\n          [ 5.1229e+00, -4.7000e+00,  2.7205e+00,  ...,  1.3188e+00,\n            1.8473e+00,  2.0944e+00],\n          ...,\n          [-6.0245e+00, -1.9102e+00,  4.0358e-01,  ...,  2.3604e+00,\n            1.3408e+00,  1.3355e+00],\n          [-5.4936e+00,  2.1390e+00, -1.3403e-01,  ...,  2.7181e+00,\n            2.1750e+00,  1.3737e+00],\n          [-1.2934e+00,  4.0860e+00, -2.1220e+00,  ...,  2.4340e+00,\n            3.3896e-01,  1.1140e+00]],\n\n         ...,\n\n         [[ 5.4727e-03,  1.4969e-02, -4.3817e-02,  ..., -3.0039e-01,\n            1.1738e-01,  2.3453e+00],\n          [-9.3585e-01,  1.6610e+00, -1.2045e+00,  ..., -1.0590e+00,\n            6.9103e-01, -2.5385e+00],\n          [-8.8610e-01,  2.2702e+00,  1.1747e+00,  ..., -1.7107e+00,\n           -1.5240e+00, -3.1733e+00],\n          ...,\n          [ 9.4467e-01, -1.8372e-01, -2.0256e-02,  ...,  2.1084e-01,\n           -2.7350e+00, -4.0278e+00],\n          [ 1.1101e+00, -2.2151e+00,  1.6296e+00,  ..., -1.1708e+00,\n           -1.5052e+00, -4.0883e+00],\n          [ 8.3699e-01, -2.0846e+00,  2.7970e-01,  ...,  8.6965e-01,\n           -2.0266e+00, -3.0972e+00]],\n\n         [[ 2.7228e-02,  2.5961e-02, -3.5073e-02,  ...,  5.4443e-01,\n            4.4475e-01,  8.4028e-01],\n          [ 2.2954e-01,  1.0210e+00, -1.6859e+00,  ...,  3.5193e+00,\n            6.4974e-01, -5.3879e-01],\n          [ 1.8964e-01,  9.0448e-02,  8.7046e-01,  ...,  2.1939e+00,\n           -3.3477e-01,  5.3138e-01],\n          ...,\n          [ 1.9603e-01, -2.4385e-01, -6.1381e-01,  ...,  2.2383e+00,\n           -1.2850e+00, -4.3273e-01],\n          [-1.3237e+00, -1.1143e+00, -2.3366e-01,  ...,  1.7414e+00,\n           -3.3808e-02, -9.9236e-01],\n          [-1.7050e+00, -6.5092e-02, -6.4913e-01,  ...,  1.6118e+00,\n           -6.3670e-01, -3.3138e-01]],\n\n         [[ 8.4090e-02, -6.8144e-02, -1.7702e-02,  ..., -3.3280e-01,\n            1.3528e+00,  6.3329e-01],\n          [ 1.9053e+00, -5.0859e+00,  1.6806e+00,  ..., -2.0356e-01,\n           -4.8222e+00,  9.6508e-01],\n          [-2.3532e+00, -4.2028e+00,  2.2080e+00,  ...,  1.7840e-01,\n           -5.2344e+00,  8.7347e-01],\n          ...,\n          [-2.2304e+00,  1.1961e+00,  1.8620e+00,  ..., -8.3341e-01,\n           -5.6237e+00,  9.5758e-01],\n          [ 1.8058e+00,  3.1312e+00,  1.1766e+00,  ...,  4.3110e-01,\n           -5.3096e+00, -4.7511e-01],\n          [ 3.0188e+00,  3.3326e+00,  9.3646e-01,  ..., -1.9392e-01,\n           -5.4617e+00, -5.2820e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5132e-02,  1.8012e-02,  1.9541e-02,  ...,  9.7809e-03,\n            2.9872e-03, -3.5270e-02],\n          [ 1.4281e-01, -1.1690e-01, -2.1661e-01,  ..., -5.3711e-02,\n           -2.7509e-01,  1.8915e-01],\n          [ 2.4982e-01, -4.4841e-01, -1.2889e-01,  ..., -3.4453e-01,\n            3.0005e-01,  2.4777e-01],\n          ...,\n          [ 3.6535e-01, -1.3034e-01, -1.3958e-01,  ..., -2.6643e-01,\n            4.5300e-01,  2.8213e-01],\n          [ 2.4861e-01, -2.5750e-01, -1.0779e-01,  ..., -4.1795e-01,\n            1.6677e-01,  4.0313e-01],\n          [ 3.0019e-01, -3.3321e-01, -1.2242e-01,  ...,  1.9265e-01,\n            5.1266e-01, -1.3264e-01]],\n\n         [[-1.9224e-02, -3.4200e-03, -4.3395e-03,  ..., -7.5942e-03,\n           -1.1307e-02,  1.7963e-02],\n          [ 2.5455e-01, -1.8495e-01,  4.3413e-01,  ..., -5.0476e-01,\n           -5.0250e-01,  8.9971e-02],\n          [ 3.3818e-02, -3.1044e-02,  5.4943e-02,  ..., -8.5431e-02,\n           -2.6772e-02, -1.2708e-01],\n          ...,\n          [-2.8894e-01, -3.3197e-02,  1.5209e-01,  ..., -5.2791e-01,\n            1.9384e-01, -4.7756e-02],\n          [-1.1962e-01,  4.4641e-02, -1.0454e-01,  ..., -8.0905e-02,\n           -6.1556e-02,  1.7375e-01],\n          [ 2.4786e-01, -8.8134e-02, -2.4973e-01,  ..., -1.6788e-01,\n           -1.0659e-01,  3.0257e-02]],\n\n         [[-1.0082e-01, -3.0075e-02,  4.1509e-02,  ..., -7.7669e-01,\n           -1.3114e-02,  1.2001e-02],\n          [ 4.5432e-02,  3.4684e-02,  1.6137e-03,  ...,  2.4898e-01,\n            6.4173e-02,  8.2674e-03],\n          [ 3.7278e-02,  2.0497e-01,  1.1244e-01,  ...,  4.9411e-01,\n            7.0016e-02,  1.0976e-01],\n          ...,\n          [-2.1570e-01, -2.0606e-01, -2.4016e-01,  ...,  4.4447e-01,\n           -7.3380e-02,  6.9049e-02],\n          [-1.0900e-02, -3.4667e-01,  2.0258e-01,  ...,  1.1281e-01,\n           -2.3070e-01,  1.1161e-01],\n          [ 3.9688e-01, -3.2756e-01, -1.8547e-01,  ...,  7.6121e-01,\n           -1.7677e-01, -1.0005e-01]],\n\n         ...,\n\n         [[-5.7387e-03,  2.5267e-02, -2.2123e-02,  ...,  1.1183e-02,\n           -2.7630e-03,  1.7944e-02],\n          [-1.3462e-01, -4.6760e-01, -4.1073e-01,  ..., -3.3048e-02,\n           -1.3100e-01, -3.1595e-01],\n          [-7.4877e-03,  1.3933e-01, -5.2769e-02,  ..., -9.8085e-02,\n            1.9186e-01,  1.9385e-02],\n          ...,\n          [-1.2679e-01,  3.3661e-01, -4.8950e-02,  ...,  7.4261e-03,\n            2.2402e-01,  3.6038e-01],\n          [-1.7555e-01, -4.5212e-02,  6.4099e-02,  ...,  2.6169e-02,\n            1.2100e-01, -1.3538e-02],\n          [-8.9102e-02, -9.6494e-02,  8.1932e-02,  ...,  9.2361e-02,\n            2.7794e-01,  3.1251e-01]],\n\n         [[ 1.8212e-01, -1.8599e-02, -2.1570e-02,  ...,  3.7844e-03,\n            1.8826e-02, -2.6143e-02],\n          [-4.6325e-01,  1.6244e-01,  2.2072e-01,  ..., -6.9552e-02,\n            6.1167e-01,  2.6188e-02],\n          [-5.3045e-03,  8.4458e-02,  6.6047e-02,  ...,  1.6932e-02,\n            2.0987e-01,  9.1609e-02],\n          ...,\n          [-4.8462e-01,  1.1965e-01,  8.3614e-02,  ..., -4.0738e-02,\n            3.4050e-01, -1.3341e-01],\n          [-3.5824e-01,  2.9223e-01,  1.8767e-02,  ...,  2.0220e-01,\n            6.3876e-02, -3.2615e-02],\n          [-7.1602e-01,  2.7131e-01, -1.1609e-01,  ...,  2.3846e-01,\n           -2.6324e-01, -6.5402e-02]],\n\n         [[-6.5726e-03,  3.8399e-03, -1.3766e-02,  ...,  6.9220e-03,\n           -4.0670e-03, -1.2715e-02],\n          [ 3.6617e-01,  2.9218e-01,  2.4028e-02,  ..., -2.0067e-04,\n           -1.1130e-01, -1.2831e-01],\n          [ 4.3841e-01, -1.9646e-01, -2.7185e-02,  ..., -6.6065e-01,\n            3.1660e-02,  2.8755e-02],\n          ...,\n          [ 2.4336e-01,  1.5147e-01, -4.0727e-01,  ...,  1.4628e-01,\n            2.9974e-02, -1.9604e-01],\n          [ 6.2762e-01,  3.9751e-01, -2.2898e-01,  ..., -5.8547e-03,\n           -1.4339e-01, -2.3870e-01],\n          [ 6.9609e-01,  3.3760e-03, -1.3496e-01,  ...,  1.7550e-01,\n           -7.9604e-02, -1.1876e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0166e-01,  6.1919e-03, -9.3783e-03,  ..., -4.7718e-02,\n           -5.6272e-01, -5.0096e-01],\n          [ 2.0590e+00, -1.9600e+00, -1.1147e+00,  ...,  1.0343e+00,\n            9.8983e-01,  2.3284e+00],\n          [-1.7154e+00, -3.9402e+00, -2.2778e+00,  ...,  9.1845e-01,\n            3.9979e+00,  2.5265e+00],\n          ...,\n          [-2.0346e+00, -1.5620e+00, -2.1357e+00,  ...,  1.7008e+00,\n            6.1601e+00,  8.7485e-01],\n          [ 1.3886e+00,  1.3091e+00, -8.0861e-01,  ..., -2.7334e-01,\n            4.8732e+00,  1.8793e+00],\n          [ 2.9248e+00,  2.4528e+00, -9.1368e-01,  ...,  2.1388e-01,\n            3.8603e+00,  1.6429e+00]],\n\n         [[-5.6379e-02,  1.4577e-03,  1.8990e-02,  ..., -1.5364e-01,\n            3.8202e-01, -2.9403e-01],\n          [-1.1381e+00, -4.8696e-01,  1.2164e+00,  ...,  1.4825e+00,\n           -1.1934e+00, -2.6655e+00],\n          [ 9.1242e-01,  1.2252e-01, -3.6523e-01,  ...,  1.1115e-02,\n           -1.4870e+00, -2.2476e+00],\n          ...,\n          [-1.7791e-01, -3.7555e-01, -1.2161e+00,  ...,  5.8366e-02,\n           -2.0055e+00, -3.0933e+00],\n          [-1.6694e+00, -4.3770e-01, -8.6095e-01,  ..., -9.8965e-02,\n           -6.4224e-01, -2.4396e+00],\n          [-6.8991e-01,  6.9570e-01, -1.6520e+00,  ...,  6.1562e-01,\n           -9.9733e-01, -2.7517e+00]],\n\n         [[ 1.8466e-02,  1.4924e-03,  1.2577e-02,  ..., -3.4818e-01,\n           -2.3574e-01, -2.2659e-02],\n          [-3.1628e-01, -1.8836e-01, -2.3994e+00,  ..., -1.8448e-01,\n            3.8541e-01,  1.1383e+00],\n          [ 4.1570e-01, -8.1133e-01, -5.0007e-01,  ...,  1.0797e+00,\n           -1.2453e+00,  8.1130e-01],\n          ...,\n          [ 5.7306e-01, -1.7102e+00, -1.1349e+00,  ...,  6.5464e-01,\n           -7.6430e-01, -1.3933e-01],\n          [-6.3758e-01, -2.0501e-01, -1.1317e+00,  ...,  1.5231e+00,\n           -9.7394e-01, -3.4505e-01],\n          [-8.7936e-01,  1.6732e-01,  1.6500e-02,  ...,  1.8085e+00,\n            3.6900e-01,  4.4737e-01]],\n\n         ...,\n\n         [[ 2.0947e-02, -6.7295e-03, -6.2162e-02,  ..., -1.3262e-01,\n            1.1672e+00,  8.1977e-01],\n          [-4.1865e+00, -2.6542e+00, -2.7694e+00,  ...,  2.7021e-01,\n           -5.0194e+00, -8.1190e-01],\n          [-1.7897e+00, -8.9176e-01, -1.9546e+00,  ..., -8.6345e-01,\n           -5.0271e+00, -7.0862e-01],\n          ...,\n          [ 1.1249e+00,  8.6208e-01,  1.1570e+00,  ...,  5.2513e-02,\n           -6.3273e+00,  8.7164e-02],\n          [ 1.5300e+00,  1.0404e+00,  5.0218e-01,  ..., -8.9805e-01,\n           -3.1471e+00, -1.1067e+00],\n          [ 1.2062e+00,  1.5224e+00,  2.3872e+00,  ...,  3.7257e-02,\n           -6.3237e+00,  2.5327e-01]],\n\n         [[-1.0139e-02, -8.0883e-03, -4.1945e-02,  ...,  1.4360e+00,\n           -3.6248e-01,  9.7605e-01],\n          [ 8.9840e-01,  2.1517e+00, -3.8953e-01,  ..., -3.9081e+00,\n            1.1726e+00, -1.3648e+00],\n          [ 1.7977e+00,  7.8718e-01,  1.1889e+00,  ..., -4.4897e+00,\n            2.2454e+00, -1.9135e+00],\n          ...,\n          [-2.4455e-01,  4.0773e-04,  2.1442e-01,  ..., -5.0472e+00,\n            2.9887e+00, -1.8345e+00],\n          [-1.7546e+00, -9.5721e-01,  1.3035e+00,  ..., -5.8629e+00,\n            1.4618e+00, -1.4617e+00],\n          [-6.8578e-01, -1.5225e-01,  1.0087e+00,  ..., -5.0745e+00,\n            1.5732e+00, -1.4222e+00]],\n\n         [[-2.0678e-02, -2.5024e-05, -2.5933e-02,  ...,  9.0611e-01,\n            1.1658e+00, -1.5922e-02],\n          [ 5.4387e-01, -1.5937e+00, -4.2302e-01,  ...,  8.4128e-02,\n           -3.1649e+00, -6.3091e-01],\n          [ 2.1531e+00, -3.2726e-01, -8.0747e-01,  ..., -9.3588e-01,\n           -1.2575e+00, -2.0457e+00],\n          ...,\n          [-1.9525e+00,  9.4569e-01,  1.2079e+00,  ..., -2.9127e+00,\n           -6.6631e-01, -4.0533e+00],\n          [-2.2828e+00,  5.9731e-01,  1.1725e+00,  ..., -1.2854e+00,\n           -1.6468e+00, -2.0581e+00],\n          [-8.2825e-01, -4.9824e-01,  2.1773e+00,  ..., -6.2644e-01,\n           -1.7132e+00, -1.9862e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3526e-02, -7.9978e-03, -6.1671e-03,  ...,  1.7907e-02,\n           -1.5949e-02, -1.2638e-02],\n          [-1.6625e-01, -7.3503e-02,  3.6931e-01,  ..., -4.4330e-01,\n            1.7235e-01,  8.7753e-01],\n          [-1.5399e-01,  2.1759e-01,  1.5064e-01,  ...,  1.4731e-02,\n            1.5621e-02,  8.8263e-02],\n          ...,\n          [-3.0861e-01,  4.5957e-01,  1.2916e-01,  ..., -2.9938e-02,\n           -6.1752e-02, -5.2606e-01],\n          [-3.3056e-01,  2.1115e-01, -1.8068e-01,  ...,  5.4267e-02,\n            7.9759e-02, -3.4859e-01],\n          [ 3.3028e-02,  1.7922e-01,  1.0573e-01,  ..., -2.1721e-01,\n            5.2501e-01, -4.0131e-01]],\n\n         [[-4.8035e-02,  1.9834e-02,  3.9587e-03,  ...,  1.7674e-02,\n            1.0605e-02,  3.7298e-04],\n          [ 1.6199e-01, -1.5647e-01,  2.5685e-01,  ..., -5.2654e-01,\n           -2.7026e-02, -5.9948e-02],\n          [ 2.1545e-01, -1.0643e-01,  4.2412e-02,  ...,  3.6600e-02,\n           -9.2282e-02, -6.1496e-02],\n          ...,\n          [ 3.1842e-01,  1.9338e-01, -3.2992e-01,  ..., -4.8378e-01,\n            3.4042e-02, -2.8658e-01],\n          [ 2.1974e-01,  1.9647e-01, -8.0642e-02,  ..., -1.7587e-01,\n           -1.4707e-01,  3.5467e-01],\n          [ 4.8984e-01,  6.0971e-02, -2.0218e-01,  ..., -5.5265e-02,\n           -1.0686e-02, -4.7457e-01]],\n\n         [[-8.0104e-03, -2.1802e-02,  1.5194e-03,  ..., -4.4974e-02,\n            3.2744e-02,  2.1210e-02],\n          [ 1.2057e-01,  1.1131e-02, -1.6751e-02,  ...,  4.2662e-01,\n            3.3568e-01,  1.9689e-01],\n          [ 1.8086e-01,  3.0699e-01, -8.3206e-02,  ..., -6.2404e-02,\n            4.9114e-01,  7.2837e-03],\n          ...,\n          [ 2.0506e-01,  2.7964e-01,  2.9877e-01,  ...,  5.7278e-02,\n            5.4048e-01, -6.4822e-02],\n          [ 3.4822e-01,  8.7741e-02,  2.1238e-01,  ..., -1.2006e-01,\n            4.1560e-01, -7.6328e-02],\n          [-7.0437e-02,  5.7161e-02,  1.2167e-01,  ..., -1.1188e-01,\n            1.5424e-01, -9.5308e-02]],\n\n         ...,\n\n         [[ 3.5515e-03, -2.1943e-02, -2.0210e-03,  ..., -1.0336e-02,\n            2.3350e-02, -1.0455e-02],\n          [ 2.4865e-01,  4.3538e-01,  4.5029e-01,  ..., -7.2358e-02,\n            1.0188e-01, -1.8383e-01],\n          [-1.0518e-01,  5.2964e-01,  3.7270e-01,  ..., -6.7872e-02,\n            1.3131e-01, -2.8911e-01],\n          ...,\n          [ 3.0064e-01,  5.5310e-01, -5.4533e-01,  ...,  1.4647e-01,\n           -1.5924e-01, -1.5693e-01],\n          [ 3.8755e-01,  3.9562e-01, -5.5967e-01,  ...,  1.4331e-01,\n           -9.4329e-02, -3.2281e-01],\n          [-3.3547e-02,  3.9899e-01, -2.8447e-01,  ..., -6.4645e-02,\n            6.6053e-02, -1.9320e-01]],\n\n         [[ 1.4852e-03,  9.5778e-03,  2.6551e-03,  ..., -1.0983e-02,\n            3.9189e-03,  1.4490e-02],\n          [ 1.8509e-02,  1.6305e-01,  1.6417e-01,  ...,  1.5492e-02,\n           -1.6470e-01,  1.1245e-01],\n          [-4.3813e-01,  3.9587e-01, -1.9904e-01,  ..., -2.9899e-01,\n            3.3618e-02, -3.4078e-02],\n          ...,\n          [-3.3382e-01,  5.3112e-01, -2.7433e-01,  ...,  1.8164e-01,\n            6.9968e-02, -5.8562e-02],\n          [-3.6744e-01,  6.0500e-01, -4.2692e-01,  ...,  1.8678e-01,\n            2.2787e-01,  1.1091e-01],\n          [-5.3681e-01,  6.5478e-01, -2.8589e-01,  ..., -3.1608e-01,\n           -2.2584e-01, -3.8646e-01]],\n\n         [[-2.9645e-01, -1.9910e-02, -1.7448e-02,  ...,  1.3408e-02,\n           -3.6502e-02, -1.3350e-02],\n          [ 7.8251e-01, -7.2778e-01,  9.3247e-01,  ...,  4.0642e-01,\n           -2.4694e-01,  6.0712e-01],\n          [ 6.3364e-01, -1.7825e-01,  2.1098e-01,  ..., -1.4767e-01,\n           -6.5873e-02,  1.0685e-02],\n          ...,\n          [ 2.3564e-01,  4.9659e-03,  2.9801e-01,  ..., -9.7692e-02,\n           -2.0164e-01,  1.4818e-01],\n          [ 5.6062e-01, -1.8731e-01, -3.0178e-01,  ..., -1.6078e-01,\n            5.3026e-04,  1.7745e-01],\n          [ 3.5423e-01, -2.3672e-01, -7.3669e-02,  ...,  1.7488e-01,\n            2.8239e-01,  1.1818e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-4.9875e-02,  2.4972e-03, -1.3115e-02,  ..., -1.0506e-01,\n           -1.0948e+00,  7.7103e-01],\n          [ 1.4863e+00,  1.5200e+00, -1.1959e+00,  ..., -1.3442e+00,\n            3.9415e+00, -5.7495e-01],\n          [-1.0452e-01,  3.3289e-01,  1.0234e-01,  ..., -2.8023e+00,\n            4.7230e+00, -3.0324e+00],\n          ...,\n          [-1.1804e-01, -2.6029e-01,  5.8443e-01,  ..., -9.4457e-01,\n            3.2178e+00, -3.4486e+00],\n          [-2.7280e-02, -4.2073e-01,  8.1108e-01,  ..., -3.5189e-01,\n            4.3311e+00, -3.7826e+00],\n          [-7.3906e-02, -1.3747e+00,  3.9041e-02,  ..., -6.8392e-01,\n            3.2543e+00, -2.6435e+00]],\n\n         [[-3.5304e-02,  2.9584e-03, -2.7550e-02,  ..., -3.4000e-01,\n            1.1422e+00,  2.6492e-02],\n          [-5.6816e-02,  4.5117e-03, -1.3083e+00,  ...,  1.1244e+00,\n           -9.8961e-02,  1.7672e+00],\n          [ 1.8967e+00,  6.0789e-01,  1.9745e-01,  ..., -6.9852e-01,\n           -1.4396e+00,  2.8527e-01],\n          ...,\n          [-5.7954e-01,  6.8881e-01,  7.5959e-01,  ..., -6.2134e-01,\n           -1.2529e+00,  5.6745e-01],\n          [-1.5116e+00,  3.8163e-01,  1.6837e+00,  ...,  4.5959e-01,\n           -1.1081e+00,  9.8418e-01],\n          [-1.2084e+00, -1.3505e-01,  2.4064e+00,  ..., -2.4949e-01,\n            7.0691e-01,  7.7508e-01]],\n\n         [[ 3.7110e-02,  1.0985e-01,  7.0407e-02,  ..., -9.0719e-01,\n           -1.1212e+00,  1.1719e+00],\n          [-4.4360e+00,  2.4716e+00,  1.2563e+00,  ...,  3.4452e+00,\n            1.7526e+00, -2.0145e-01],\n          [-3.4237e+00,  1.0477e+00,  2.8551e+00,  ...,  2.7749e+00,\n            9.4620e-01, -1.3547e+00],\n          ...,\n          [ 2.9569e+00, -1.6506e+00,  4.2026e+00,  ...,  4.4533e+00,\n            1.0390e+00, -1.0966e+00],\n          [ 4.2534e+00, -2.2310e+00,  2.0792e+00,  ...,  2.6211e+00,\n            1.2067e+00, -7.3345e-01],\n          [ 3.0529e+00, -1.4761e+00,  4.9123e-01,  ...,  3.2055e+00,\n            1.5274e+00, -1.3190e+00]],\n\n         ...,\n\n         [[ 2.9833e-03, -9.0822e-03,  1.8295e-02,  ...,  6.1501e-01,\n           -1.1246e-01,  8.5372e-01],\n          [ 3.3304e-01, -6.4003e-01, -1.0934e+00,  ..., -3.9152e+00,\n            4.6173e-01, -5.7613e-01],\n          [-5.1271e-02, -3.1483e-01, -2.7131e-01,  ..., -3.7386e+00,\n           -2.9720e-01, -2.6095e+00],\n          ...,\n          [-2.3231e-01,  7.7547e-01, -3.5580e-01,  ..., -4.7908e+00,\n            1.2966e+00, -2.2226e+00],\n          [-2.2539e-03,  1.0059e+00,  2.9037e-01,  ..., -3.9186e+00,\n            3.1690e+00, -2.5507e+00],\n          [-2.3137e-01,  4.1141e-01,  6.0183e-01,  ..., -4.8506e+00,\n            3.3978e+00, -3.1643e+00]],\n\n         [[ 3.1923e-01,  6.5008e-02, -1.5732e-01,  ...,  2.5957e-01,\n           -6.0922e-01,  5.0128e-01],\n          [ 9.7237e+00,  4.0054e+00, -2.1038e+00,  ...,  1.1219e+00,\n           -1.1703e+00,  1.6707e-01],\n          [ 7.4607e-01,  4.6296e+00, -1.1777e+00,  ...,  1.3249e+00,\n           -1.6541e+00,  1.2219e+00],\n          ...,\n          [-1.0187e+01, -5.7691e-01,  2.1336e+00,  ..., -3.7384e-02,\n           -9.9703e-01,  1.5344e+00],\n          [-2.8958e+00, -4.0741e+00,  2.4365e+00,  ...,  1.3634e-01,\n           -1.3923e+00,  1.4176e+00],\n          [ 7.5061e+00, -3.6183e+00,  3.2269e+00,  ...,  6.6178e-01,\n           -1.4693e+00,  1.5031e+00]],\n\n         [[-4.7712e-02, -1.6998e-02,  4.7513e-02,  ..., -2.0291e+00,\n           -3.1721e-01, -1.9669e+00],\n          [ 4.6644e-01, -2.0919e-01,  3.7741e+00,  ...,  1.8680e+00,\n            9.3282e-01,  4.8208e-01],\n          [ 1.7686e+00, -1.0605e+00,  2.7540e+00,  ...,  4.3334e+00,\n           -7.2462e-01,  1.3663e+00],\n          ...,\n          [ 7.0148e-01, -1.6251e-01,  9.9140e-02,  ...,  4.4427e+00,\n           -2.3735e+00,  2.5939e+00],\n          [-1.2951e+00,  1.2367e+00,  2.1290e-03,  ...,  4.9486e+00,\n           -1.9682e-01,  2.4470e+00],\n          [-1.8347e+00,  2.0486e+00,  7.5327e-02,  ...,  4.4172e+00,\n            5.1234e-01,  1.1785e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.5693e-03,  3.5057e-02,  1.6987e-02,  ..., -3.8882e-02,\n           -8.0818e-02,  3.6926e-02],\n          [ 8.1845e-02,  6.0510e-01,  6.9635e-01,  ..., -8.5384e-01,\n            5.5322e-02, -1.0471e+00],\n          [-9.1735e-02,  2.2055e-01, -3.3842e-01,  ..., -3.2321e-01,\n            8.8266e-01,  3.1458e-01],\n          ...,\n          [-2.6894e-01, -3.2540e-01, -4.2203e-01,  ..., -2.5452e-01,\n            4.4596e-02, -5.8919e-01],\n          [ 1.0207e-01,  1.5196e-01, -8.9608e-01,  ...,  1.8337e-01,\n           -1.6134e-01, -1.1361e-02],\n          [ 2.3689e-01,  9.0550e-01,  2.4622e-01,  ...,  2.5552e-01,\n            1.2596e-01,  2.7388e-01]],\n\n         [[-8.8539e-03, -1.8422e-02, -2.0399e-02,  ...,  2.3280e-02,\n           -7.1595e-03, -4.6043e-03],\n          [ 3.3777e-03, -1.4677e-01,  3.8548e-01,  ..., -2.8452e-03,\n           -2.4198e-01,  2.2888e-01],\n          [-4.7083e-02, -3.2774e-02,  3.5672e-01,  ...,  8.3398e-02,\n            3.9802e-01,  9.0960e-02],\n          ...,\n          [ 2.5897e-01,  2.0614e-01, -2.2329e-01,  ...,  2.8248e-01,\n            3.0558e-01,  4.1906e-01],\n          [ 1.6578e-01,  1.4408e-01,  1.6520e-01,  ...,  3.1801e-01,\n            1.2584e-01,  1.9883e-01],\n          [-5.2341e-01, -2.3311e-02,  1.2760e-01,  ...,  2.7007e-01,\n           -1.2230e-01,  1.1001e-01]],\n\n         [[-6.7410e-02, -4.4090e-03,  1.6212e-02,  ..., -1.1219e-02,\n            2.1429e-03, -1.5527e-03],\n          [-7.4422e-02, -4.2096e-02,  2.0127e-01,  ..., -7.1574e-02,\n            9.6221e-02,  3.5584e-01],\n          [-1.4204e-01,  8.8811e-02,  1.2295e-01,  ...,  5.4589e-02,\n           -2.7241e-01,  2.8080e-01],\n          ...,\n          [-1.0015e+00,  2.3462e-01,  1.7334e-01,  ...,  1.5034e-01,\n            2.8653e-01, -2.0339e-01],\n          [-4.9001e-01,  4.7482e-01, -6.2534e-02,  ...,  1.9007e-01,\n           -1.1071e-01, -1.2971e-01],\n          [ 4.0019e-01,  4.1475e-01, -2.3828e-01,  ...,  3.3566e-01,\n           -2.9233e-01, -2.2862e-01]],\n\n         ...,\n\n         [[ 4.7063e-04,  3.1449e-02, -1.7503e-02,  ...,  8.2695e-03,\n           -1.0970e-02, -1.1543e-02],\n          [-1.2277e+00, -3.2604e-01, -6.1393e-02,  ..., -3.8778e-02,\n           -9.9841e-02, -5.4652e-01],\n          [ 9.1386e-02,  4.2278e-01,  9.2652e-02,  ...,  8.1694e-03,\n           -3.0594e-02,  1.4136e-01],\n          ...,\n          [-2.2290e-01, -6.7235e-02,  5.7267e-02,  ...,  2.3843e-01,\n           -2.7630e-02, -1.2387e-02],\n          [ 2.0408e-01,  3.7040e-01,  2.9052e-02,  ..., -4.8617e-02,\n            2.0986e-01,  3.6221e-01],\n          [ 4.6196e-01, -4.6471e-01, -4.3433e-02,  ...,  1.3075e-01,\n            5.5972e-02,  2.5477e-01]],\n\n         [[ 9.3577e-03,  3.9018e-03,  9.3528e-04,  ..., -5.8127e-03,\n           -1.1375e-03,  1.1215e-03],\n          [-4.8039e-01, -1.3629e-01, -7.1690e-01,  ...,  5.1239e-01,\n            3.5236e-01, -1.1517e-01],\n          [-6.2005e-01, -4.2717e-01, -4.7873e-01,  ..., -3.8481e-03,\n           -4.0798e-01, -8.2157e-01],\n          ...,\n          [-5.1856e-01, -1.4422e-01, -2.7818e-01,  ...,  1.5228e-01,\n           -2.3082e-01, -3.3898e-01],\n          [-1.9378e-01,  5.9553e-01, -1.1286e-01,  ...,  5.0783e-01,\n            4.6441e-02,  8.9126e-02],\n          [-1.1452e-01, -1.2341e-01,  1.7265e-01,  ..., -9.1119e-02,\n           -1.0231e-02,  1.4952e-01]],\n\n         [[ 1.1949e-02, -2.2031e-02,  8.2400e-03,  ...,  8.8179e-03,\n            8.7592e-03,  1.6473e-04],\n          [ 3.2632e-01, -2.0327e-01, -2.1340e-01,  ...,  8.7580e-02,\n           -2.2788e-01,  3.6569e-01],\n          [ 2.6529e-01, -2.1978e-01, -2.1967e-01,  ..., -1.4477e-01,\n           -1.6774e-01,  6.5762e-02],\n          ...,\n          [ 2.4309e-01, -2.5524e-01, -2.3644e-01,  ..., -1.4594e-01,\n           -2.1775e-01, -5.8270e-01],\n          [ 3.2517e-01, -5.5770e-01, -4.9247e-01,  ...,  8.8617e-02,\n           -3.3503e-01,  1.0231e-01],\n          [ 1.8709e-01, -2.3732e-01, -2.8045e-01,  ..., -7.9458e-02,\n           -1.2273e-01,  9.1866e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0001e-02,  5.3479e-03, -9.2337e-03,  ...,  2.9982e-02,\n            8.8365e-01, -1.0928e+00],\n          [ 8.7294e-01, -2.6140e-01,  4.2966e-01,  ..., -1.0903e+00,\n           -2.5277e+00,  1.1819e+00],\n          [ 4.6817e-01,  9.8864e-01,  7.3982e-01,  ..., -4.6623e-01,\n           -2.0306e+00,  2.4614e+00],\n          ...,\n          [-2.7831e-01,  5.0420e-01,  1.2072e+00,  ...,  3.8389e-01,\n           -1.8511e+00,  2.0652e+00],\n          [-7.9975e-01,  6.8814e-01,  6.6837e-01,  ...,  1.7069e+00,\n           -2.4216e+00,  3.3887e+00],\n          [-7.2989e-01, -6.6200e-01,  2.8208e-01,  ...,  1.5478e+00,\n           -3.1226e+00,  2.1741e+00]],\n\n         [[-6.1644e-02,  2.6081e-02,  6.2417e-02,  ...,  6.3455e-01,\n            3.6131e-02,  2.4065e-01],\n          [-2.1200e+00, -1.3107e-01, -1.4443e-01,  ..., -3.1277e-01,\n           -1.2450e+00,  1.0522e+00],\n          [ 1.3968e+00, -1.1912e+00, -2.1702e+00,  ..., -9.9641e-01,\n           -2.4775e-01,  3.1098e-01],\n          ...,\n          [ 9.3467e-01, -2.4026e+00, -1.5782e+00,  ..., -6.1785e-01,\n           -1.6054e+00,  1.0960e+00],\n          [-3.8580e-01, -6.8748e-01, -1.3747e+00,  ..., -4.0310e-01,\n           -3.4155e-01,  4.8981e-01],\n          [-1.8219e+00,  6.2990e-01, -2.6710e-02,  ...,  9.7308e-01,\n           -1.9349e+00,  1.1708e+00]],\n\n         [[-1.5109e-02, -3.5196e-03,  6.1797e-02,  ...,  6.3407e-01,\n           -5.1091e-01, -8.2480e-01],\n          [ 1.5792e+00, -1.2319e+00,  1.1765e+00,  ..., -6.3072e-01,\n            1.3554e+00,  1.6394e+00],\n          [ 1.4461e+00, -4.4069e-01,  9.2721e-01,  ..., -3.9494e-01,\n            5.3216e-01,  7.6567e-01],\n          ...,\n          [-1.2211e+00,  3.1120e-02, -1.2459e+00,  ..., -2.1893e+00,\n            2.6633e+00,  1.2852e-01],\n          [-1.3804e+00,  1.4137e+00, -1.5049e+00,  ..., -1.5894e+00,\n            1.4744e+00,  7.6856e-01],\n          [-8.3890e-01,  4.5816e-01, -1.0203e+00,  ..., -2.3383e+00,\n            2.0474e-01,  8.8204e-01]],\n\n         ...,\n\n         [[ 2.0095e-02, -1.9391e-02, -1.4458e-03,  ..., -6.3438e-01,\n           -7.4186e-02,  1.2699e+00],\n          [-7.9215e-01, -1.0645e+00, -8.1103e-01,  ...,  3.6834e-01,\n            1.0701e+00, -2.1562e+00],\n          [-1.5776e-01, -1.8322e-01, -2.8168e-01,  ..., -2.7338e-02,\n           -1.2710e+00, -2.8762e+00],\n          ...,\n          [-2.1154e-01, -1.1368e-01, -4.2548e-01,  ...,  2.2172e-01,\n            1.3162e+00, -6.0494e+00],\n          [ 4.8693e-01,  6.2472e-01, -5.3281e-01,  ..., -8.7062e-01,\n           -2.0747e+00, -4.9780e+00],\n          [-3.7886e-01,  1.0626e+00,  3.1687e-01,  ...,  2.7412e-01,\n           -5.9731e-01, -3.4808e+00]],\n\n         [[ 1.5396e-01,  1.1387e-02,  1.6595e-02,  ..., -3.2535e-01,\n            1.6872e-01, -1.8801e+00],\n          [ 6.0560e-01, -1.1477e+00, -7.4180e-01,  ...,  1.6262e-01,\n            2.3737e+00,  4.6392e-01],\n          [-2.6162e-01, -2.2885e+00, -1.3641e+00,  ..., -3.8918e-01,\n            1.7129e+00,  4.5273e+00],\n          ...,\n          [-1.7206e+00, -1.4077e+00, -1.1191e+00,  ..., -6.5098e-01,\n            7.7733e-01,  3.0853e+00],\n          [ 5.1052e-01, -4.2147e-01, -1.0513e+00,  ..., -1.1379e+00,\n            1.7694e+00,  3.7997e+00],\n          [ 2.3640e+00,  1.4077e+00, -6.7150e-01,  ..., -1.7631e-01,\n            2.6162e+00,  4.0207e+00]],\n\n         [[-9.3324e-03,  1.2826e-02,  1.0013e-02,  ...,  1.6950e-01,\n            5.9047e-01,  9.9431e-01],\n          [ 1.0309e-01,  5.6140e-02,  2.8503e-02,  ..., -2.4224e+00,\n           -2.5067e+00, -2.2298e+00],\n          [-2.9537e-01,  2.1882e-01, -2.3786e-01,  ..., -1.1905e+00,\n           -2.1368e+00, -3.1597e+00],\n          ...,\n          [-4.5620e-02, -2.2910e-02, -7.9293e-02,  ..., -1.2766e+00,\n           -2.2234e+00, -2.7992e+00],\n          [ 5.6757e-01,  4.9995e-02,  4.2694e-02,  ...,  1.8118e+00,\n           -2.6661e+00, -2.8046e+00],\n          [ 1.2093e-01,  3.0445e-01, -7.8137e-02,  ...,  4.3429e-03,\n           -1.2910e+00, -1.7973e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0155, -0.0141,  0.0053,  ..., -0.0020, -0.0138, -0.0081],\n          [ 0.8893, -0.2964,  0.6060,  ..., -0.1289,  0.1849,  0.7236],\n          [ 0.0516,  0.4002,  0.5555,  ...,  0.0818, -0.7465,  0.1008],\n          ...,\n          [ 0.0592, -0.2722, -0.1607,  ..., -0.1261, -0.0721, -0.5363],\n          [ 0.5351, -0.0285,  0.3295,  ..., -0.1638, -0.3833, -0.1046],\n          [-0.8901,  0.3488,  0.5780,  ..., -0.4821,  0.0581, -0.2783]],\n\n         [[ 0.1125,  0.0091, -0.0121,  ..., -0.0521, -0.0187,  0.0434],\n          [-0.1717, -0.0280,  0.0977,  ...,  0.2039,  0.0755,  0.0779],\n          [-0.0912, -0.1047, -0.0915,  ...,  0.2498,  0.2045, -0.1584],\n          ...,\n          [ 0.1055,  0.1303, -0.6172,  ...,  0.2406, -0.3415,  0.1561],\n          [ 0.1275,  0.6940, -0.4025,  ...,  0.4515, -0.3610,  0.4613],\n          [-0.0474,  0.1060, -0.5638,  ..., -0.1677, -0.0342, -0.1656]],\n\n         [[-0.0043,  0.0186, -0.1110,  ..., -0.0808, -0.0089, -0.0305],\n          [-0.3964, -0.1324,  0.5198,  ...,  1.0464, -0.9780,  0.0426],\n          [-0.1032,  0.1052,  0.0733,  ..., -0.1005, -0.2119, -0.1483],\n          ...,\n          [ 0.5971, -0.0133, -0.1638,  ...,  0.7413, -0.2981,  0.5895],\n          [-0.3874,  0.2512,  0.1428,  ..., -0.1238, -0.3346,  0.1015],\n          [-0.5534,  0.6762, -0.0895,  ...,  0.5219, -1.0554, -0.1006]],\n\n         ...,\n\n         [[-0.0131,  0.0143, -0.0275,  ..., -0.0653, -0.0193,  0.0376],\n          [ 0.8291, -0.1804, -0.8852,  ..., -0.2940,  0.9659,  1.1397],\n          [-0.0343, -0.3794,  0.1225,  ..., -0.3974,  0.3114, -0.2152],\n          ...,\n          [ 0.2208, -0.3272, -0.4170,  ...,  0.4110, -0.2584,  0.1152],\n          [ 0.3429,  0.2526,  0.5178,  ...,  0.0889, -0.2551, -0.0285],\n          [-0.3247,  0.7592, -0.4231,  ..., -0.3010,  0.5201, -0.0014]],\n\n         [[-0.0173, -0.0193, -0.0596,  ..., -0.0225, -0.0123,  0.6400],\n          [-0.3333, -0.1722, -0.1961,  ...,  0.1566, -0.3654, -0.0344],\n          [ 0.1247,  0.0793,  0.1349,  ...,  0.2280,  0.2710, -0.4460],\n          ...,\n          [-0.0558, -0.2370,  0.1744,  ...,  0.4919, -0.3033, -0.4331],\n          [-0.0634,  0.1525,  0.0596,  ...,  0.4171,  0.0781, -0.6077],\n          [-0.1919, -0.1908,  0.0544,  ...,  0.5652, -0.0541, -0.7545]],\n\n         [[ 0.0253, -0.0222, -0.0151,  ...,  0.0164, -0.0272,  0.0803],\n          [ 0.2412,  0.8380, -0.4417,  ...,  0.6459, -0.6842,  0.2706],\n          [-0.2601,  0.0518, -0.1952,  ...,  0.2642, -0.4465,  0.4295],\n          ...,\n          [ 0.2313, -1.3138,  0.6014,  ..., -0.3423,  0.3664, -1.1917],\n          [ 0.5604,  0.0668,  0.1291,  ..., -0.2465,  0.0116, -0.3682],\n          [-0.4762,  0.0341,  0.3839,  ..., -0.5269, -0.5123,  0.2965]]]],\n       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.5414e-02,  4.6433e-02,  1.4184e-01,  ...,  5.4812e-01,\n           -4.8990e-01,  7.3753e-01],\n          [ 4.3876e+00,  5.3694e-01,  1.1721e+00,  ...,  1.3193e+00,\n           -1.3237e+00, -2.3558e-01],\n          [ 9.9249e-01,  3.1166e-02,  3.6171e-01,  ..., -1.5124e+00,\n           -2.4290e+00, -6.7962e-01],\n          ...,\n          [-1.1431e+00, -2.3096e+00, -3.1234e-01,  ..., -2.1145e+00,\n           -1.1770e+00, -4.5518e-01],\n          [-1.2081e+00, -1.9764e+00, -1.5585e-01,  ..., -2.3098e+00,\n           -2.8112e+00, -5.7975e-01],\n          [-1.8406e+00, -1.0024e+00, -2.8180e+00,  ..., -2.2056e+00,\n           -2.6257e+00, -4.7722e-01]],\n\n         [[ 1.2824e-02, -1.6541e-02,  1.6095e-03,  ..., -1.4238e-02,\n           -1.3309e+00,  2.2263e+00],\n          [-1.3956e+00,  8.9412e-02,  1.3159e-01,  ..., -4.4196e-01,\n            8.5787e-01, -9.0655e+00],\n          [-9.7274e-01, -5.4072e-01, -4.7400e-02,  ..., -3.0700e-01,\n            2.3624e+00, -5.6773e+00],\n          ...,\n          [ 1.3576e+00, -6.6439e-01,  1.1036e+00,  ..., -1.6988e+00,\n            3.5039e+00, -9.4972e+00],\n          [ 5.6010e-01, -9.3117e-01,  2.8272e+00,  ...,  5.8490e-01,\n            3.5013e+00, -5.3245e+00],\n          [ 5.9122e-01,  8.4591e-02,  2.3271e+00,  ...,  1.8021e+00,\n            3.4855e+00, -7.5563e+00]],\n\n         [[-1.3162e-01, -1.1779e-01, -8.8633e-02,  ...,  1.2191e+00,\n           -4.0260e-01, -9.9839e-01],\n          [-2.8954e+00, -3.4110e+00, -2.7247e+00,  ...,  4.0143e-01,\n           -2.4058e-01, -8.6029e-01],\n          [ 1.9712e+00, -1.7039e-02,  1.4560e-01,  ...,  1.4807e+00,\n           -6.6160e-01,  7.3650e-01],\n          ...,\n          [ 2.6792e+00,  3.1427e+00,  1.5565e+00,  ...,  1.3970e+00,\n            2.8025e-01,  4.6396e-01],\n          [-1.7699e+00,  3.4014e+00,  2.0562e+00,  ...,  1.0994e+00,\n            7.3417e-01, -1.7674e-01],\n          [-3.7179e+00,  2.6620e+00,  2.3894e+00,  ...,  1.0780e+00,\n            7.0768e-01,  8.1163e-01]],\n\n         ...,\n\n         [[ 2.6366e-02, -1.0544e-02,  6.2874e-04,  ...,  1.4033e-01,\n           -8.2465e-01, -3.7078e-01],\n          [ 1.6810e+00, -8.1752e-03, -1.3071e+00,  ...,  1.3426e+00,\n            2.8860e+00, -9.1175e-01],\n          [ 3.1420e+00,  1.8545e+00, -2.0833e+00,  ...,  2.5170e+00,\n            2.2813e+00, -2.3463e-01],\n          ...,\n          [-3.6326e+00,  2.8218e+00, -1.7481e+00,  ...,  4.2701e+00,\n            4.2890e+00, -3.0365e+00],\n          [-3.9798e+00,  1.4441e+00, -2.9004e-01,  ...,  4.0922e+00,\n            3.2314e+00, -1.7111e+00],\n          [ 4.9226e-01, -1.0174e+00, -2.0602e-01,  ...,  4.6128e+00,\n            3.7229e-01, -7.3213e-01]],\n\n         [[ 5.1096e-02,  1.2720e-01,  6.0000e-02,  ..., -1.7026e+00,\n            2.6214e-01, -1.8377e+00],\n          [-3.9598e+00,  1.7712e+00,  2.9831e+00,  ...,  2.2901e+00,\n           -7.9707e-01,  2.7464e+00],\n          [-2.6725e+00, -1.8800e-01,  2.2646e+00,  ...,  1.1920e+00,\n           -4.4188e-01,  2.6991e+00],\n          ...,\n          [ 1.1788e+00, -3.0626e+00,  8.2353e-01,  ...,  4.1268e+00,\n           -5.6293e-01,  2.0691e+00],\n          [ 3.1263e+00, -1.6974e+00, -2.8394e+00,  ...,  1.7192e+00,\n            1.1469e+00,  4.7347e+00],\n          [ 1.9080e+00, -1.7398e+00, -3.5397e+00,  ...,  3.0596e+00,\n            5.7570e-01,  1.9988e+00]],\n\n         [[ 1.9741e-02, -1.3131e-02,  1.8652e-02,  ..., -9.7016e-01,\n           -5.0637e-02,  4.6040e-01],\n          [-2.7769e-01, -5.1565e-01, -1.4570e-01,  ...,  2.3636e+00,\n            2.2137e+00, -1.0835e+00],\n          [-4.5133e-01,  1.6131e-01,  6.5179e-01,  ...,  3.1053e+00,\n            9.6139e-01,  3.5502e-01],\n          ...,\n          [ 3.0989e-01,  2.3544e-01, -5.7656e-01,  ...,  1.9273e+00,\n            8.0125e-01, -1.4189e+00],\n          [ 6.2809e-01,  1.4889e-01, -2.7151e-01,  ...,  3.2865e+00,\n            2.2598e-01, -1.6323e+00],\n          [ 6.7133e-02,  5.2969e-01, -7.0295e-01,  ...,  2.6760e+00,\n            4.5591e-02, -2.5868e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.4963e-02,  2.4116e-02, -1.4267e-02,  ...,  1.2166e-03,\n            2.0411e-02, -9.7046e-04],\n          [ 2.2547e-01,  1.4090e-01, -1.3826e-01,  ...,  2.4552e-02,\n           -2.0530e-01, -1.1754e-01],\n          [ 5.6882e-02,  1.7094e-01, -4.2025e-01,  ..., -9.0925e-02,\n           -1.7636e-01,  2.0896e-01],\n          ...,\n          [-2.9027e-01,  9.1443e-03, -4.6778e-01,  ...,  2.0404e-02,\n           -7.6829e-01, -8.6308e-02],\n          [-4.3335e-01,  3.2009e-01, -6.4599e-01,  ...,  2.0640e-01,\n           -6.1962e-01,  6.1385e-02],\n          [-3.5915e-01,  2.0407e-01, -4.2703e-01,  ...,  5.8975e-02,\n           -4.5818e-01,  4.4285e-01]],\n\n         [[ 3.4210e-02, -3.4803e-03, -1.2506e-03,  ..., -2.4340e-02,\n           -3.4289e-02,  4.0968e-02],\n          [-2.5548e-02,  8.6896e-01, -3.9595e-01,  ...,  5.3780e-01,\n           -2.1199e-01,  1.3437e+00],\n          [-1.8097e-01,  2.0235e-01,  3.2570e-02,  ..., -9.3057e-02,\n            2.3465e-01,  3.6229e-01],\n          ...,\n          [-5.9932e-02,  6.5331e-01,  2.2758e-01,  ..., -8.5440e-02,\n           -1.8942e-02,  5.7877e-01],\n          [ 1.5629e-01,  3.0888e-01,  1.4111e-01,  ...,  3.8784e-01,\n            3.4181e-01,  3.7496e-01],\n          [ 3.0771e-01, -1.3075e-01,  2.9143e-01,  ..., -2.0617e-01,\n           -3.7792e-01,  2.2385e-01]],\n\n         [[ 9.4470e-02,  1.9155e-01,  3.5195e-02,  ..., -3.0459e-02,\n            3.4772e-01, -7.3498e-03],\n          [-5.1428e-01,  3.0442e-01,  4.2104e-01,  ..., -5.7540e-01,\n           -1.5479e+00,  3.6643e-01],\n          [ 7.6056e-02,  1.6991e-01,  4.9027e-01,  ...,  1.1887e-01,\n           -2.2015e+00,  4.4847e-01],\n          ...,\n          [ 1.4213e-01,  5.0866e-01,  1.2624e-01,  ..., -2.8348e-01,\n           -2.0461e+00,  2.5732e-01],\n          [ 1.4330e-02, -2.6393e-01,  4.7206e-01,  ..., -2.3471e-01,\n           -2.9259e+00, -3.8417e-01],\n          [-1.4677e-01,  2.3432e-01,  1.5036e-02,  ..., -2.7339e-01,\n           -2.3934e+00,  2.3624e-02]],\n\n         ...,\n\n         [[-8.7270e-02, -6.6465e-02, -3.0371e-02,  ...,  1.7711e-02,\n            6.4740e-02, -3.5456e-03],\n          [-6.0243e-01,  3.2345e-02, -9.2731e-01,  ..., -2.6678e-01,\n            2.1238e+00, -8.8268e-01],\n          [-2.0218e-02, -2.0306e-01, -2.8669e-01,  ...,  4.7327e-01,\n           -2.6078e-01,  7.2375e-03],\n          ...,\n          [ 3.9089e-01,  1.1831e+00,  5.6502e-01,  ..., -7.5588e-01,\n           -7.9102e-01,  7.6052e-01],\n          [-1.4055e+00, -6.4177e-01, -5.4610e-01,  ..., -6.1377e-01,\n            5.9486e-02,  8.7491e-01],\n          [ 9.0679e-01,  5.2043e-02,  8.9971e-01,  ...,  3.3745e-01,\n            1.2728e+00, -7.4647e-02]],\n\n         [[ 2.7315e-02,  9.0491e-02, -7.3440e-02,  ..., -2.2172e-03,\n            4.7945e-02,  2.3947e-02],\n          [-4.5239e-02, -4.2668e-01,  4.3743e-01,  ..., -2.0161e-01,\n           -7.7504e-02, -3.8835e-01],\n          [-2.5332e-01, -1.4399e-01, -8.7746e-02,  ...,  2.9873e-01,\n           -1.4090e-01,  1.6136e-01],\n          ...,\n          [ 3.7944e-01,  4.1455e-01,  3.3392e-01,  ..., -2.2405e-01,\n           -4.0171e-01,  1.4341e-01],\n          [-2.0055e-01,  5.0283e-01, -2.5512e-01,  ..., -1.3557e-01,\n           -3.7245e-01, -1.0968e-01],\n          [ 8.1803e-02, -1.1857e+00,  8.5675e-01,  ..., -2.0428e-01,\n            1.8285e-01, -1.6003e-02]],\n\n         [[ 2.5544e-02, -4.4024e-02, -3.5656e-02,  ...,  9.9875e-03,\n            5.0772e-02,  1.3128e-03],\n          [-8.6449e-01, -1.1419e+00,  1.4385e+00,  ..., -2.5004e-01,\n           -1.8690e-01,  6.5207e-01],\n          [-2.7724e-01,  1.3995e-01,  2.3331e-01,  ..., -2.2554e-01,\n            2.5031e-01,  4.4207e-02],\n          ...,\n          [-9.7353e-02, -1.2743e-01,  3.8092e-01,  ..., -1.7040e+00,\n            1.8006e-01,  6.6705e-01],\n          [ 9.8546e-01,  2.8762e-01, -2.3287e-01,  ..., -2.0138e-01,\n           -2.4919e-01,  6.5736e-02],\n          [-1.5538e+00,  9.0598e-01,  6.9267e-01,  ...,  6.6153e-02,\n            3.3056e-01, -3.1213e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m traced_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2-1b-quantized.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:1000\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    994\u001b[0m     check_if_torch_exportable,\n\u001b[1;32m    995\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[1;32m    996\u001b[0m     log_torchscript_usage,\n\u001b[1;32m    997\u001b[0m )\n\u001b[1;32m    999\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1000\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_export\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TS2EPConverter\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:695\u001b[0m, in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages/torch/jit/_trace.py:1275\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of BaseModelOutputWithPast(last_hidden_state=tensor([[[ 1.5326, -1.3593,  2.4385,  ..., -1.0574,  0.4010, -1.1256],\n         [-1.1032,  3.1969,  3.7620,  ..., -5.2792, -4.0093,  2.8944],\n         [ 1.5108,  1.3452, -0.2563,  ..., -5.3894, -6.9593, -0.1786],\n         ...,\n         [ 1.4442,  1.9402,  1.1227,  ..., -3.1643, -6.0105, -2.4108],\n         [ 3.6687,  4.9128,  1.4363,  ..., -3.1012, -4.0978, -0.2268],\n         [-0.4561,  4.2190,  2.2034,  ..., -2.9271, -3.0164, -0.9648]]],\n       grad_fn=<MulBackward0>), past_key_values=((tensor([[[[ 9.5610e-02,  1.7827e-01,  3.6755e-02,  ..., -1.7652e+00,\n            1.7963e+00,  7.9892e-03],\n          [-1.0441e+00, -1.9604e+00, -1.3867e+00,  ...,  6.3562e-01,\n           -2.3245e+00, -2.2302e+00],\n          [-4.0632e+00, -8.6298e-01, -3.0124e+00,  ...,  2.7366e+00,\n           -1.4887e+00, -2.8741e-01],\n          ...,\n          [-1.1385e+00, -3.3855e+00, -2.5555e+00,  ...,  1.7879e+00,\n           -1.7340e+00, -2.0997e+00],\n          [ 3.8586e+00, -3.0442e+00, -1.6802e+00,  ...,  2.6446e+00,\n           -1.3799e+00, -2.4386e+00],\n          [ 6.2070e+00, -9.6388e-02, -1.1381e+00,  ...,  1.5129e+00,\n           -1.4761e+00, -1.9522e+00]],\n\n         [[ 1.3574e-03, -6.1648e-03,  1.7492e-02,  ...,  1.3465e+00,\n           -3.6406e-01,  2.4415e-01],\n          [ 4.1891e+00, -2.6343e+00,  2.6864e+00,  ...,  1.2204e+00,\n            2.0671e+00, -2.0118e+00],\n          [ 4.8540e-01, -4.4151e-01,  4.9244e-01,  ..., -2.1734e-01,\n            1.3597e+00, -7.4817e-01],\n          ...,\n          [-2.6706e+00,  6.8963e-01,  2.3554e+00,  ...,  3.0610e-01,\n            1.7619e+00, -1.2003e+00],\n          [-2.0480e+00,  3.3310e-01,  1.2774e+00,  ..., -3.3540e-01,\n            2.1050e+00, -2.6796e+00],\n          [-1.8765e+00,  2.3173e+00,  6.0927e-01,  ..., -4.6700e-01,\n            1.8176e+00, -2.1536e+00]],\n\n         [[ 9.4520e-03, -1.5944e-02, -2.4548e-02,  ...,  1.3063e+00,\n            2.7019e+00, -1.7912e+00],\n          [ 8.7218e-01, -1.0578e+00, -1.5827e+00,  ...,  3.3310e-02,\n           -3.7639e-01,  4.6344e-01],\n          [-1.4165e-01, -3.0495e-01,  3.4495e-02,  ..., -3.5958e-01,\n           -1.0394e+00,  6.4655e-01],\n          ...,\n          [-5.6807e-01,  8.5252e-01, -7.0232e-01,  ..., -1.0918e-01,\n           -2.6005e+00,  7.6607e-01],\n          [-8.6511e-02,  3.2448e-01, -1.0111e+00,  ..., -1.8479e+00,\n            6.0253e-01,  1.4022e+00],\n          [ 1.2307e+00, -1.5465e-02,  7.3055e-01,  ..., -4.3984e-01,\n           -1.7431e+00, -2.3357e+00]],\n\n         ...,\n\n         [[ 1.8422e-01,  1.7773e-01,  2.1953e-02,  ...,  1.0377e+00,\n           -1.6470e+00,  1.0830e+00],\n          [-2.4002e+00,  1.6295e+00, -1.3280e+00,  ...,  4.8186e-02,\n            1.6535e+00, -4.3970e-01],\n          [ 3.0804e-01, -3.1984e-01, -3.4331e-01,  ...,  1.0190e+00,\n           -1.2367e+00, -1.4422e+00],\n          ...,\n          [ 5.7487e-01, -2.2988e-01, -5.5945e-02,  ..., -1.1135e+00,\n            4.8710e-01, -1.2589e+00],\n          [-9.0519e-01,  5.3804e-01,  8.3072e-01,  ..., -1.3570e+00,\n           -1.9944e-02, -1.3020e+00],\n          [-7.4401e-02, -2.7901e-01,  8.7861e-02,  ..., -1.0235e+00,\n            2.4862e-01, -9.2794e-01]],\n\n         [[-2.6273e-03,  2.6221e-03, -3.0507e-02,  ...,  1.1223e+00,\n            1.1003e+00,  1.8951e-01],\n          [ 4.7903e+00,  3.9686e+00,  1.0754e-01,  ...,  2.1310e-01,\n           -2.8108e-02, -8.7820e-01],\n          [ 1.0246e+00,  1.3096e+00,  1.4995e+00,  ..., -2.0850e+00,\n           -1.6414e-02, -1.2134e-01],\n          ...,\n          [-6.4768e+00, -1.2898e+00, -1.7894e+00,  ..., -3.3122e+00,\n           -6.5641e-01, -2.1036e+00],\n          [-8.0486e-01, -8.0386e-01, -6.5649e-01,  ..., -1.9635e+00,\n            3.5160e-02, -2.1034e-01],\n          [-3.8587e-01, -4.6915e+00, -1.9725e+00,  ..., -4.0251e+00,\n           -1.0439e+00, -2.0460e+00]],\n\n         [[-1.6762e-02,  9.5977e-02, -5.1725e-02,  ...,  1.3930e+00,\n            7.2637e-01, -1.7956e+00],\n          [-9.7841e-01,  3.2913e+00, -1.2614e+00,  ..., -1.5560e+00,\n           -1.1715e+00,  1.6520e+00],\n          [ 1.6378e-01, -3.6365e-01, -1.2429e+00,  ...,  3.4448e-01,\n            1.0371e+00, -4.1897e-01],\n          ...,\n          [-1.8854e-01, -5.7922e-01,  4.0452e-01,  ..., -7.6429e-01,\n           -6.2004e-01,  8.1454e-01],\n          [ 3.4380e-01, -1.2955e+00, -1.9310e+00,  ...,  2.8621e-02,\n            1.0520e+00,  5.6551e-01],\n          [ 1.8523e+00, -2.8961e+00,  1.2598e+00,  ..., -4.3885e-01,\n           -8.9879e-01,  7.6234e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.5795e-03,  1.3599e-02, -4.5031e-02,  ...,  3.4113e-04,\n           -2.8627e-03,  1.1434e-01],\n          [-9.9041e-02, -6.0634e-02, -2.1137e-02,  ...,  3.0084e-01,\n            2.7410e-02, -2.8304e-01],\n          [ 5.0372e-02, -7.0389e-03, -5.9483e-03,  ...,  1.2424e-02,\n            9.2066e-02, -1.9506e-02],\n          ...,\n          [ 9.5086e-02, -2.9229e-04,  5.0430e-03,  ...,  3.8130e-02,\n           -3.3565e-02, -8.2930e-03],\n          [ 2.4362e-02, -2.1318e-02, -3.0878e-02,  ..., -6.8150e-02,\n            8.7928e-02,  7.7262e-02],\n          [ 3.4411e-02, -1.3991e-03, -2.1157e-02,  ..., -3.8938e-02,\n            6.3289e-02, -5.3524e-03]],\n\n         [[ 4.5735e-04,  2.5294e-04, -4.7072e-05,  ..., -4.1792e-04,\n            5.8174e-05, -1.2571e-03],\n          [-6.8077e-02,  4.3200e-02, -1.6258e-01,  ...,  1.6611e-01,\n            1.8847e-01,  1.0549e-01],\n          [-1.6937e-03, -3.3735e-03, -7.3070e-03,  ...,  7.0302e-03,\n           -5.8456e-03,  4.7597e-03],\n          ...,\n          [ 2.0076e-01, -6.8954e-02,  1.4551e-02,  ..., -4.2534e-02,\n            1.5192e-01,  3.5590e-01],\n          [-6.8439e-02, -1.4327e-01,  2.9145e-03,  ...,  2.8026e-02,\n            8.6677e-02, -1.3608e-01],\n          [-1.7211e-01,  4.3724e-02, -4.5069e-02,  ..., -1.0670e-02,\n            4.1900e-02, -3.6702e-02]],\n\n         [[-4.7917e-03, -3.6565e-03,  3.6446e-04,  ..., -3.5328e-04,\n           -6.5906e-04, -9.0832e-04],\n          [-9.4917e-02, -2.8329e-02,  4.3898e-02,  ...,  3.1837e-02,\n           -3.1751e-02, -6.8372e-02],\n          [-1.9278e-02, -3.1923e-02, -7.9513e-03,  ..., -4.0204e-02,\n            1.9414e-03, -1.5249e-02],\n          ...,\n          [ 2.0179e-01, -1.4612e-02,  3.0915e-02,  ...,  7.6138e-03,\n           -8.9133e-02,  4.7802e-02],\n          [-5.1400e-02,  5.7345e-02,  7.4082e-02,  ..., -2.9667e-02,\n           -1.5746e-02,  4.2932e-02],\n          [ 1.4780e-01,  4.6155e-03,  4.9988e-02,  ...,  6.1138e-02,\n            9.1901e-03,  1.6400e-02]],\n\n         ...,\n\n         [[-3.7790e-03,  6.0383e-04, -3.2364e-03,  ..., -5.6062e-04,\n           -1.4380e-04,  4.3967e-04],\n          [ 1.7510e-02, -3.3539e-02, -3.3570e-02,  ..., -1.7791e-02,\n           -4.1592e-03,  1.0626e-02],\n          [-1.4188e-01, -2.0370e-02, -3.2847e-02,  ...,  1.2244e-02,\n            6.5973e-03,  2.4519e-02],\n          ...,\n          [ 2.2899e-02, -7.2733e-02,  5.6826e-02,  ...,  2.8298e-02,\n            2.9018e-02, -1.1067e-03],\n          [ 3.3121e-03,  2.2296e-02, -5.5816e-02,  ...,  2.6916e-02,\n           -3.7854e-02, -2.1149e-02],\n          [ 6.6373e-02, -1.4837e-02, -1.9332e-03,  ..., -4.1016e-02,\n           -2.2104e-02,  1.5020e-02]],\n\n         [[-5.6312e-04, -1.6511e-04,  4.0350e-03,  ...,  8.4011e-05,\n           -1.4644e-03, -8.6666e-04],\n          [ 1.0925e-01,  5.0212e-02,  1.4440e-01,  ...,  6.7587e-03,\n           -1.4874e-02, -1.4358e-01],\n          [-7.5719e-03,  5.1073e-03,  1.8936e-02,  ...,  1.7475e-03,\n           -3.7511e-03,  1.0725e-02],\n          ...,\n          [-6.6414e-02,  5.5474e-02, -1.0623e-01,  ..., -7.9432e-02,\n            4.9748e-02, -1.3237e-01],\n          [ 5.7485e-03,  4.3640e-03, -2.1794e-02,  ...,  6.7570e-03,\n            3.4369e-03,  7.0808e-03],\n          [ 1.9519e-01,  4.5070e-02,  1.5320e-02,  ...,  3.4810e-02,\n           -5.0381e-02, -1.9583e-01]],\n\n         [[ 5.6741e-03,  2.7614e-03,  1.6247e-03,  ...,  2.1552e-05,\n            5.5974e-04, -1.0744e-03],\n          [ 3.1769e-02, -1.8088e-01,  7.9264e-03,  ...,  3.3389e-02,\n           -7.1930e-02,  6.4582e-02],\n          [ 7.2849e-03,  2.0841e-03, -1.0420e-03,  ...,  1.5843e-02,\n           -1.1953e-02,  9.6781e-03],\n          ...,\n          [-1.5549e-01,  1.2683e-01, -1.9559e-02,  ..., -5.6759e-02,\n            3.0501e-04,  7.7877e-02],\n          [ 1.7493e-03,  7.1972e-02,  4.0724e-02,  ..., -1.5790e-01,\n           -8.9693e-02, -8.0710e-02],\n          [-1.0907e-02, -1.9946e-01, -1.1967e-01,  ..., -5.7081e-02,\n           -4.5260e-02,  4.0982e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.7967e-02, -1.5538e-01, -3.5137e-02,  ..., -4.5287e-01,\n           -9.7894e-02,  9.9902e-01],\n          [ 2.1435e+00, -1.1052e+00,  2.9003e-01,  ...,  3.4048e-01,\n           -1.8103e-01, -4.0469e+00],\n          [-1.1528e+00,  8.0551e-02, -3.3165e+00,  ..., -4.6863e-01,\n            1.6778e-02, -3.6705e+00],\n          ...,\n          [-2.0375e+00,  2.7392e+00, -9.9266e-01,  ...,  3.6336e-01,\n            3.0317e-01, -3.8284e+00],\n          [-1.0166e+00,  2.8167e+00, -1.3171e+00,  ..., -1.0825e-01,\n           -2.5986e-01, -3.9744e+00],\n          [ 2.1676e+00,  1.0250e+00,  1.3725e-01,  ..., -1.1101e+00,\n            4.4459e-01, -4.0472e+00]],\n\n         [[ 2.0526e-01, -1.0120e-01,  2.8459e-02,  ...,  8.7780e-01,\n           -3.6544e-01, -9.6916e-01],\n          [ 1.4555e+00, -5.0022e+00, -2.4639e+00,  ..., -1.7220e+00,\n            1.2669e+00,  2.8843e+00],\n          [-1.7542e+00, -3.8467e+00, -3.0021e+00,  ..., -1.2341e+00,\n           -2.3595e+00,  3.0494e+00],\n          ...,\n          [-2.5242e+00, -5.1299e-01, -2.8127e+00,  ..., -3.1357e+00,\n            6.3081e-01,  2.5451e+00],\n          [ 3.9007e+00,  1.7564e+00, -1.6855e+00,  ..., -1.6215e+00,\n            1.2152e-01,  3.5846e+00],\n          [ 3.4854e+00,  3.3406e+00, -2.1783e+00,  ..., -2.6598e+00,\n           -1.6483e+00,  2.4277e+00]],\n\n         [[-2.4163e-02, -1.3793e-02, -1.5605e-02,  ..., -3.3829e-01,\n           -2.1568e-01, -2.7395e-01],\n          [-1.6232e+00, -9.9561e-01, -2.0859e+00,  ...,  7.0393e-01,\n           -1.0576e+00, -9.0824e-01],\n          [-7.1267e-02, -2.0515e+00, -1.4744e+00,  ...,  5.6114e-01,\n            5.6179e-01,  4.5942e-01],\n          ...,\n          [ 2.2636e+00, -3.5167e-03, -1.3527e+00,  ...,  6.2748e-01,\n           -4.1023e-01,  6.6814e-01],\n          [ 7.4718e-01,  8.5385e-01, -5.1241e-01,  ...,  6.5449e-01,\n           -2.8959e-01,  1.1189e+00],\n          [-5.5142e-01,  2.2874e+00,  1.1139e-03,  ..., -6.2647e-01,\n           -1.2708e+00,  1.7367e-01]],\n\n         ...,\n\n         [[ 1.8175e-01, -6.8501e-02,  9.8065e-02,  ...,  5.6903e-01,\n            6.9947e-01,  3.4793e-01],\n          [ 1.2064e+00, -1.8463e-01,  2.7027e+00,  ..., -5.7690e+00,\n           -3.5259e+00, -2.6167e+00],\n          [-4.0773e+00,  1.5611e+00,  1.7085e+00,  ..., -1.9952e+00,\n           -4.0612e+00, -4.2319e+00],\n          ...,\n          [-2.2839e+00,  3.9910e+00, -5.4121e-01,  ..., -4.6933e+00,\n           -5.7405e+00, -2.9257e+00],\n          [ 2.5067e+00,  1.7773e+00, -1.2392e+00,  ..., -6.0192e+00,\n           -5.4825e+00, -4.3812e+00],\n          [ 5.1151e+00, -6.7553e-02, -2.5948e+00,  ..., -3.0317e+00,\n           -4.5868e+00,  2.0757e+00]],\n\n         [[ 2.4641e-01, -7.5120e-04,  1.4931e-01,  ...,  4.2755e-02,\n            2.9609e-01, -5.9467e-01],\n          [ 2.2968e+00, -1.0072e+00,  2.1115e+00,  ..., -2.2388e+00,\n            1.3980e+00,  2.3438e+00],\n          [-2.3381e+00, -3.5296e+00,  1.1245e+00,  ...,  1.0777e+00,\n            8.0039e-02,  3.0540e+00],\n          ...,\n          [-4.5341e+00, -1.4650e+00, -1.5961e+00,  ...,  1.7733e-01,\n           -1.4686e+00,  4.1432e+00],\n          [ 7.0741e-01,  1.2569e+00, -2.8351e+00,  ..., -1.0399e+00,\n           -7.8148e-01,  3.2712e+00],\n          [ 6.3685e+00,  2.0896e+00, -4.8673e+00,  ...,  7.3676e-01,\n           -1.1852e+00,  4.6863e+00]],\n\n         [[ 6.5294e-02,  5.4152e-03,  9.5116e-02,  ..., -2.6992e-01,\n            1.3247e-01, -1.0223e+00],\n          [ 1.0522e+00,  1.3931e+00,  1.9404e+00,  ..., -1.5272e+00,\n            1.4662e+00,  1.2632e+00],\n          [ 1.4162e-01,  8.8900e-01,  3.2562e-01,  ...,  4.7265e-01,\n            1.1191e+00,  1.5841e+00],\n          ...,\n          [-9.9284e-01,  5.0924e-01, -1.1408e+00,  ...,  2.1557e+00,\n           -1.8703e-01,  7.6806e-01],\n          [ 1.0320e-01,  1.1855e-01, -1.0493e+00,  ...,  2.0507e+00,\n            1.0512e+00,  7.7381e-01],\n          [ 1.9356e+00, -2.4486e+00, -6.1142e-01,  ..., -7.0353e-01,\n           -8.7412e-01,  8.5476e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-3.2377e-03,  3.6782e-03, -2.0835e-03,  ...,  2.1884e-03,\n           -1.7065e-03, -1.4043e-02],\n          [-6.8249e-02, -9.0891e-02,  7.4523e-02,  ...,  4.0324e-02,\n            1.1355e-01,  8.2261e-02],\n          [-1.2549e-01,  8.1891e-02, -1.0486e-01,  ..., -2.0578e-02,\n           -8.8899e-02,  1.4754e-01],\n          ...,\n          [ 3.4477e-01,  2.5028e-02, -1.5197e-02,  ..., -1.4269e-01,\n            2.6218e-01,  6.2984e-02],\n          [-1.9109e-01, -1.9266e-02,  1.1804e-01,  ..., -3.0260e-01,\n            2.0018e-01,  3.0816e-01],\n          [-1.1212e-01, -8.4712e-02, -8.4004e-03,  ..., -1.1759e-01,\n            6.7740e-02,  6.6850e-02]],\n\n         [[ 4.1326e-04,  1.7973e-03, -3.2491e-03,  ..., -1.8416e-03,\n           -2.1346e-03, -5.6098e-03],\n          [ 7.9583e-02, -1.7815e-01, -5.7443e-03,  ...,  2.3471e-01,\n            1.9975e-01,  7.2629e-02],\n          [ 2.4855e-02,  1.8370e-01, -4.9052e-02,  ..., -9.4083e-02,\n           -9.4262e-03, -1.7727e-02],\n          ...,\n          [ 2.2283e-01,  5.1064e-02,  8.6225e-02,  ..., -5.5915e-02,\n            3.2762e-01,  1.3147e-01],\n          [-6.8345e-02,  6.0235e-02,  1.0391e-01,  ...,  4.2214e-02,\n            2.4259e-02, -2.2577e-01],\n          [-1.5042e-02,  8.3515e-02, -3.7921e-02,  ..., -2.3938e-01,\n            1.6055e-01, -5.0694e-02]],\n\n         [[ 7.5014e-03, -3.1590e-03,  1.1861e-02,  ...,  9.1303e-03,\n            8.1564e-03, -1.5911e-01],\n          [ 6.1164e-02, -7.1458e-03, -6.6609e-02,  ...,  7.1957e-02,\n           -2.3087e-02,  5.9988e-01],\n          [-2.5318e-01, -1.0410e-01, -5.7920e-02,  ...,  2.6127e-01,\n            3.5043e-02,  8.3340e-01],\n          ...,\n          [-3.3453e-02,  1.8735e-01,  6.7241e-02,  ...,  2.0223e-01,\n            1.4901e-01,  5.9556e-01],\n          [ 1.0063e-01,  1.1752e-01,  4.5007e-01,  ...,  6.4598e-02,\n            1.2316e-01,  3.3600e-01],\n          [ 7.8331e-02, -7.1940e-02, -2.5699e-01,  ...,  1.2051e-02,\n            2.1061e-01,  4.6554e-01]],\n\n         ...,\n\n         [[-8.6541e-03, -3.8981e-03, -2.6316e-02,  ...,  1.9426e-02,\n            1.4738e-03,  3.2282e-03],\n          [-5.9135e-02, -9.9246e-02, -3.6900e-01,  ...,  1.0190e-01,\n            2.3477e-01, -3.7322e-03],\n          [ 6.5881e-02, -9.7573e-02, -9.5627e-02,  ...,  9.0878e-02,\n           -1.9417e-02, -1.0951e-02],\n          ...,\n          [-1.9661e-01,  1.1670e-01, -2.6479e-02,  ...,  1.9701e-02,\n            2.5399e-02, -9.7587e-02],\n          [-1.2173e-01, -1.6699e-01,  9.9858e-02,  ...,  5.7304e-02,\n           -4.9886e-02, -4.7641e-02],\n          [ 1.9774e-01,  1.5135e-02, -5.3215e-02,  ...,  8.8475e-02,\n            1.5693e-01, -9.6357e-02]],\n\n         [[ 5.1806e-03, -5.2374e-03, -2.2343e-03,  ..., -1.5930e-03,\n            3.5463e-03, -4.3412e-03],\n          [-5.9287e-02,  7.1173e-02, -2.3751e-02,  ..., -2.0564e-01,\n            2.2858e-01,  3.2094e-01],\n          [ 8.5236e-02,  6.9737e-02,  6.1274e-02,  ..., -1.8012e-01,\n           -3.3260e-02,  2.3315e-02],\n          ...,\n          [ 1.1825e-02, -5.9067e-02, -1.0706e-01,  ..., -1.6356e-01,\n           -3.4370e-01,  2.4248e-02],\n          [ 4.9969e-02,  1.4826e-01, -3.9919e-02,  ..., -4.0458e-02,\n            6.6567e-02, -8.3417e-02],\n          [ 1.0430e-01, -3.2007e-02, -1.4930e-01,  ...,  3.6746e-01,\n            1.4962e-01,  1.6755e-01]],\n\n         [[-5.6988e-04, -8.3401e-03, -8.2436e-03,  ..., -1.5176e-02,\n            7.5245e-03, -3.7639e-03],\n          [ 1.8856e-01,  2.7176e-01, -1.5545e-01,  ..., -1.4014e-02,\n           -3.9068e-02,  1.3135e-01],\n          [-5.3303e-02,  1.8317e-02,  3.4295e-02,  ..., -9.3917e-02,\n            6.7584e-02,  3.9990e-02],\n          ...,\n          [-1.3570e-01,  1.3095e-01,  1.1496e-01,  ..., -5.9814e-01,\n            9.0253e-02,  1.7751e-01],\n          [ 3.8223e-02,  3.8875e-02, -1.8843e-01,  ..., -9.1675e-02,\n            5.0060e-02, -2.1880e-02],\n          [-4.1986e-01,  2.1757e-01, -1.4327e-01,  ...,  7.7314e-02,\n            9.4747e-02,  8.5389e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7701e-01,  1.2814e-01, -5.6786e-02,  ...,  1.3507e-01,\n           -1.4585e+00, -6.9457e-01],\n          [ 2.6764e-01,  2.9066e+00, -3.2170e+00,  ...,  8.7905e-02,\n            4.8198e+00,  2.8236e+00],\n          [ 9.2695e-01,  1.4156e+00, -7.8957e-01,  ...,  1.7659e-01,\n            3.0773e+00,  9.0030e-01],\n          ...,\n          [-2.9926e-01,  3.5457e-02, -1.4828e+00,  ..., -1.5730e-01,\n            4.5621e+00,  9.8181e-01],\n          [-6.7563e-01, -2.2247e+00, -1.5221e+00,  ...,  5.3594e-01,\n            4.2234e+00,  1.0064e+00],\n          [-2.3657e+00, -1.6536e+00, -1.2066e+00,  ..., -4.5254e-01,\n            5.0909e+00,  1.6524e+00]],\n\n         [[-6.3640e-02, -1.6707e-02,  1.4471e-02,  ...,  4.7435e-01,\n           -3.9084e-01, -2.5429e-01],\n          [ 1.1823e+00,  5.1502e-01, -4.1835e-01,  ...,  1.1009e-01,\n           -2.9313e-01, -8.9718e-01],\n          [ 1.3956e+00,  1.6913e+00, -8.9101e-01,  ...,  8.5906e-02,\n            7.4851e-01, -8.5068e-01],\n          ...,\n          [-1.0720e+00,  1.5869e+00, -1.4292e+00,  ...,  4.5084e-01,\n           -3.9096e-01, -2.0741e+00],\n          [-1.3015e+00, -2.1746e-01, -1.5711e+00,  ...,  8.3230e-01,\n            9.6939e-01, -7.3736e-01],\n          [-1.5033e+00, -8.2268e-01, -7.3206e-01,  ...,  1.1917e+00,\n           -1.2171e+00, -1.7185e+00]],\n\n         [[ 8.8415e-02, -7.7824e-02,  5.8160e-02,  ...,  3.3606e-01,\n           -1.3101e+00, -1.1014e+00],\n          [-2.3669e+00, -3.6616e+00, -1.2765e+00,  ...,  2.0766e+00,\n            3.3702e+00,  5.0247e+00],\n          [-3.1251e+00, -3.1575e+00, -7.4505e-01,  ...,  4.9128e-01,\n            1.8747e+00,  4.4647e+00],\n          ...,\n          [ 2.0000e+00, -6.1031e-01, -1.6045e+00,  ...,  7.2175e-01,\n            3.4087e+00,  3.7380e+00],\n          [ 3.2028e+00,  2.2939e+00, -4.2439e+00,  ...,  4.8598e-01,\n            2.9832e+00,  3.1867e+00],\n          [ 2.3772e+00,  2.5177e+00, -1.3139e+00,  ...,  1.4361e+00,\n            3.7860e+00,  4.9523e+00]],\n\n         ...,\n\n         [[-2.5395e-01, -1.4475e-01,  1.7906e-01,  ..., -1.9359e-01,\n           -3.9308e-01,  8.8821e-01],\n          [-2.6796e+00, -2.0157e+00,  3.1024e+00,  ...,  4.2189e-01,\n           -1.3306e+00, -4.9016e+00],\n          [ 1.6348e+00, -5.3055e-02,  1.9740e+00,  ..., -1.4344e+00,\n           -2.4934e-01, -3.7893e+00],\n          ...,\n          [ 2.8807e+00,  1.4479e+00, -1.4614e-03,  ..., -1.0776e+00,\n            2.9119e-01, -3.3255e+00],\n          [-1.1927e+00,  1.2245e+00, -1.0029e+00,  ...,  5.8190e-01,\n            1.8443e+00, -3.6302e+00],\n          [-3.7693e+00,  2.5642e+00, -1.8331e+00,  ..., -1.2635e+00,\n           -5.5469e-02, -3.2170e+00]],\n\n         [[ 1.0696e-02, -8.8757e-02, -1.3791e-01,  ..., -5.1921e-01,\n            2.0680e-01, -9.1169e-02],\n          [ 6.0183e-01, -8.7852e-01, -2.0071e+00,  ...,  3.2347e-01,\n            6.0076e-01, -6.7956e-01],\n          [ 1.1107e+00, -1.4100e-01, -1.3417e+00,  ...,  2.6395e+00,\n            1.8479e+00,  1.0224e+00],\n          ...,\n          [-1.5660e+00,  8.7282e-01,  9.3701e-01,  ...,  1.8857e+00,\n            8.2834e-01,  2.4639e-01],\n          [-1.4213e+00,  1.9002e+00,  7.0173e-01,  ...,  1.7888e+00,\n            1.6171e+00, -8.0769e-01],\n          [-1.2590e+00,  1.6183e+00,  1.4074e+00,  ...,  8.6553e-01,\n           -4.7060e-01,  5.8349e-01]],\n\n         [[ 7.1884e-02,  1.4028e-01,  6.8085e-02,  ...,  2.4312e-01,\n           -1.7492e-01,  2.5649e-01],\n          [ 5.1536e+00,  3.8639e+00,  2.2726e+00,  ...,  4.6500e-01,\n           -2.1876e+00, -8.3482e-01],\n          [ 2.9126e+00,  3.1976e+00,  3.2291e+00,  ...,  7.0219e-01,\n           -1.1357e-01,  1.1334e+00],\n          ...,\n          [-4.1209e+00,  2.7656e-01,  2.6617e+00,  ..., -3.8955e-01,\n           -2.3895e-01,  3.2122e+00],\n          [-2.8110e+00,  3.8436e-01,  1.9234e+00,  ..., -1.6735e+00,\n            7.6658e-01,  1.5494e+00],\n          [-1.6525e+00, -1.5763e+00,  6.6924e-01,  ..., -1.4195e+00,\n           -1.4014e+00,  4.7321e-02]]]], grad_fn=<AddBackward0>), tensor([[[[-6.1820e-03,  5.4064e-03,  8.8012e-03,  ...,  1.4157e-02,\n            5.4133e-03, -9.3104e-03],\n          [ 2.9941e-01,  6.7903e-01, -1.6327e-01,  ...,  8.3898e-02,\n            2.1157e-01, -7.8838e-02],\n          [ 3.1184e-01,  7.6078e-02,  3.5302e-01,  ..., -2.8107e-01,\n           -1.5467e-01,  4.1138e-02],\n          ...,\n          [-9.6306e-02,  6.9173e-02,  6.5667e-02,  ...,  2.6577e-01,\n           -6.4739e-02, -3.4857e-02],\n          [-2.4713e-01, -3.5561e-01, -1.4759e-01,  ...,  1.7052e-01,\n            7.5728e-02, -1.5410e-01],\n          [-2.3465e-01,  1.5675e-01,  2.8819e-01,  ...,  1.2776e-01,\n            6.8719e-02, -2.5834e-01]],\n\n         [[-1.2713e-02,  1.2328e-01,  2.5158e-03,  ..., -2.7015e-03,\n            9.6137e-03,  1.6953e-03],\n          [ 7.1951e-01, -1.6914e-01, -1.1247e-01,  ..., -4.8129e-01,\n            1.4451e-01, -6.2263e-02],\n          [-2.4158e-01, -5.0414e-02,  1.1081e-01,  ...,  4.6947e-02,\n           -1.9938e-01,  7.7349e-02],\n          ...,\n          [-1.9011e-01, -1.5957e-01,  1.9725e-01,  ...,  2.3459e-01,\n            1.6987e-01,  1.8029e-01],\n          [-3.1155e-01, -3.1968e-01, -2.1609e-01,  ...,  2.5294e-01,\n            6.0543e-02,  1.5184e-01],\n          [ 1.5025e-01,  1.5711e-01, -1.4395e-02,  ..., -1.5603e-01,\n            6.3068e-02,  3.9409e-01]],\n\n         [[ 7.1219e-03,  3.1417e-03, -1.0653e-02,  ...,  1.6981e-02,\n           -7.8259e-03,  4.3196e-03],\n          [-1.9960e-01,  2.7524e-01,  1.9772e-02,  ..., -1.6835e-01,\n            8.5904e-02, -2.8022e-01],\n          [-1.0467e-01,  3.5048e-02,  1.1350e-02,  ...,  5.4866e-02,\n           -1.1864e-01,  2.1198e-01],\n          ...,\n          [-1.8213e-01,  9.0273e-03, -1.1572e-01,  ..., -9.9041e-02,\n            8.4770e-02,  4.1519e-01],\n          [-1.2092e-01, -3.8780e-01, -3.6987e-01,  ..., -3.8170e-01,\n           -2.6693e-02,  3.6690e-01],\n          [-5.4785e-02, -3.9241e-01, -1.1562e-01,  ...,  4.1272e-01,\n            3.9609e-01,  1.8474e-01]],\n\n         ...,\n\n         [[ 3.3089e-02,  1.1034e-03, -6.1658e-03,  ..., -2.2012e-01,\n            2.6411e-03, -9.0748e-04],\n          [ 2.0938e-01, -6.8443e-01, -3.5323e-01,  ...,  1.0073e+00,\n            2.5970e-01,  2.7422e-01],\n          [-9.7218e-01, -6.1478e-01,  2.2379e-01,  ...,  1.2794e+00,\n            1.3615e-01, -8.1561e-01],\n          ...,\n          [-3.2407e-01, -5.6607e-01,  1.0611e-01,  ...,  1.0359e+00,\n           -1.1281e-01, -1.8238e-01],\n          [-2.6283e-01,  2.3867e-02, -5.1711e-01,  ...,  1.1837e+00,\n            1.3125e-01,  1.7411e-01],\n          [-3.9711e-01, -2.7209e-01, -1.6776e-01,  ...,  8.4565e-01,\n           -7.8791e-02,  3.2866e-02]],\n\n         [[ 5.5222e-03,  5.5187e-03, -2.4976e-03,  ..., -1.4439e-02,\n            2.8718e-03, -1.2763e-02],\n          [-9.6075e-02, -8.6414e-02,  7.9327e-02,  ...,  3.3177e-02,\n            5.1969e-02, -1.4187e-01],\n          [-7.6938e-02, -2.0906e-01,  1.4066e-01,  ..., -1.1291e-01,\n            1.5014e-01, -7.1458e-02],\n          ...,\n          [ 1.4898e-02, -6.8294e-02, -7.0356e-02,  ...,  2.5958e-02,\n           -4.2350e-02,  7.3146e-02],\n          [-1.7195e-01, -3.7946e-01, -6.4187e-02,  ...,  1.7663e-02,\n           -1.0420e-02,  1.2907e-01],\n          [-1.8034e-01,  2.7801e-03,  4.4255e-01,  ..., -2.3100e-01,\n           -4.6818e-02,  2.5140e-01]],\n\n         [[ 9.3314e-03, -1.0290e-02, -8.7502e-03,  ...,  1.6052e-03,\n            4.6391e-03, -5.5440e-04],\n          [ 3.4664e-01, -8.4540e-01,  1.8082e-01,  ..., -1.9415e-01,\n            1.3139e-01,  3.4814e-02],\n          [ 6.2430e-01,  1.2965e-01,  3.7513e-02,  ..., -2.3402e-02,\n            5.5743e-02,  9.9857e-02],\n          ...,\n          [ 1.9298e-01, -3.6481e-01,  2.9809e-02,  ...,  2.0951e-02,\n            6.1376e-02, -8.9263e-02],\n          [-1.6985e-01,  3.1712e-01, -2.7905e-01,  ..., -2.4076e-01,\n            5.8419e-02, -1.1336e-01],\n          [-1.5146e-01, -2.7222e-01,  1.6510e-03,  ..., -6.8705e-03,\n           -4.0594e-01,  1.8751e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.2282e-02, -1.0165e-01, -1.1627e-01,  ..., -1.1198e+00,\n            1.2367e+00,  9.2278e-01],\n          [ 2.6947e+00, -1.1263e+00, -2.5378e+00,  ...,  3.8508e-01,\n            7.6976e-01, -9.4456e-01],\n          [ 1.3896e+00, -5.4602e-01, -1.3699e+00,  ..., -3.5502e-01,\n            9.4520e-01, -1.5680e+00],\n          ...,\n          [-1.7908e+00,  9.5140e-01, -1.4063e+00,  ..., -2.5337e-01,\n            6.5877e-02, -5.4208e-01],\n          [-7.0496e-01,  1.6231e-01, -5.4211e-01,  ..., -6.9233e-01,\n            1.3365e+00, -2.8431e+00],\n          [-2.0937e-01, -1.1077e-01, -5.4180e-01,  ..., -4.4429e-01,\n            6.7587e-01, -9.7499e-01]],\n\n         [[ 2.0206e-01,  1.7876e-01,  2.1073e-01,  ..., -1.2444e-01,\n            6.5449e-01,  1.8805e-01],\n          [ 5.5494e+00,  2.7431e-01,  2.8697e+00,  ...,  7.7296e-01,\n           -4.0696e-01,  8.3838e-01],\n          [ 1.1737e+00,  3.4204e-03,  2.3668e+00,  ...,  3.9985e-01,\n            9.8103e-01, -7.5665e-01],\n          ...,\n          [-3.2539e+00, -3.5224e+00,  2.2123e+00,  ..., -1.0414e+00,\n           -5.9234e-01,  1.4053e+00],\n          [-8.2012e-01, -4.5371e+00,  3.9510e-01,  ...,  1.8237e+00,\n            1.3257e+00,  1.9354e+00],\n          [ 1.8927e+00, -2.3753e+00, -4.3274e-01,  ..., -7.4800e-01,\n           -4.4143e-01,  1.1267e+00]],\n\n         [[ 2.1973e-02,  7.3497e-02, -5.5480e-02,  ..., -1.8062e-01,\n            1.7539e+00, -1.5784e+00],\n          [ 3.3127e+00,  2.5819e+00, -2.4716e+00,  ...,  1.6049e+00,\n           -5.4206e+00,  6.3410e+00],\n          [ 2.7562e+00,  2.5899e+00, -3.3643e+00,  ..., -1.7875e-01,\n           -4.6014e+00,  6.0351e+00],\n          ...,\n          [-2.7383e+00,  8.6641e-01, -4.0272e+00,  ...,  6.0972e-01,\n           -5.1756e+00,  5.1306e+00],\n          [-2.2211e+00,  1.1909e+00, -2.4088e+00,  ...,  3.1355e-01,\n           -5.1743e+00,  4.1928e+00],\n          [-1.0697e+00, -1.9966e+00, -1.2744e+00,  ..., -1.0002e-01,\n           -5.1887e+00,  5.8612e+00]],\n\n         ...,\n\n         [[ 1.1266e-01, -1.1705e-01,  7.7527e-02,  ..., -5.3134e-01,\n           -1.2289e+00,  2.8861e-01],\n          [ 3.4821e-01, -2.3385e+00,  1.6136e+00,  ..., -1.4544e+00,\n            4.0061e-01, -3.1225e-01],\n          [-1.1731e+00, -4.6898e-02,  1.0132e+00,  ..., -1.2839e+00,\n           -1.2630e-01,  9.5137e-01],\n          ...,\n          [-2.0203e-01,  1.2111e+00, -1.6005e-02,  ..., -5.1681e-01,\n           -3.3450e-01, -2.1627e-01],\n          [ 1.1841e+00, -2.7654e-01,  1.8011e-01,  ..., -5.9574e-01,\n           -6.3698e-01,  3.0269e-01],\n          [ 1.0746e+00,  7.7070e-01, -9.6412e-01,  ..., -1.0072e+00,\n           -7.1167e-02, -1.6784e-01]],\n\n         [[-2.3820e-01, -1.1915e-01, -1.3883e-01,  ...,  1.2986e+00,\n           -1.2263e+00,  1.8063e+00],\n          [-1.6037e+00, -1.0995e+00, -2.6342e+00,  ..., -1.7757e+00,\n           -1.0154e+00, -2.1865e+00],\n          [ 1.9105e+00,  8.1207e-01, -7.8228e-01,  ..., -1.4002e+00,\n           -1.6378e+00, -2.3000e+00],\n          ...,\n          [ 9.5449e-01,  1.6893e+00,  1.3387e+00,  ..., -9.9793e-01,\n            3.8434e-01, -8.7498e-01],\n          [-1.4753e+00,  2.2700e+00,  1.4921e+00,  ...,  2.1434e+00,\n            1.2297e+00, -9.2150e-01],\n          [-2.7791e+00,  3.6354e-01,  2.2254e+00,  ..., -2.0619e+00,\n           -1.6319e+00, -2.4281e+00]],\n\n         [[ 1.3673e-01,  1.1688e-01,  1.1799e-01,  ..., -7.8946e-01,\n           -2.1710e+00,  1.2962e+00],\n          [ 1.4964e+00,  3.5646e+00,  6.2662e-01,  ..., -6.0104e-01,\n            4.9614e+00, -1.0345e-01],\n          [-9.5273e-01,  8.1729e-01,  6.4827e-01,  ...,  9.7054e-02,\n            7.1365e+00,  5.5422e-01],\n          ...,\n          [-7.5994e-01, -3.1559e-01,  3.6393e-01,  ..., -2.4653e-01,\n            5.8504e+00, -1.6054e-01],\n          [ 6.1265e-01, -4.1892e-01,  2.2466e-01,  ..., -3.9562e-01,\n            6.2402e+00,  2.3792e-01],\n          [ 1.0237e+00, -1.5125e+00, -1.0386e-02,  ...,  2.2863e-01,\n            5.2406e+00, -2.2171e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.4321e-03, -8.7532e-03, -4.7245e-03,  ...,  8.1068e-03,\n            8.7612e-04,  6.9124e-03],\n          [-1.0467e-01, -1.9611e-02,  2.4791e-02,  ..., -4.3568e-02,\n           -5.7890e-02,  2.3274e-01],\n          [-9.9035e-02, -3.5960e-01,  1.4438e-01,  ..., -2.2842e-02,\n            1.6327e-01,  3.1524e-01],\n          ...,\n          [-3.0338e-01, -7.6661e-03,  4.5093e-01,  ...,  1.9181e-01,\n            2.8696e-01, -9.1073e-02],\n          [-3.0866e-02, -6.2382e-02,  2.9084e-02,  ..., -1.5676e-01,\n           -1.7092e-01,  1.1898e-01],\n          [-2.6783e-01, -1.7185e-01,  1.7530e-01,  ...,  1.3997e-01,\n            9.0545e-02, -3.8462e-01]],\n\n         [[ 7.0619e-04, -3.2205e-03,  8.7327e-03,  ...,  1.4436e-03,\n            2.0123e-02,  9.0113e-04],\n          [ 1.2734e-02, -4.0825e-02,  4.3852e-03,  ..., -1.2760e-02,\n           -5.4517e-01, -2.6769e-01],\n          [-1.6712e-01, -1.6344e-01,  9.6333e-02,  ..., -6.8344e-03,\n           -2.1898e-02, -3.1479e-01],\n          ...,\n          [ 3.0523e-01,  2.7330e-01, -6.8375e-03,  ..., -1.5098e-01,\n           -1.0993e-01,  3.0583e-01],\n          [ 3.4018e-01, -7.7130e-02, -1.4879e-01,  ..., -3.0749e-02,\n            5.6093e-01,  1.9593e-01],\n          [ 1.0211e-01,  1.3157e-01, -1.3349e-01,  ...,  1.2502e-01,\n           -6.5941e-02, -1.1809e-01]],\n\n         [[-2.7866e-03,  1.0993e-02,  6.1155e-03,  ...,  1.5371e-02,\n            2.8720e-03, -2.3152e-02],\n          [-6.4073e-02,  1.5430e-02,  2.6343e-01,  ...,  8.0253e-02,\n            1.7867e-01,  1.2398e+00],\n          [ 5.1941e-02,  1.3583e-01, -1.6573e-01,  ..., -8.3535e-02,\n            1.3651e-01,  1.5384e-01],\n          ...,\n          [-1.2322e-01,  1.8373e-01, -8.2243e-02,  ...,  1.0742e-01,\n            3.1176e-01,  3.3107e-01],\n          [-1.7566e-02,  1.9896e-01, -1.6854e-02,  ...,  7.8815e-02,\n            2.5707e-01,  1.6920e-02],\n          [ 7.9629e-03,  1.1664e-02, -1.9343e-01,  ..., -2.2412e-02,\n            2.4916e-01,  3.5274e-01]],\n\n         ...,\n\n         [[-1.4118e-02, -3.8184e-03,  1.3744e-02,  ..., -4.0826e-04,\n           -4.0499e-03,  8.0336e-03],\n          [ 5.4029e-02, -1.0008e-01, -3.6924e-01,  ..., -5.0542e-01,\n           -1.2258e-01, -1.0312e-01],\n          [ 2.2759e-02, -7.9112e-02, -9.1752e-02,  ..., -3.1552e-01,\n           -1.4367e-01,  7.4229e-02],\n          ...,\n          [-2.3685e-01, -5.3028e-01, -7.6086e-02,  ..., -9.8673e-02,\n           -1.3463e-02,  2.3224e-01],\n          [-4.5401e-01,  1.1683e-01, -2.2798e-02,  ..., -6.0475e-01,\n           -3.5990e-01, -1.3324e-01],\n          [-1.8999e-01,  7.5736e-02,  2.8255e-02,  ..., -1.2506e-01,\n           -1.9876e-01,  2.6780e-01]],\n\n         [[-3.5600e-03,  2.4092e-04, -9.2962e-03,  ..., -1.4673e-02,\n            4.7458e-03, -1.1192e-02],\n          [-1.5482e-02,  1.1980e-01,  2.1448e-01,  ...,  6.4581e-02,\n           -1.7597e-01,  1.1077e-01],\n          [ 2.6037e-02,  7.4305e-02, -1.0224e-01,  ..., -6.9476e-02,\n           -9.4906e-02, -2.6536e-01],\n          ...,\n          [ 1.0273e-01,  8.0308e-02,  1.1856e-01,  ...,  1.3968e-01,\n            1.5410e-01, -1.1429e-01],\n          [ 1.1800e-01,  5.7852e-02,  1.4395e-01,  ...,  1.1984e-01,\n            1.3060e-01, -9.2328e-02],\n          [-1.5998e-01, -8.2837e-02,  8.2714e-02,  ..., -3.5956e-02,\n           -9.6886e-02, -8.8186e-02]],\n\n         [[ 5.8911e-03, -8.2265e-03,  5.3291e-03,  ...,  8.7417e-04,\n           -1.3503e-02, -1.0993e-02],\n          [ 2.8653e-03,  4.0489e-03, -2.4093e-01,  ..., -1.3937e-01,\n            2.8739e-01,  5.5181e-01],\n          [ 1.2024e-01, -1.2629e-02,  2.7387e-02,  ...,  1.7477e-01,\n            1.2676e-01,  1.6036e-01],\n          ...,\n          [ 5.3377e-02,  2.0845e-01,  4.0209e-03,  ...,  7.6788e-02,\n            5.8192e-01,  2.6301e-01],\n          [ 3.5664e-01, -8.8840e-02, -1.1054e-01,  ...,  7.6085e-03,\n            6.6291e-01, -1.2942e-01],\n          [ 2.7074e-02,  1.2920e-01, -1.4235e-01,  ...,  3.5492e-01,\n            7.7556e-02,  6.1752e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0673, -0.0380,  0.0587,  ..., -0.2401,  1.2458, -0.5890],\n          [-2.6230, -1.4096,  1.0336,  ..., -1.0997,  0.7870, -2.2670],\n          [ 0.0684, -0.8596, -0.4222,  ...,  0.0686,  1.8364, -1.7942],\n          ...,\n          [ 0.4360,  0.0993, -0.3778,  ...,  0.2330,  0.4170, -1.8798],\n          [-0.4057,  0.0715, -0.6293,  ...,  0.2380,  1.6907, -3.2166],\n          [-0.4575,  0.9622, -0.8712,  ...,  0.7507,  0.7713, -1.9774]],\n\n         [[-0.1482,  0.1215, -0.1029,  ..., -1.0353, -1.0146,  0.8210],\n          [-2.7964,  1.4028, -2.8288,  ..., -0.8176, -0.8423,  1.3311],\n          [ 1.2582,  0.1732, -1.6765,  ..., -0.2601, -0.9366,  1.1228],\n          ...,\n          [ 1.3964, -1.2859,  0.7055,  ...,  0.5220, -1.2482, -0.1308],\n          [-1.7408, -1.5983,  0.9078,  ...,  0.0471, -0.5481,  0.2924],\n          [-1.7288, -1.0642,  1.4368,  ...,  0.5570, -1.4549, -0.8904]],\n\n         [[ 0.0888, -0.0451, -0.0820,  ...,  1.2527, -1.6093,  0.8153],\n          [ 1.1792, -1.2164, -3.3174,  ...,  1.8395,  2.3515, -0.6136],\n          [-1.5866,  0.1771, -1.9733,  ...,  1.0520,  2.0242, -0.3927],\n          ...,\n          [-1.0689,  1.1365, -0.1952,  ...,  0.5337,  1.3976, -0.5168],\n          [ 0.3324,  1.2364, -0.5633,  ...,  0.5687,  2.1237, -0.3122],\n          [ 2.0921,  0.1781,  0.9043,  ...,  1.3126,  1.3001,  0.6655]],\n\n         ...,\n\n         [[ 0.1052, -0.0565,  0.0683,  ..., -0.1388,  0.3983,  1.4133],\n          [ 3.4661, -1.0068,  1.4672,  ..., -0.7612, -0.6297, -3.9924],\n          [ 0.2750,  0.7824,  0.9173,  ..., -0.4139,  0.6396, -3.5821],\n          ...,\n          [-1.0323,  0.2570, -0.7744,  ..., -0.0266,  0.3943, -1.8256],\n          [ 0.6397,  0.2168, -0.2871,  ..., -0.5229,  0.9866, -2.3851],\n          [ 0.4325,  0.0755, -0.7504,  ..., -0.3195,  1.3672, -1.1089]],\n\n         [[ 0.1472,  0.0827,  0.0479,  ...,  0.3101,  0.0898, -0.3013],\n          [ 1.7290,  1.5935,  0.6793,  ...,  0.7582,  1.3635,  0.5977],\n          [-0.9900,  0.1284,  1.2406,  ...,  0.2236, -0.4528, -1.5582],\n          ...,\n          [-0.7661, -0.5840,  2.1097,  ..., -0.0846,  0.3760,  0.1993],\n          [ 0.5308, -1.5368,  1.5667,  ..., -0.7289, -0.2877, -1.1430],\n          [ 1.6712, -0.8101,  1.7596,  ...,  1.8957,  0.6416,  0.1783]],\n\n         [[-0.1321,  0.1387, -0.0576,  ..., -0.8415,  1.0974,  0.2400],\n          [ 0.5177,  3.2533, -1.6478,  ..., -0.1231, -0.1318,  1.0980],\n          [ 2.1462,  0.3267, -2.0895,  ..., -0.5722,  0.1243,  0.8788],\n          ...,\n          [ 0.0263, -0.7484, -1.8913,  ..., -1.8921, -0.5946,  1.4830],\n          [-1.3986, -0.6276, -1.3345,  ..., -1.0882,  1.3606,  1.5931],\n          [-1.7844, -1.3445, -0.5183,  ..., -0.7938, -0.0828,  1.6731]]]],\n       grad_fn=<AddBackward0>), tensor([[[[ 7.0184e-03, -2.6607e-01,  8.9918e-03,  ...,  1.0008e-02,\n            9.8950e-03,  2.1153e-02],\n          [-1.7189e-02,  7.4637e-01,  7.3324e-02,  ...,  1.7711e-01,\n            1.8271e-01, -1.9652e-01],\n          [ 4.3036e-01,  4.6241e-01, -2.1777e-02,  ..., -5.7014e-02,\n           -4.1061e-01, -3.6297e-01],\n          ...,\n          [ 1.1786e-01,  3.7584e-01, -1.6500e-01,  ...,  4.3060e-01,\n            1.1172e-01, -3.4991e-01],\n          [ 1.5505e-01,  6.0234e-01, -2.4335e-03,  ..., -1.4892e-01,\n            1.3472e-01, -1.7865e-01],\n          [ 8.8841e-02,  5.1718e-01, -3.3933e-01,  ...,  2.1001e-01,\n           -2.9975e-02, -1.9431e-01]],\n\n         [[-5.5726e-04, -7.7484e-03, -5.9076e-03,  ..., -2.0097e-02,\n           -3.3195e-03, -1.2772e-02],\n          [-1.1337e-01,  2.4335e-02,  1.2576e-01,  ..., -2.8663e-02,\n            7.0448e-01,  1.6985e-01],\n          [-4.6211e-01,  5.2771e-02,  1.5753e-01,  ..., -2.2525e-01,\n            1.1754e-01, -1.0439e-01],\n          ...,\n          [-5.2622e-01,  3.3897e-01, -3.6210e-03,  ..., -1.6833e-01,\n            8.1798e-03, -3.5989e-02],\n          [-1.7898e-01,  1.4902e-01,  4.3760e-02,  ..., -6.7576e-02,\n            9.4174e-02, -1.0657e-01],\n          [ 1.6964e-01, -3.3308e-01, -1.3923e-01,  ..., -3.0010e-01,\n           -2.0117e-01,  1.6435e-02]],\n\n         [[-3.5693e-02, -7.9178e-03,  1.0561e-02,  ...,  7.9003e-03,\n            1.1250e-02,  1.1793e-02],\n          [-2.1157e-01, -9.3278e-01,  1.2789e-01,  ..., -2.2399e-02,\n           -3.4019e-01, -3.9702e-01],\n          [ 1.5232e-01, -6.8308e-01, -1.2667e-02,  ..., -4.4224e-02,\n            1.6209e-01, -4.1417e-01],\n          ...,\n          [ 1.5063e-01, -2.0814e-01,  2.1461e-01,  ..., -9.8562e-02,\n           -3.6636e-02, -3.1627e-01],\n          [ 3.9176e-01, -1.8605e-01,  2.8391e-02,  ..., -9.8264e-02,\n           -9.5677e-03, -3.8438e-01],\n          [-5.0047e-02, -4.1200e-01,  2.9480e-01,  ...,  3.6376e-01,\n           -8.1028e-02, -4.4138e-01]],\n\n         ...,\n\n         [[-1.7020e-02,  2.0514e-02, -1.3219e-02,  ..., -2.7396e-03,\n           -2.3856e-03,  1.1714e-02],\n          [-4.4627e-02, -9.1744e-02, -9.9211e-02,  ...,  7.2449e-02,\n            9.8154e-02, -3.2808e-02],\n          [ 7.0124e-02,  1.2697e-01,  1.8704e-01,  ..., -1.5831e-01,\n           -1.7343e-02,  1.6386e-01],\n          ...,\n          [-2.0358e-01, -1.5230e-01,  3.7804e-02,  ...,  7.0350e-02,\n           -4.1890e-01,  5.0514e-02],\n          [-1.2607e-02,  2.3928e-01, -3.0951e-02,  ...,  1.8131e-01,\n           -3.2832e-01, -2.9254e-01],\n          [-7.5559e-02, -2.4791e-02, -7.3714e-02,  ...,  2.6189e-01,\n           -1.7322e-01, -8.1548e-02]],\n\n         [[ 2.9759e-03,  9.9851e-03,  1.2614e-02,  ...,  4.7422e-02,\n            1.3567e-02, -3.1465e-03],\n          [ 1.0880e-01,  1.3510e+00,  1.3634e-01,  ...,  2.3499e-02,\n           -1.7103e-01,  1.1104e-01],\n          [ 1.5093e-01,  5.5053e-01,  1.4742e-01,  ..., -5.6128e-02,\n            8.8363e-02,  1.1552e-01],\n          ...,\n          [-8.5937e-02,  1.9849e-01, -5.3613e-02,  ..., -2.0514e-01,\n           -2.6807e-02, -6.9927e-03],\n          [-3.7672e-01, -3.1404e-01, -1.6301e-01,  ..., -1.0568e-01,\n           -2.0208e-01,  3.4364e-01],\n          [ 1.1627e-01,  4.4837e-01, -6.7039e-01,  ...,  2.4221e-01,\n           -4.9293e-01,  2.4366e-01]],\n\n         [[-9.0115e-03,  5.8156e-03,  5.7625e-03,  ...,  1.5618e-03,\n           -8.1958e-03, -1.0273e-02],\n          [ 3.4790e-01, -3.6076e-01,  2.4894e-01,  ..., -3.2158e-01,\n           -5.9973e-02, -1.2560e-01],\n          [ 2.7315e-01, -8.5368e-02,  1.3310e-01,  ...,  1.5860e-01,\n           -1.6372e-01, -8.2068e-03],\n          ...,\n          [-1.0695e-01, -2.5185e-03,  1.4617e-01,  ..., -4.5404e-02,\n           -1.9578e-01, -2.9609e-02],\n          [ 1.1120e-01,  9.2414e-02,  3.8157e-02,  ..., -4.1317e-02,\n           -1.5420e-01, -2.1623e-01],\n          [-1.6257e-01, -1.2010e-02,  6.4325e-02,  ...,  2.4202e-01,\n           -1.9851e-01, -1.4417e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.6317e-02,  1.3466e-01,  5.4830e-02,  ..., -9.1992e-01,\n           -1.4019e+00,  1.4818e+00],\n          [ 1.2326e+00,  3.1613e+00,  1.6568e+00,  ...,  1.8509e-01,\n           -1.3722e+00, -6.6491e+00],\n          [ 1.9328e+00,  1.8872e+00,  2.0568e+00,  ...,  3.9007e-01,\n           -7.1264e-01, -6.8318e+00],\n          ...,\n          [-1.2359e+00, -1.2981e+00,  2.6715e+00,  ..., -5.8398e-01,\n            1.1117e-01, -7.2999e+00],\n          [-1.6475e+00, -1.7938e+00,  9.2559e-02,  ..., -3.8479e-02,\n           -1.5146e-01, -6.6211e+00],\n          [-1.8923e+00, -2.7682e+00,  4.9398e-01,  ..., -9.5186e-01,\n           -5.1755e-01, -7.0788e+00]],\n\n         [[-5.3628e-02,  8.0139e-02, -8.0991e-02,  ..., -1.3485e+00,\n           -1.5041e+00,  6.0794e-01],\n          [-1.0587e+00,  1.2573e+00, -1.7250e+00,  ...,  2.2708e-01,\n            5.9032e-01,  5.7291e-01],\n          [-2.1097e-01,  4.3254e-01, -1.0558e+00,  ...,  1.4978e+00,\n            1.1671e+00,  1.4344e+00],\n          ...,\n          [ 1.2665e+00, -7.3746e-01, -6.9133e-01,  ...,  5.8056e+00,\n            1.6770e+00,  1.7636e+00],\n          [-6.8971e-01, -3.6889e-01,  6.9248e-01,  ...,  6.6919e+00,\n            4.2297e+00,  1.0080e+00],\n          [-4.3376e-01, -1.0968e+00,  5.8477e-01,  ...,  5.3802e+00,\n            6.0880e-01,  1.7648e+00]],\n\n         [[-5.5008e-05, -5.0495e-02, -5.0035e-02,  ..., -3.3727e-01,\n            2.7300e-01, -4.0619e-01],\n          [ 6.1092e-01, -4.9943e-01, -5.3615e-01,  ...,  4.8645e-01,\n           -1.3054e+00,  7.6184e-01],\n          [ 8.0350e-01, -2.8751e-01, -1.0037e+00,  ...,  5.0757e+00,\n           -3.8508e+00,  2.2675e+00],\n          ...,\n          [-3.5077e-01,  3.9110e-01, -7.7770e-02,  ...,  6.1296e+00,\n           -2.7130e+00,  1.4375e+00],\n          [-2.9203e-01,  4.3968e-01, -1.1092e-01,  ...,  5.5444e+00,\n           -1.5172e+00,  3.0202e+00],\n          [-2.7358e-02, -3.0619e-02,  3.5029e-01,  ...,  5.5981e+00,\n           -5.3268e-01,  3.4868e+00]],\n\n         ...,\n\n         [[ 1.0437e-01, -3.3548e-02,  1.2506e-01,  ...,  1.6803e+00,\n           -1.1270e+00, -1.2944e-01],\n          [ 1.7417e+00, -2.8875e-01,  2.3643e+00,  ..., -2.9597e+00,\n           -1.9080e+00,  1.2661e+00],\n          [ 2.9395e-02,  7.7660e-01,  4.5813e-01,  ..., -5.1198e+00,\n            2.3659e-02, -2.0262e-01],\n          ...,\n          [-1.4781e+00,  1.3819e+00, -1.3117e+00,  ..., -3.8288e+00,\n           -1.2726e+00, -1.0869e+00],\n          [ 3.4382e-01,  7.7357e-01, -2.1937e+00,  ..., -3.9219e+00,\n            3.8463e-01, -1.7025e+00],\n          [ 5.3140e-01, -1.3983e-01, -1.8373e+00,  ..., -4.2518e+00,\n           -1.2880e-02, -1.2317e+00]],\n\n         [[-6.8644e-02, -8.1956e-02, -4.0383e-02,  ...,  8.9328e-01,\n            1.8409e+00,  1.4446e+00],\n          [-1.1454e+00, -1.9503e+00, -9.7478e-01,  ...,  3.1692e-01,\n           -3.3628e+00, -1.6152e-01],\n          [ 4.7400e-01, -8.7426e-01, -1.1007e+00,  ...,  3.3010e-01,\n           -3.5354e+00, -2.1006e-01],\n          ...,\n          [ 1.1901e+00,  1.3551e+00, -4.0182e-01,  ..., -9.0979e-02,\n           -3.8868e+00,  6.3973e-01],\n          [ 1.3428e-01,  7.1527e-01, -3.9368e-03,  ...,  7.7908e-02,\n           -3.9934e+00,  3.0507e-01],\n          [-1.0753e+00,  9.4585e-01,  1.7405e-01,  ...,  1.5428e-01,\n           -4.0549e+00, -2.9338e-01]],\n\n         [[-4.6730e-02,  3.6169e-02,  1.8060e-01,  ..., -1.3124e+00,\n            3.0565e-01, -9.7563e-01],\n          [-4.1502e+00,  1.2285e+00,  3.0599e+00,  ..., -1.2249e+00,\n            1.0287e+00, -3.6803e-01],\n          [-1.2745e+00,  9.7856e-01,  1.4054e+00,  ..., -1.7665e+00,\n            7.9501e-01, -9.4531e-01],\n          ...,\n          [ 2.1988e+00,  2.1710e-01, -1.2996e+00,  ..., -7.9220e-01,\n           -2.9255e-01, -7.1018e-01],\n          [ 1.0463e-01, -9.0516e-01, -6.1460e-01,  ..., -1.5183e+00,\n           -2.8626e-01, -1.4258e-01],\n          [ 3.0811e-01, -2.4144e+00, -2.5698e+00,  ...,  7.1495e-01,\n           -7.5689e-01,  3.9869e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5741e-02,  1.2704e-02,  1.0975e-02,  ..., -1.9629e-02,\n           -4.1710e-02,  1.3925e-02],\n          [ 1.0290e-01, -8.6253e-02, -7.6577e-02,  ..., -3.2430e-02,\n            4.8261e-01, -1.7361e-01],\n          [ 7.6148e-02,  1.4310e-01, -2.5367e-01,  ..., -1.9399e-01,\n            5.5266e-01, -2.5191e-01],\n          ...,\n          [-1.1625e-01,  1.5059e-01, -1.2364e-02,  ...,  3.2351e-02,\n            5.4142e-01,  1.3567e-01],\n          [-3.2112e-01,  1.6964e-01, -1.5878e-02,  ..., -8.1791e-02,\n            4.7819e-01,  5.1879e-02],\n          [ 1.2166e-01, -2.2661e-02,  1.4029e-01,  ...,  2.2474e-01,\n            6.1302e-01,  7.6070e-02]],\n\n         [[-7.9166e-04,  2.3437e-03,  8.0485e-03,  ...,  7.7399e-03,\n            9.5045e-03,  4.6631e-03],\n          [ 4.2380e-02, -8.7425e-02, -1.3761e-01,  ...,  7.3599e-02,\n           -2.6021e-01, -2.5217e-01],\n          [-1.7206e-01,  1.6248e-01, -2.3086e-01,  ..., -3.4205e-01,\n           -2.5810e-01, -3.5532e-01],\n          ...,\n          [-9.6249e-02,  2.2249e-01, -6.6095e-01,  ..., -2.5532e-01,\n           -2.2615e-01, -2.3028e-01],\n          [ 9.3356e-02,  2.3495e-01, -5.7557e-01,  ..., -4.1006e-01,\n           -3.8410e-01,  2.0350e-02],\n          [ 1.2238e-01,  2.7590e-01, -1.5102e-01,  ..., -1.0727e-01,\n           -3.4931e-01, -3.5034e-01]],\n\n         [[ 3.9210e-02,  1.9393e-04,  3.3262e-03,  ...,  8.1781e-03,\n           -9.4381e-03, -7.9167e-03],\n          [-9.4547e-01,  1.1973e-01, -2.8968e-01,  ..., -1.8359e-01,\n            1.4042e-01,  8.3514e-03],\n          [-8.3745e-01,  2.7255e-01, -3.6366e-01,  ...,  3.1175e-01,\n           -9.0469e-01,  3.0255e-02],\n          ...,\n          [-8.4997e-01,  2.1602e-01,  7.3421e-02,  ...,  1.1707e-02,\n           -1.1145e-01, -2.7753e-02],\n          [-8.5703e-01, -2.1940e-02, -5.1277e-02,  ..., -1.4274e-01,\n            1.5582e-01,  1.0579e-01],\n          [-8.0754e-01, -1.9184e-02,  1.6539e-01,  ...,  6.7397e-02,\n            3.5809e-02, -1.3506e-01]],\n\n         ...,\n\n         [[ 1.9551e-02,  1.0297e-02,  1.5773e-03,  ..., -1.1819e-02,\n           -1.2796e-03, -1.1804e-02],\n          [ 1.8765e-01, -2.8786e-02, -2.9936e-02,  ..., -3.0006e-02,\n           -2.2696e-01, -1.0051e-01],\n          [-6.9160e-02,  2.4853e-01, -1.6608e-01,  ..., -4.4749e-01,\n            2.3618e-02,  1.7608e-01],\n          ...,\n          [ 1.6023e-01,  2.2103e-01, -3.2907e-01,  ..., -1.1427e-01,\n            1.2678e-01,  1.8312e-02],\n          [-2.5502e-02,  4.3437e-02, -8.4752e-02,  ..., -1.3916e-01,\n            1.1505e-02, -9.1755e-02],\n          [ 9.1606e-02,  1.1718e-01, -2.1248e-01,  ...,  1.8931e-01,\n            1.1512e-01, -1.6510e-01]],\n\n         [[ 2.2793e-02,  2.3541e-02, -2.8061e-02,  ..., -1.2769e-01,\n           -7.2716e-03,  6.0188e-04],\n          [-3.0313e-01,  2.7208e-02, -1.7947e-01,  ...,  3.1877e-01,\n            8.2309e-03, -1.9664e-01],\n          [-1.6683e-01,  2.7815e-01,  1.8993e-02,  ...,  1.8573e-01,\n            4.8218e-02, -3.9488e-02],\n          ...,\n          [-2.5782e-02, -2.7187e-01,  9.3562e-02,  ...,  4.0017e-02,\n           -1.2506e-01, -2.9782e-02],\n          [ 1.4955e-01,  1.9041e-01,  2.7136e-01,  ...,  4.8256e-02,\n           -3.2006e-01,  1.2765e-01],\n          [-7.3341e-02, -8.2576e-02,  1.6007e-02,  ...,  4.9811e-02,\n           -5.3329e-02, -6.1567e-02]],\n\n         [[-2.4003e-02,  1.4413e-03, -9.7189e-03,  ..., -1.0038e-02,\n           -4.0050e-03, -7.8792e-05],\n          [ 4.3207e-02, -1.1919e-01, -5.4457e-01,  ..., -1.4651e-01,\n            1.0372e-01,  2.7977e-01],\n          [-1.0743e-01, -2.7909e-01, -5.8523e-01,  ..., -1.0493e-01,\n           -1.3021e-01, -2.6057e-01],\n          ...,\n          [ 2.4492e-01,  4.2667e-01, -2.2949e-01,  ...,  1.7911e-01,\n            1.6487e-01,  9.0538e-02],\n          [-1.0448e-01,  1.6701e-01, -4.7813e-01,  ..., -1.7286e-02,\n            1.5522e-01,  9.7537e-02],\n          [-1.7028e-02,  8.0929e-01,  9.4702e-01,  ..., -3.3726e-02,\n           -1.0608e-01,  4.0858e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.7858e-02,  1.7457e-01, -2.2763e-02,  ..., -1.1239e+00,\n            9.3052e-01,  9.3458e-01],\n          [ 4.1589e+00,  3.1935e+00, -2.4707e+00,  ..., -1.0910e+00,\n            2.0046e+00,  2.3571e+00],\n          [ 4.1812e+00,  7.1243e-01, -4.3240e+00,  ..., -5.7747e-01,\n            1.6257e+00,  1.6119e+00],\n          ...,\n          [-2.7209e+00, -4.7509e+00, -4.3103e+00,  ..., -2.4002e+00,\n            1.1299e-01,  1.4929e+00],\n          [-3.3909e+00, -3.7879e+00, -2.5160e+00,  ..., -1.4558e+00,\n            6.1843e-01,  2.3745e+00],\n          [-2.5503e+00, -2.7173e+00, -1.4323e+00,  ..., -1.3430e+00,\n           -6.9395e-01,  1.7852e+00]],\n\n         [[ 5.7449e-02, -3.3637e-03, -1.6319e-02,  ..., -4.2206e-01,\n            1.2233e+00,  3.4318e-01],\n          [ 3.2573e+00, -1.4257e+00, -1.1359e+00,  ..., -6.8138e-01,\n            1.1662e+00, -1.3537e-01],\n          [-4.8836e-01, -1.0289e+00, -1.0035e-03,  ..., -5.0336e-01,\n            4.7493e-01,  2.7646e-01],\n          ...,\n          [-1.5530e+00, -1.6113e+00, -3.6268e-01,  ..., -4.6333e-01,\n           -7.9367e-01,  8.5537e-01],\n          [ 9.4621e-01,  1.2660e+00, -6.1222e-01,  ...,  4.1511e-01,\n           -2.5061e-01,  1.0104e+00],\n          [ 2.2199e+00,  2.2013e+00, -1.1727e+00,  ...,  5.7435e-01,\n           -7.1837e-01,  1.9721e-01]],\n\n         [[ 8.8649e-05, -1.0856e-02,  6.7287e-03,  ..., -1.1209e+00,\n           -8.4819e-01,  1.1060e+00],\n          [ 3.0503e-02,  3.2406e-01,  3.5938e-01,  ...,  9.3989e+00,\n            4.5584e+00, -9.7403e+00],\n          [ 6.1180e-03, -4.0424e-02,  1.3981e-01,  ...,  8.2844e+00,\n            9.3161e-01, -9.1253e+00],\n          ...,\n          [-3.2833e-01, -3.9624e-03,  3.8422e-02,  ...,  1.5234e+00,\n            3.0882e+00, -8.6050e+00],\n          [-7.1255e-01, -1.5154e-01, -4.9607e-02,  ...,  5.0184e-01,\n            3.0396e+00, -6.9793e+00],\n          [-4.2383e-02, -2.2849e-01, -5.1579e-01,  ..., -1.3567e+00,\n            8.6881e+00, -4.9598e+00]],\n\n         ...,\n\n         [[ 4.7751e-03,  2.4765e-02,  6.5427e-02,  ...,  1.1498e+00,\n           -5.5660e-01, -2.3739e+00],\n          [-2.7048e-01,  1.9240e+00,  9.2797e-01,  ...,  6.1590e-01,\n           -5.3501e-02,  3.5105e+00],\n          [-1.4208e+00,  7.9953e-01,  7.2649e-01,  ...,  1.4090e+00,\n           -1.0943e-01,  4.6916e+00],\n          ...,\n          [ 1.4458e+00, -1.2390e+00, -1.6505e-01,  ...,  1.6354e+00,\n           -5.8472e-01,  4.2420e+00],\n          [ 1.2140e+00, -1.6530e+00, -7.6998e-01,  ...,  1.3969e+00,\n           -6.5632e-01,  4.7695e+00],\n          [ 4.2530e-01, -1.0632e+00, -1.5249e+00,  ...,  9.5883e-01,\n           -1.4171e+00,  4.5755e+00]],\n\n         [[ 4.9551e-03,  4.8713e-03, -6.7961e-02,  ..., -9.5603e-01,\n           -2.0773e+00, -1.4528e+00],\n          [-2.4987e+00,  4.8178e-01, -8.5864e-01,  ..., -1.1032e+00,\n           -4.5302e-01, -2.4382e+00],\n          [-1.3630e+00,  1.4044e+00, -7.8732e-01,  ..., -3.2960e-01,\n           -6.6378e-01, -1.6802e+00],\n          ...,\n          [ 1.0112e+00,  6.0710e-01,  3.2059e-01,  ..., -1.3903e-01,\n           -1.1601e+00, -1.1141e+00],\n          [ 5.1350e-01, -1.1856e+00, -1.4531e-01,  ...,  3.1771e-01,\n           -4.8929e-02, -3.4188e-01],\n          [ 5.9443e-01, -1.3259e+00,  2.0242e-02,  ..., -1.4919e+00,\n           -5.6953e-01, -7.6237e-01]],\n\n         [[-7.9350e-02, -5.4308e-02, -9.5349e-03,  ...,  9.4083e-01,\n            2.5170e+00, -3.2786e+00],\n          [-2.1497e+00, -1.7756e+00,  8.0995e-01,  ..., -3.3403e+00,\n           -1.8374e-01,  4.8019e+00],\n          [ 1.6129e+00,  5.4331e-01,  6.1357e-01,  ..., -3.7763e+00,\n            1.2750e+00,  7.3814e+00],\n          ...,\n          [ 9.7731e-01,  1.4124e+00,  3.8733e-01,  ..., -2.2185e+00,\n            1.4152e+00,  8.3236e+00],\n          [-1.5950e+00, -1.5879e-01,  4.6176e-01,  ..., -1.2807e+00,\n            1.1276e+00,  8.4439e+00],\n          [-7.3651e-01, -4.5895e-03, -7.2717e-02,  ..., -3.6994e+00,\n           -1.4185e+00,  6.0466e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 9.3590e-04,  2.8529e-02,  5.6978e-03,  ...,  1.2661e-02,\n            8.9264e-03, -8.7175e-03],\n          [ 3.0078e-01,  2.6767e-01,  1.9271e-01,  ...,  1.4708e-01,\n            7.1249e-02,  1.2822e-01],\n          [-8.4698e-02,  5.7607e-01, -3.0367e-01,  ...,  1.0784e-01,\n           -2.7994e-02,  4.0537e-01],\n          ...,\n          [ 1.2736e-01, -7.2686e-02,  3.0685e-01,  ..., -1.6118e-02,\n           -5.6757e-01, -3.7781e-01],\n          [ 1.8854e-01,  4.3236e-01,  2.4099e-01,  ..., -4.3005e-02,\n           -9.0187e-01,  1.1908e-01],\n          [ 1.0048e-01, -7.0765e-03,  4.2359e-02,  ..., -6.1907e-01,\n           -1.0317e-02,  2.9996e-01]],\n\n         [[ 4.2125e-02, -1.4949e-02,  6.8945e-03,  ...,  6.5633e-03,\n           -1.9918e-03, -1.2263e-02],\n          [-8.6123e-01,  2.1418e-02, -4.6317e-02,  ..., -4.7484e-01,\n           -2.6559e-01, -2.0109e-01],\n          [-3.3328e-01,  3.3497e-02, -1.6863e-01,  ..., -2.9473e-01,\n           -3.0380e-01,  8.4597e-02],\n          ...,\n          [-3.1169e-01,  1.6245e-01, -5.2212e-02,  ..., -2.4400e-01,\n           -3.6954e-01, -2.5663e-02],\n          [-9.2145e-02,  4.4051e-01,  4.6001e-02,  ..., -1.2436e-03,\n           -9.6444e-02, -4.9024e-01],\n          [ 2.0358e-01,  1.3464e-01, -2.1163e-01,  ..., -1.3686e-01,\n           -1.3435e-01,  1.6847e-01]],\n\n         [[ 4.8812e-03,  6.3428e-03, -1.5907e-02,  ..., -7.9164e-03,\n            1.9445e-03,  9.0020e-03],\n          [-9.8095e-02, -8.6381e-02, -3.6844e-01,  ..., -5.7589e-02,\n            2.1959e-01, -7.0982e-02],\n          [ 3.0170e-01, -1.0434e-01, -3.8194e-03,  ...,  6.2394e-02,\n            1.2737e-01, -1.1202e-01],\n          ...,\n          [ 2.2344e-01,  9.3603e-03, -2.2775e-01,  ..., -7.9702e-03,\n            2.8950e-02,  1.9388e-01],\n          [ 3.4498e-01,  8.0839e-02, -7.5499e-02,  ...,  5.3918e-02,\n           -1.1760e-01,  1.5815e-01],\n          [ 1.9497e-01, -1.1156e-01, -2.7446e-01,  ..., -6.2212e-02,\n            1.6979e-01,  5.9713e-02]],\n\n         ...,\n\n         [[-2.1932e-03,  6.7271e-03,  1.6275e-02,  ..., -1.2165e-02,\n            6.1163e-03, -3.1671e-02],\n          [ 9.7741e-02,  9.4036e-02,  2.4281e-02,  ...,  1.0980e-01,\n           -1.0405e-01,  1.0067e-01],\n          [-3.2719e-02,  5.3270e-02, -6.5072e-02,  ...,  1.3246e-01,\n           -4.2380e-01,  1.8715e-01],\n          ...,\n          [ 4.3528e-01,  1.0122e-01,  1.3358e-01,  ...,  4.0793e-01,\n           -2.2985e-01, -1.7101e-01],\n          [ 3.1636e-02,  1.8576e-02,  2.6358e-01,  ...,  3.2357e-01,\n           -3.8288e-01, -1.5802e-01],\n          [ 1.3856e-01, -7.9769e-02,  8.5927e-02,  ...,  4.6440e-01,\n           -3.4063e-02, -3.4477e-01]],\n\n         [[ 1.3362e-02,  9.6465e-03, -3.3372e-03,  ..., -2.1057e-02,\n           -1.5878e-02,  1.7561e-02],\n          [-1.7414e-01,  1.1742e-01,  1.4036e-01,  ...,  2.4521e-01,\n            9.3921e-02,  1.9606e-01],\n          [ 8.1271e-02,  4.4230e-01,  2.0403e-02,  ..., -1.2735e-01,\n           -1.8625e-01,  1.1084e-01],\n          ...,\n          [-2.4271e-01,  1.0439e-01, -1.1056e-01,  ...,  3.6153e-01,\n           -3.2894e-01,  2.8341e-02],\n          [ 5.9896e-02,  1.3758e-01, -2.8612e-01,  ...,  2.2576e-01,\n           -4.9372e-01,  5.5741e-02],\n          [-3.2227e-01,  4.0608e-01, -5.1128e-01,  ...,  4.2920e-01,\n           -1.1371e-01,  7.4510e-01]],\n\n         [[ 1.2613e-02,  4.4390e-03, -2.8977e-02,  ...,  1.5352e-02,\n           -3.1253e-03,  1.0260e-02],\n          [ 2.6141e-02,  4.4875e-01,  4.2975e-01,  ...,  3.2282e-01,\n           -2.1937e-01, -2.1450e-01],\n          [-1.0085e-01,  1.5843e-01,  9.9052e-01,  ...,  1.2787e-01,\n           -2.0459e-01, -5.0992e-01],\n          ...,\n          [-1.1244e-01,  6.0204e-02,  4.3369e-02,  ...,  1.0944e-01,\n            3.6751e-03, -9.8643e-02],\n          [-6.7898e-02, -1.1371e-01,  2.6787e-01,  ..., -1.7534e-01,\n            2.3881e-01, -4.7611e-02],\n          [-1.0737e-01, -1.8074e-01,  1.1761e-01,  ...,  5.3748e-03,\n           -3.4666e-01, -4.4969e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.2986e-02, -5.6758e-02,  4.2181e-02,  ...,  6.8194e-01,\n           -2.4365e-01, -2.6245e-01],\n          [-2.0277e+00,  3.6989e-01,  1.0358e+00,  ...,  1.5145e+00,\n           -2.0044e+00,  8.4235e-01],\n          [-1.3090e+00,  8.2125e-01,  5.5625e-01,  ..., -1.8567e-01,\n           -1.1091e+00,  5.4527e-01],\n          ...,\n          [ 1.5006e+00,  4.5542e-01,  4.3176e-01,  ...,  7.1436e-01,\n           -8.7396e-01, -4.8069e-01],\n          [ 1.0901e+00,  3.9606e-01, -4.7152e-01,  ...,  3.2983e-01,\n           -5.0065e-01,  5.0166e-01],\n          [ 1.2840e-01, -8.3067e-01, -8.1293e-01,  ..., -4.0872e-01,\n           -1.9316e+00, -1.9433e+00]],\n\n         [[-6.8734e-02,  8.0266e-02, -9.0677e-03,  ...,  7.3990e-01,\n            8.9014e-02, -5.7725e-01],\n          [-2.7317e+00,  2.1390e+00,  1.1494e+00,  ..., -3.4212e+00,\n           -3.1490e-01, -1.6709e+00],\n          [-2.9074e-01, -5.3336e-02, -5.1275e-01,  ..., -2.8581e+00,\n            4.7676e-02, -2.8024e-01],\n          ...,\n          [ 1.6968e+00, -1.9554e+00,  1.7006e+00,  ..., -4.3257e+00,\n           -2.2115e-01,  6.2416e-01],\n          [-9.2230e-01, -9.7474e-01,  3.9166e-01,  ..., -4.3167e+00,\n           -2.2765e-01, -4.8433e-01],\n          [-6.0461e-01, -1.0264e+00,  3.7290e-01,  ..., -3.7289e+00,\n           -6.2502e-02,  5.3949e-01]],\n\n         [[ 1.7127e-03, -3.6500e-02, -9.9987e-03,  ...,  3.8029e-01,\n            7.0632e-01, -9.5575e-02],\n          [-1.7413e-01, -2.3737e-01, -9.7911e-01,  ...,  1.6357e+00,\n           -9.5725e-02,  5.3678e-01],\n          [-8.8745e-01,  3.5649e-02, -1.8132e-01,  ...,  3.8635e-01,\n           -3.3454e-01,  3.4820e-01],\n          ...,\n          [ 4.0956e-01,  9.8702e-01, -1.6676e+00,  ..., -6.2349e-02,\n           -3.2785e-01,  3.2371e-01],\n          [ 1.3126e+00, -1.0274e-01, -5.2804e-01,  ...,  3.1959e-01,\n           -1.4227e-01,  1.1870e-01],\n          [ 7.6431e-01,  1.3596e-01, -2.6380e-01,  ...,  5.5026e-02,\n            4.8367e-02,  1.2360e+00]],\n\n         ...,\n\n         [[ 4.9501e-02, -3.4285e-02,  4.6124e-02,  ..., -3.8699e-01,\n            1.4646e+00, -1.7441e-01],\n          [ 1.2642e+00, -1.4507e+00,  1.6048e+00,  ...,  2.4330e+00,\n           -4.2889e+00, -5.5782e-01],\n          [-1.4796e-01,  3.3218e-01,  3.1144e-01,  ...,  2.9231e+00,\n           -3.6052e+00, -1.9999e+00],\n          ...,\n          [-1.2134e+00,  1.0892e+00, -3.7682e-01,  ...,  1.4461e+00,\n           -4.1793e+00, -2.2762e+00],\n          [ 7.4900e-01,  1.1938e-01, -4.1763e-01,  ...,  1.6771e+00,\n           -4.0144e+00, -2.6294e+00],\n          [ 4.3123e-01,  6.7885e-01, -1.1292e+00,  ...,  1.6564e+00,\n           -2.8780e+00, -1.3151e+00]],\n\n         [[-4.2510e-02,  3.7889e-02,  6.0747e-02,  ..., -5.7372e-02,\n           -1.2709e+00, -7.8387e-01],\n          [-5.0359e-01,  8.3469e-01,  1.2501e+00,  ...,  2.3276e-02,\n           -1.6576e+00, -4.7826e+00],\n          [ 3.5352e-01, -6.4913e-02,  5.3792e-01,  ..., -1.1122e-01,\n           -1.0168e+00, -5.8695e+00],\n          ...,\n          [ 5.3220e-02, -7.1725e-01,  4.9906e-01,  ..., -9.5418e-01,\n           -3.0370e+00, -2.7932e+00],\n          [-4.3403e-03, -3.6534e-01, -2.3779e-02,  ..., -1.4896e+00,\n           -2.6524e+00, -4.4556e+00],\n          [-3.7900e-01, -5.6974e-01, -3.5380e-02,  ..., -2.3949e+00,\n           -1.5284e+00, -4.1054e+00]],\n\n         [[ 5.9208e-02, -7.4041e-02, -3.8141e-02,  ...,  4.1372e-01,\n            1.9973e+00,  1.8169e+00],\n          [-3.5135e-01, -1.5177e+00, -1.0858e+00,  ...,  2.8560e-01,\n           -6.1455e-01, -2.1975e+00],\n          [-1.7711e+00, -5.5017e-01, -8.7693e-01,  ...,  1.6907e-01,\n           -5.6715e-02, -2.5102e+00],\n          ...,\n          [ 1.6425e-01,  2.4681e+00, -1.4838e+00,  ...,  4.7841e-01,\n           -1.6130e+00, -4.9014e+00],\n          [ 1.0952e+00,  1.1468e+00,  6.9954e-01,  ..., -7.0833e-01,\n           -1.6066e+00, -4.8652e+00],\n          [ 1.0568e+00,  6.1719e-01,  4.0996e-01,  ..., -2.6305e-02,\n           -3.2821e+00, -4.5679e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.5075e-03, -4.3818e-02,  3.2011e-02,  ...,  3.3098e-02,\n           -1.9107e-02, -3.6052e-02],\n          [-2.1269e-03,  1.6216e-01, -1.5261e-01,  ..., -2.4489e-01,\n           -1.2112e-01,  1.9746e-01],\n          [-2.4059e-01,  3.1051e-01,  5.7210e-02,  ..., -2.6637e-01,\n           -6.1936e-02,  2.2837e-01],\n          ...,\n          [-1.1204e-01,  2.9352e-01,  2.7793e-01,  ..., -1.0572e-01,\n            2.2897e-02,  5.1392e-01],\n          [-1.3869e-01,  1.0130e-01, -6.4869e-02,  ...,  4.4783e-02,\n            4.8176e-03,  2.2737e-01],\n          [-4.1347e-03,  2.8174e-02, -1.6069e-01,  ...,  2.7191e-01,\n            2.8911e-04,  4.5144e-01]],\n\n         [[-1.2707e-02,  7.7928e-03, -2.4912e-02,  ...,  4.3719e-04,\n            1.3313e-02,  3.7711e-03],\n          [ 1.4313e-01, -3.8084e-01,  4.3599e-01,  ..., -1.1904e-01,\n            2.5198e-01,  2.1550e-01],\n          [-5.0879e-03, -3.7911e-02,  2.5485e-01,  ..., -2.7624e-01,\n           -1.2211e-01,  2.4520e-01],\n          ...,\n          [-1.3643e-01, -3.5685e-01,  4.4867e-01,  ...,  1.2322e-01,\n            6.0183e-01,  2.4804e-01],\n          [-2.0965e-01, -3.1830e-01,  3.1960e-01,  ...,  3.6249e-02,\n            3.7764e-01,  6.0210e-03],\n          [-8.3376e-02, -2.1156e-01,  1.5272e-01,  ..., -6.9034e-02,\n            2.5154e-01, -1.3800e-01]],\n\n         [[ 4.9766e-03,  1.6422e-02, -9.3895e-04,  ..., -1.0992e-02,\n            2.7222e-02, -3.3118e-03],\n          [ 7.0952e-02,  1.4994e-01, -1.4358e-01,  ...,  1.3321e-01,\n           -9.9199e-01, -5.0039e-02],\n          [ 2.2315e-03,  1.3689e-02,  5.4795e-01,  ...,  4.3925e-01,\n           -3.8316e-01,  1.2275e-01],\n          ...,\n          [ 1.4568e-01,  3.6804e-01,  1.8879e-01,  ...,  6.3231e-01,\n           -6.8411e-01,  8.1384e-02],\n          [ 1.6347e-01,  5.0384e-02,  1.6249e-01,  ...,  2.5391e-01,\n            4.1089e-02,  8.6733e-02],\n          [ 1.0754e-01,  2.0364e-03,  3.1858e-02,  ...,  1.2100e-01,\n           -6.8371e-01, -2.3201e-02]],\n\n         ...,\n\n         [[-6.8232e-03, -3.4541e-03,  3.0257e-03,  ..., -1.1610e-02,\n            1.4196e-02, -4.2651e-03],\n          [-1.9102e-01, -9.6374e-02, -3.8347e-01,  ..., -6.8594e-02,\n            1.5969e-01, -2.9198e-01],\n          [-1.1796e-01, -3.7198e-01, -1.7730e-01,  ...,  3.0197e-01,\n            4.4423e-01, -3.5287e-01],\n          ...,\n          [ 2.5382e-01,  1.8584e-02, -1.4926e-01,  ...,  1.1287e-01,\n            1.5608e-01,  1.8151e-01],\n          [ 5.1760e-02,  1.7294e-02, -1.8609e-01,  ..., -1.3589e-01,\n            6.0654e-02, -8.9075e-02],\n          [-3.4706e-01, -2.0907e-01, -1.7131e-01,  ..., -2.4358e-01,\n           -1.2957e-01, -2.4586e-01]],\n\n         [[ 1.8800e-02, -2.2288e-02, -1.8583e-02,  ...,  2.8067e-02,\n            2.0518e-02,  7.2470e-03],\n          [-2.2676e-01,  2.8546e-01,  1.0846e-01,  ...,  7.3832e-02,\n           -2.8026e-02,  2.3904e-06],\n          [-2.1163e-02,  7.5956e-01,  1.8756e-01,  ...,  3.1446e-01,\n            5.6013e-01, -2.0784e-01],\n          ...,\n          [ 1.9506e-01,  1.2693e-01, -2.0888e-01,  ...,  1.7235e-01,\n            6.0297e-02, -8.2514e-02],\n          [ 4.7009e-01,  4.0924e-01, -2.4973e-01,  ..., -1.2039e-01,\n            1.1012e-01,  5.1201e-02],\n          [ 2.7584e-01,  3.3358e-01, -1.8232e-01,  ...,  2.0032e-01,\n            2.2118e-01,  5.0746e-02]],\n\n         [[ 1.1655e-02, -1.1191e-02, -2.4126e-02,  ...,  1.7493e-02,\n           -1.3610e-02, -2.2237e-02],\n          [-1.6543e-01, -2.5003e-01,  8.6125e-02,  ...,  5.6394e-02,\n            3.4471e-02,  9.6381e-02],\n          [-1.6387e-01, -1.1931e-01,  5.5219e-01,  ...,  5.0510e-01,\n           -1.5565e-01,  8.6988e-02],\n          ...,\n          [-3.0309e-01, -2.2586e-02,  1.5457e-01,  ...,  1.6099e-01,\n           -5.5603e-01, -2.0773e-01],\n          [-4.4897e-01, -2.6678e-02, -3.1037e-01,  ...,  1.3030e-01,\n           -9.0136e-01,  5.7729e-02],\n          [ 2.0204e-01,  3.6599e-02,  1.9856e-01,  ...,  7.2303e-02,\n           -2.2883e-01, -1.2853e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4815e-02,  3.8855e-03, -6.5161e-02,  ...,  3.6477e-01,\n           -7.6282e-01,  3.9310e-01],\n          [ 1.7472e-01,  8.0178e-01, -5.7563e-01,  ..., -1.6855e+00,\n           -3.8904e+00, -1.9497e+00],\n          [-9.5987e-01,  5.8723e-01, -7.1325e-01,  ..., -1.8820e+00,\n           -1.9519e+00, -3.5142e+00],\n          ...,\n          [ 2.9018e-01, -2.2332e-01, -2.2718e-01,  ..., -1.3363e+00,\n           -1.4313e+00, -2.5188e+00],\n          [ 3.7244e-01, -1.9810e-01, -1.9881e-01,  ..., -6.5588e-01,\n           -1.1756e+00, -4.1595e+00],\n          [ 6.5614e-01, -1.0736e+00,  8.7181e-01,  ..., -9.0678e-01,\n           -7.1075e-01, -3.9002e+00]],\n\n         [[-5.2089e-03,  1.4205e-02, -1.2689e-03,  ..., -5.0026e-01,\n           -5.8205e-01,  5.3615e-01],\n          [-1.5271e+00,  8.8098e-01, -5.7725e-02,  ...,  4.1800e+00,\n           -1.7192e+00,  1.1539e+00],\n          [ 3.9878e-01, -2.5377e-01, -2.3752e-01,  ...,  4.6732e+00,\n           -8.8458e-01, -5.0242e-01],\n          ...,\n          [ 7.0503e-01, -1.1254e+00,  2.2898e-01,  ...,  3.6139e+00,\n           -7.5675e-02,  9.0863e-01],\n          [-3.5906e-01, -1.2928e-01,  1.5099e-01,  ...,  2.8251e+00,\n           -3.7178e-01, -2.4456e+00],\n          [-7.6586e-01, -1.8641e-01,  8.9007e-01,  ...,  7.6406e-01,\n           -8.1659e-01, -3.2165e-01]],\n\n         [[-4.6029e-02, -7.7140e-02,  7.5468e-02,  ..., -7.1893e-02,\n            1.2510e+00, -1.8972e-01],\n          [-1.7925e+00, -1.6732e+00,  2.4657e+00,  ..., -3.3280e-01,\n            1.4177e+00, -1.9065e-01],\n          [-1.3771e-01, -1.2481e-01,  1.1394e+00,  ...,  2.8269e-03,\n            1.3836e+00,  4.2271e-01],\n          ...,\n          [ 1.1215e+00,  1.7611e+00, -2.1272e+00,  ...,  4.1990e-02,\n            1.2248e+00,  4.4655e-01],\n          [-4.5170e-03,  1.8393e+00, -5.3411e-01,  ..., -2.2830e-05,\n            5.9196e-01,  8.4567e-01],\n          [-1.4081e+00,  2.5823e-01, -2.0935e+00,  ...,  8.9313e-02,\n           -9.1095e-03, -7.3424e-01]],\n\n         ...,\n\n         [[ 3.8313e-02,  3.1387e-02, -4.9776e-02,  ...,  1.4955e+00,\n            1.4901e+00,  3.7326e-01],\n          [ 4.7177e-01, -1.9497e-01, -9.5550e-01,  ..., -1.4786e+00,\n           -5.3615e+00, -4.5147e+00],\n          [-2.1454e-01, -3.4196e-01,  1.6850e-01,  ..., -2.6179e+00,\n           -6.5332e+00, -5.7047e+00],\n          ...,\n          [-5.5921e-01,  7.6688e-01,  5.1687e-01,  ..., -2.3091e+00,\n           -6.1471e+00, -5.2471e+00],\n          [ 5.3142e-01, -1.4104e-02,  2.5696e-01,  ..., -2.2951e+00,\n           -6.7571e+00, -5.0329e+00],\n          [ 6.1794e-01, -4.1871e-01,  8.8736e-01,  ..., -9.6804e-01,\n           -6.1409e+00, -5.1368e+00]],\n\n         [[ 1.6163e-01,  7.1201e-02,  7.7224e-02,  ..., -9.5703e-01,\n            5.5020e-01,  6.5097e-01],\n          [ 6.2034e-01,  1.2192e+00,  2.8485e+00,  ..., -4.1860e+00,\n            2.0176e+00, -1.7881e+00],\n          [-3.9736e+00, -1.6081e+00,  2.0517e+00,  ..., -4.9265e+00,\n            2.1788e+00,  1.6002e-01],\n          ...,\n          [-1.7394e+00, -3.7851e+00, -5.4124e-01,  ..., -4.0162e+00,\n            2.0071e+00, -5.5163e-01],\n          [ 3.5902e+00, -2.1066e+00, -1.7955e+00,  ..., -2.4751e+00,\n            3.4249e+00, -7.4834e-03],\n          [ 5.7654e+00, -5.9770e-01, -2.7081e+00,  ..., -4.5666e+00,\n            3.4483e+00, -4.5146e-01]],\n\n         [[ 9.2309e-03, -1.3726e-02, -6.7525e-02,  ...,  5.8759e-01,\n            1.1254e+00,  8.7347e-01],\n          [ 6.7081e-01,  1.3083e+00, -2.8793e-01,  ..., -1.8503e+00,\n            1.0474e+00, -1.1659e+00],\n          [-1.9691e+00,  6.1067e-02, -4.9139e-02,  ..., -2.0803e+00,\n            1.0640e+00, -1.6873e+00],\n          ...,\n          [-1.9309e-01, -8.1366e-01, -1.0745e+00,  ..., -1.1412e+00,\n            1.2140e+00, -2.7790e+00],\n          [ 1.2697e+00, -4.0457e-01,  5.4169e-01,  ..., -2.0708e+00,\n           -3.0930e-01, -3.3942e+00],\n          [ 1.1935e+00, -3.6645e-01,  1.2198e+00,  ..., -2.5969e+00,\n            3.4379e-02, -4.6407e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3331e-02, -4.6928e-03, -7.3019e-03,  ..., -1.9747e-02,\n           -3.0523e-03,  2.9128e-02],\n          [ 1.2601e-01, -5.6824e-02,  1.2379e-01,  ...,  1.2374e-01,\n           -3.3303e-02, -3.7515e-01],\n          [ 1.9364e-01, -1.2125e-01,  2.8124e-01,  ..., -1.0768e-01,\n            6.3876e-02, -6.0966e-01],\n          ...,\n          [-4.2872e-02, -1.5920e-01,  3.1075e-01,  ...,  7.3740e-02,\n           -1.0305e-01, -3.5542e-01],\n          [ 9.1024e-02, -1.0297e-01,  4.4406e-01,  ...,  3.4327e-01,\n           -1.5741e-01, -6.0071e-01],\n          [ 1.5528e-01, -2.7940e-01,  3.7453e-01,  ..., -7.5135e-02,\n           -3.9484e-02, -1.8948e-01]],\n\n         [[-4.6705e-03,  2.1753e-03,  2.7855e-02,  ..., -1.3684e-03,\n            6.2462e-02,  2.0781e-03],\n          [-1.2567e-03, -1.0440e-01, -1.3493e-02,  ..., -1.8711e-01,\n           -9.1693e-02,  6.7549e-02],\n          [ 2.9302e-01,  2.4074e-01, -4.7760e-02,  ..., -3.8108e-01,\n            2.2722e-02,  2.2241e-01],\n          ...,\n          [ 6.7676e-02,  3.6794e-01,  2.8198e-01,  ..., -1.3390e-01,\n            5.1727e-01,  2.9591e-01],\n          [ 5.2915e-02,  3.9826e-01,  1.7824e-01,  ..., -8.1494e-03,\n            1.1654e-01,  4.1005e-01],\n          [ 6.6410e-02,  2.2054e-01,  2.0727e-01,  ...,  6.0972e-02,\n            2.6168e-01,  1.0454e-01]],\n\n         [[-6.5995e-03, -3.0940e-02,  3.0484e-02,  ...,  5.9859e-04,\n            1.1084e-02, -1.7138e-01],\n          [ 1.1578e-01,  2.1733e-03, -2.2893e-01,  ..., -1.7371e-01,\n           -2.2726e-03, -5.9387e-02],\n          [ 2.0877e-01,  2.2600e-01, -1.2645e-01,  ..., -3.3830e-01,\n            6.7676e-02,  1.6034e-01],\n          ...,\n          [ 2.2702e-01,  3.6602e-02, -2.1516e-01,  ..., -9.3808e-01,\n            2.2783e-01, -1.7914e-01],\n          [ 4.6318e-01,  1.4795e-02,  1.8363e-02,  ..., -5.6283e-01,\n            1.2641e-01,  4.9621e-01],\n          [-1.6959e-01, -2.2225e-03,  2.8095e-01,  ..., -4.9871e-01,\n            1.7634e-01,  7.5611e-01]],\n\n         ...,\n\n         [[ 4.8023e-03,  4.0914e-01, -1.4497e-02,  ...,  7.0290e-03,\n            1.2445e-02, -2.1458e-02],\n          [ 1.3051e-02, -9.3685e-01,  1.1798e-01,  ..., -1.4003e-01,\n           -1.2717e-01,  8.9570e-02],\n          [-1.6168e-01, -1.6029e+00,  2.3556e-02,  ...,  4.6295e-02,\n            6.0343e-02,  1.0125e-01],\n          ...,\n          [-1.6702e-01, -1.0712e+00,  2.1793e-01,  ...,  1.0436e-01,\n           -4.7951e-02, -1.9403e-01],\n          [-1.1241e-01, -1.2524e+00,  2.6625e-01,  ..., -2.1663e-02,\n           -7.1329e-02, -2.4839e-01],\n          [-1.3651e-01, -6.7578e-01, -2.5005e-02,  ...,  6.8521e-02,\n           -2.7406e-01, -2.5056e-01]],\n\n         [[-1.6400e-02, -9.9874e-03, -1.3735e-02,  ..., -2.2474e-02,\n            2.0281e-02,  2.2781e-01],\n          [ 4.7074e-01, -4.9741e-01, -3.5659e-01,  ...,  9.1268e-02,\n            1.1694e-01, -2.1805e-01],\n          [ 2.0684e-01,  4.0959e-01, -5.0544e-02,  ...,  9.8892e-02,\n            2.3886e-01,  1.7650e-01],\n          ...,\n          [ 2.1865e-01,  2.6039e-01, -1.8954e-02,  ..., -1.6628e-01,\n           -2.1735e-02, -5.7966e-01],\n          [ 1.3507e-01,  1.4209e-01,  4.9038e-02,  ...,  1.2472e-01,\n           -8.2507e-02,  2.5974e-01],\n          [ 2.4348e-01, -3.2005e-02, -2.9278e-01,  ..., -3.8628e-02,\n            1.9323e-01, -4.1019e-01]],\n\n         [[-5.9746e-03,  1.0048e-02, -4.2604e-03,  ..., -1.0930e-01,\n            3.4326e-02,  8.4342e-02],\n          [ 1.0278e-01, -3.1082e-02,  5.8721e-02,  ...,  6.1986e-01,\n           -2.0531e-01, -6.2644e-01],\n          [ 1.1943e-01, -5.8391e-02,  5.6865e-02,  ...,  7.2164e-01,\n           -1.2859e-01, -6.6454e-01],\n          ...,\n          [-3.3965e-01, -1.6304e-01,  5.2154e-01,  ...,  7.4580e-01,\n           -5.2450e-02, -6.4519e-01],\n          [-2.0817e-01,  2.0681e-01,  3.1963e-01,  ...,  7.1918e-01,\n           -3.6935e-02, -6.8810e-01],\n          [-3.5905e-01,  3.3189e-01,  2.7067e-01,  ...,  2.4084e-01,\n           -5.4943e-02, -6.5760e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7947e-02,  7.9241e-03,  5.4580e-02,  ...,  7.3166e-01,\n            7.5432e-01, -5.3334e-01],\n          [-1.4227e+00, -3.1328e-01,  7.6773e-01,  ..., -1.4749e+00,\n            1.3679e+00,  1.1102e+00],\n          [ 2.1317e-01, -3.5648e-01, -4.1964e-01,  ...,  1.3892e-01,\n            8.3991e-01, -9.4518e-01],\n          ...,\n          [ 2.0481e-01,  6.3052e-01,  5.3492e-01,  ...,  1.6286e+00,\n            7.7440e-01, -2.1364e+00],\n          [-9.4441e-01,  1.3181e+00, -9.8881e-01,  ...,  9.1665e-01,\n            8.6446e-01, -2.0505e+00],\n          [-9.4097e-01,  1.0915e+00, -3.3495e-02,  ..., -2.5536e-01,\n            1.0748e+00, -1.3074e+00]],\n\n         [[-2.9458e-02,  1.3561e-02, -1.8575e-02,  ..., -1.3401e+00,\n           -1.5849e+00,  8.7977e-01],\n          [-2.5556e+00,  6.0933e-01, -1.5862e+00,  ..., -2.3096e+00,\n           -2.1301e+00,  1.4235e-01],\n          [-1.0071e+00,  4.9559e-02, -6.3062e-01,  ..., -3.2193e+00,\n           -1.8662e+00, -1.0737e-02],\n          ...,\n          [ 6.9421e-01, -4.1986e-01, -9.7129e-01,  ..., -2.5432e+00,\n           -1.3412e+00,  3.5665e-01],\n          [ 1.0528e+00, -1.8289e+00, -3.9715e-01,  ..., -2.5711e+00,\n           -1.0718e+00, -2.1864e-01],\n          [ 1.1092e+00, -1.4605e+00, -1.4248e+00,  ..., -1.9361e+00,\n            7.9981e-01,  4.2452e-01]],\n\n         [[ 4.9120e-02,  7.5759e-02,  2.0578e-02,  ...,  5.4245e-01,\n            3.2780e-01, -1.0823e+00],\n          [ 1.9434e+00,  1.4119e+00,  8.9912e-01,  ..., -2.4708e+00,\n           -5.8531e-01,  1.9458e+00],\n          [ 1.3223e+00,  6.1247e-01,  1.3897e+00,  ..., -2.6598e+00,\n           -2.1375e+00,  9.3404e-01],\n          ...,\n          [-1.0583e+00, -1.9213e+00,  1.4193e+00,  ..., -1.9858e+00,\n           -2.4528e+00,  1.8812e+00],\n          [-1.6972e+00, -1.7027e+00,  1.3032e-01,  ..., -1.7238e+00,\n           -3.0209e+00,  1.1120e+00],\n          [ 6.4561e-02, -9.5799e-01,  4.1644e-01,  ..., -2.1372e+00,\n           -2.3745e+00,  6.9682e-01]],\n\n         ...,\n\n         [[ 3.2195e-02,  5.6718e-02,  3.6552e-02,  ..., -1.1915e+00,\n            2.4083e+00, -8.5296e-01],\n          [ 1.9427e+00,  8.3545e-01,  1.8341e+00,  ..., -2.2661e+00,\n           -6.1698e+00, -2.3955e+00],\n          [ 1.3729e+00, -1.2718e+00,  9.0652e-01,  ..., -2.7209e+00,\n           -4.1600e+00, -2.1662e+00],\n          ...,\n          [-1.8673e+00, -1.3913e+00, -1.8973e+00,  ..., -2.7562e+00,\n           -3.9062e+00, -3.6888e+00],\n          [-1.5251e+00, -1.6881e+00, -7.6363e-01,  ..., -1.6459e+00,\n           -3.8735e+00, -1.6431e+00],\n          [-1.5214e-01, -6.2781e-01, -2.1714e+00,  ..., -1.4013e+00,\n           -4.1542e+00, -2.9377e+00]],\n\n         [[-3.5472e-02, -2.0856e-02,  8.1324e-03,  ..., -5.8972e-02,\n           -1.6879e+00, -5.6755e-01],\n          [-9.1058e-01, -1.8759e+00,  1.3397e+00,  ..., -6.5463e-01,\n            2.4758e+00,  1.8893e+00],\n          [ 7.3099e-01, -1.5181e+00,  3.8525e-01,  ...,  9.1481e-02,\n            2.7941e+00,  7.3755e-01],\n          ...,\n          [ 5.8331e-01, -2.1469e-01,  6.6975e-01,  ...,  9.8887e-03,\n            3.0421e+00,  9.7136e-01],\n          [-9.5213e-01,  1.7801e+00, -4.6554e-01,  ..., -1.5791e+00,\n            3.7799e+00,  1.1569e+00],\n          [-2.1433e+00,  2.5012e+00, -1.9069e-01,  ..., -8.2747e-01,\n            3.1538e+00,  4.1107e-01]],\n\n         [[ 6.5458e-03,  4.6341e-03,  1.3568e-01,  ...,  6.9160e-01,\n            6.4990e-01,  5.2006e-01],\n          [ 4.8618e+00,  4.0616e+00,  3.7445e+00,  ...,  1.9428e+00,\n            1.0039e+00,  1.3118e+00],\n          [ 3.8549e+00,  1.3905e+00,  2.3375e+00,  ...,  7.9430e-01,\n           -8.6311e-01,  2.3621e+00],\n          ...,\n          [-3.1003e+00,  8.0585e-01, -1.8532e+00,  ..., -3.3204e-01,\n           -1.2169e+00,  1.0019e+00],\n          [-3.9758e+00,  5.6328e-01, -3.1805e+00,  ..., -5.5188e-01,\n           -1.2724e+00,  3.1984e+00],\n          [-6.5815e-01, -5.3763e-01, -4.4679e+00,  ...,  1.2719e+00,\n           -1.3666e+00,  7.4437e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.1082e-04,  3.5931e-03,  1.1365e-02,  ...,  2.7242e-02,\n           -1.2164e-02,  4.3938e-03],\n          [ 7.1937e-01, -3.0655e-01, -7.0021e-02,  ...,  7.0112e-02,\n            6.7781e-02, -5.4168e-01],\n          [ 7.3877e-01,  2.1703e-02, -1.0473e-01,  ..., -2.1154e-02,\n            2.0039e-01, -5.4422e-01],\n          ...,\n          [ 6.1225e-01, -1.2619e-01, -3.8412e-01,  ..., -1.9074e-01,\n            3.5200e-01, -5.5218e-01],\n          [ 5.8258e-01,  1.7563e-02, -1.8405e-01,  ..., -3.2325e-02,\n            1.4457e-01, -4.9312e-01],\n          [ 6.1921e-01, -1.5764e-01, -2.0414e-01,  ...,  8.6097e-02,\n           -6.4259e-02, -8.1969e-01]],\n\n         [[-1.2056e-02, -5.5574e-03,  3.8054e-02,  ..., -1.7010e-01,\n            3.2242e-02, -1.3090e-02],\n          [-1.2494e-01,  1.3325e-01,  6.3530e-02,  ...,  3.1447e-02,\n            9.3956e-02,  1.3487e-01],\n          [-2.0624e-01,  2.0004e-01,  1.4182e-01,  ..., -3.0684e-02,\n           -2.0348e-01,  2.2170e-01],\n          ...,\n          [ 8.2619e-02, -4.3028e-02,  5.5799e-02,  ..., -3.4660e-01,\n           -3.2581e-01,  5.0147e-01],\n          [ 8.1035e-02,  2.4790e-01,  1.1859e-01,  ..., -1.9436e-02,\n           -1.4303e-01,  1.4264e-01],\n          [-1.4075e-01,  2.3101e-01, -2.6854e-01,  ...,  1.1051e-01,\n           -3.3257e-01,  5.5553e-02]],\n\n         [[ 3.2441e-02,  7.3343e-03, -7.6267e-03,  ..., -6.4834e-03,\n           -1.7868e-02, -1.3072e-02],\n          [ 3.0421e-01,  8.5115e-02, -7.7033e-02,  ..., -1.5478e-01,\n            1.5215e-01, -6.6995e-02],\n          [ 3.4015e-01,  3.5630e-02, -1.5668e-02,  ..., -1.7520e-01,\n            2.5132e-01,  1.6824e-01],\n          ...,\n          [ 1.7575e-01, -3.0521e-01,  4.1171e-01,  ...,  7.2286e-02,\n           -2.5222e-01, -1.4530e-01],\n          [ 1.4619e-01,  1.0188e-01, -2.4380e-01,  ..., -1.6767e-01,\n           -3.9960e-01,  1.6171e-01],\n          [-2.3076e-01,  2.3107e-02,  4.7495e-03,  ...,  7.6523e-02,\n            3.8540e-02, -1.1297e-01]],\n\n         ...,\n\n         [[-7.2353e-03,  1.2753e-02,  5.2595e-03,  ..., -1.2895e-02,\n           -5.8279e-03, -7.6916e-03],\n          [ 2.4093e-01, -6.4406e-02,  1.2542e-01,  ...,  1.0988e-01,\n            3.0498e-01,  1.3181e-01],\n          [-1.0176e-01,  1.6784e-02, -1.7389e-01,  ..., -1.2551e-01,\n            1.2583e-01, -1.7929e-02],\n          ...,\n          [-5.6606e-02,  4.1440e-02, -2.0578e-01,  ...,  3.2717e-01,\n            1.3850e-03, -8.0089e-02],\n          [ 1.3123e-01, -6.5005e-02, -1.2321e-02,  ...,  1.7689e-02,\n            1.9605e-01, -9.2188e-02],\n          [-3.0912e-02, -2.2582e-01,  2.3516e-01,  ...,  1.8610e-01,\n            3.1048e-01, -4.0471e-02]],\n\n         [[ 2.8551e-03,  4.7691e-02, -3.8349e-02,  ...,  4.4290e-02,\n           -2.6177e-04, -3.4950e-01],\n          [ 4.8099e-01, -3.4549e-01,  1.2902e-01,  ...,  7.0070e-01,\n            3.1902e-01,  1.7167e-01],\n          [ 2.5321e-01, -3.6839e-02, -1.0152e-01,  ...,  3.4972e-01,\n            2.1394e-01,  1.3799e-01],\n          ...,\n          [ 1.2337e-01,  1.2475e-01, -2.7210e-01,  ...,  3.3952e-01,\n           -2.4348e-01, -2.5896e-01],\n          [ 1.2148e-01, -5.1883e-02, -2.5086e-01,  ...,  5.8157e-01,\n            3.1913e-03, -1.3361e-02],\n          [ 5.2966e-01, -4.1191e-02, -3.2714e-01,  ...,  2.6910e-01,\n           -2.4594e-01,  1.3633e-01]],\n\n         [[-8.1304e-03,  1.4941e-02, -1.0689e-02,  ...,  7.0810e-03,\n            5.8582e-03, -1.3757e-02],\n          [-2.2038e-01,  2.3056e-01,  2.9953e-02,  ..., -4.9608e-02,\n           -1.3103e-01, -1.4165e-01],\n          [-5.8053e-01,  9.6231e-03,  2.1723e-01,  ...,  2.3857e-01,\n            1.9165e-01,  2.1279e-02],\n          ...,\n          [-2.3823e-02,  1.5250e-01, -4.2803e-01,  ..., -1.1787e-01,\n            4.9306e-01, -3.7298e-01],\n          [-1.1632e-03,  2.6019e-03, -1.5353e-01,  ..., -2.3680e-01,\n            2.4624e-01, -7.7515e-02],\n          [-3.3636e-01, -1.6598e-01,  2.2549e-01,  ..., -7.8078e-02,\n            2.5051e-01,  2.2947e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2312e-01,  8.0175e-04,  6.5801e-02,  ...,  3.4184e-01,\n            7.8137e-01, -1.7893e-01],\n          [-2.0273e+00,  2.5088e-01,  2.2136e+00,  ...,  2.1602e+00,\n           -3.6832e+00,  5.5925e-02],\n          [ 5.8843e-01, -2.2986e+00,  2.9580e+00,  ...,  2.8248e+00,\n           -4.8020e+00, -7.1777e-01],\n          ...,\n          [ 3.2741e+00, -1.0181e+00,  8.7457e-01,  ...,  2.4720e+00,\n           -6.6660e+00, -1.2930e+00],\n          [ 3.3239e-01,  1.3234e+00,  1.0918e+00,  ...,  3.1143e+00,\n           -6.8487e+00, -1.3001e+00],\n          [-1.6789e+00,  1.6649e+00,  6.4973e-01,  ...,  4.9136e+00,\n           -5.9759e+00, -1.3086e+00]],\n\n         [[-4.0276e-02, -6.4417e-02, -5.3369e-02,  ...,  4.9013e-01,\n            9.7800e-02, -3.2408e-01],\n          [-2.2293e+00, -1.7687e+00, -1.3600e+00,  ..., -1.2043e+00,\n           -3.0898e-01, -8.6754e-01],\n          [-1.8947e+00, -4.6684e-01, -4.1457e-01,  ..., -2.0658e+00,\n           -5.5696e-02,  2.4413e-01],\n          ...,\n          [ 7.9429e-01,  2.2565e+00,  9.3412e-01,  ..., -8.6943e-01,\n           -6.8542e-01, -3.4692e-01],\n          [ 1.1933e+00,  7.4079e-01,  1.6663e+00,  ..., -9.9109e-01,\n           -1.0008e+00,  5.6319e-01],\n          [ 5.9951e-01,  1.5651e+00,  2.5191e+00,  ..., -3.8812e-02,\n           -6.5149e-01, -1.8521e-02]],\n\n         [[ 3.3561e-02,  8.4039e-02, -2.8396e-02,  ..., -4.7250e-01,\n            1.9230e+00, -2.1323e-01],\n          [ 5.7200e-01,  2.2279e+00,  1.5472e-01,  ..., -6.2765e-01,\n            8.7105e-01, -2.3120e-01],\n          [-1.0779e+00,  2.0015e-01,  1.5254e+00,  ...,  1.1903e+00,\n           -1.7934e-01,  2.8011e-02],\n          ...,\n          [-5.1735e-01, -1.3757e+00,  1.9088e+00,  ...,  4.7389e+00,\n           -1.3282e+00, -6.7751e-01],\n          [ 8.7289e-01, -1.1860e+00,  1.6457e+00,  ...,  1.5667e+00,\n           -1.8226e+00,  1.0986e-01],\n          [ 1.0428e+00, -1.9213e+00,  2.9362e-01,  ..., -5.2215e-01,\n           -1.1983e+00,  2.4624e-01]],\n\n         ...,\n\n         [[ 3.5742e-03, -1.2710e-02, -1.4841e-02,  ...,  9.3606e-03,\n           -1.9389e-01,  7.3719e-02],\n          [-4.5218e-01,  6.1590e-02,  2.1453e-01,  ...,  1.5978e+00,\n            4.0628e+00, -3.0448e-01],\n          [-2.9151e-03,  2.6232e-01,  3.1400e-01,  ...,  4.0447e+00,\n            5.4559e+00, -3.1808e+00],\n          ...,\n          [ 4.8316e-01,  1.1920e-02,  8.1735e-01,  ...,  4.6892e+00,\n            4.6682e+00, -1.2101e+00],\n          [ 6.1053e-01, -5.4636e-01,  1.2391e+00,  ...,  4.1324e+00,\n            6.1381e+00, -2.8616e+00],\n          [ 7.9651e-01,  2.1781e-01,  8.0535e-01,  ...,  4.9993e+00,\n            4.2333e+00, -2.6715e+00]],\n\n         [[ 3.2944e-02, -4.3002e-03,  7.3976e-03,  ..., -7.6260e-01,\n            7.9404e-01, -7.5428e-01],\n          [-5.1165e-01,  3.3352e-01,  1.6246e-01,  ...,  1.1631e+00,\n            1.7283e+00,  3.4625e-01],\n          [ 1.5882e-01, -2.4296e-02, -2.3312e-01,  ...,  1.3329e+00,\n            8.8855e-01, -4.3772e-02],\n          ...,\n          [ 3.1424e-01, -2.7989e-01, -6.4704e-01,  ...,  2.0611e+00,\n            8.6480e-01,  1.1318e+00],\n          [ 1.9290e-02,  2.0101e-01, -5.4484e-01,  ...,  1.2569e+00,\n            5.4770e-01,  2.0927e+00],\n          [ 1.4652e-01,  7.5938e-01, -9.8990e-02,  ...,  1.3441e+00,\n            1.2334e+00,  3.1849e+00]],\n\n         [[-2.4856e-02,  5.0377e-02, -2.0058e-02,  ..., -7.2910e-02,\n            1.7153e+00,  6.1020e-01],\n          [ 1.2633e+00,  1.7550e+00, -1.0946e+00,  ..., -4.0205e-01,\n           -3.2863e+00,  1.8942e+00],\n          [ 1.4678e+00,  9.7763e-01, -9.2193e-01,  ..., -1.7589e-01,\n           -3.5836e+00,  3.3629e+00],\n          ...,\n          [-4.3405e-01,  4.0004e-01, -1.2932e+00,  ..., -8.0985e-01,\n           -2.9375e+00,  3.4383e+00],\n          [-1.2732e+00,  6.0986e-02, -6.0903e-01,  ..., -6.1697e-01,\n           -3.2768e+00,  3.0371e+00],\n          [-6.4617e-01, -2.6350e-01, -7.1758e-01,  ..., -1.4704e+00,\n           -2.5376e+00,  4.9321e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-9.8676e-03,  1.1584e-03, -2.5280e-03,  ...,  1.0427e-03,\n           -1.0717e-02, -7.4770e-03],\n          [-7.7805e-01, -9.1382e-02, -8.1707e-02,  ..., -4.8083e-01,\n           -7.2615e-01,  4.8550e-01],\n          [-1.7969e-01,  5.2268e-03, -3.1901e-01,  ..., -2.2387e-01,\n           -2.4940e-01,  4.2651e-02],\n          ...,\n          [ 4.2639e-01, -4.6097e-01,  9.1851e-02,  ...,  2.3704e-01,\n           -1.4049e-01,  4.5012e-01],\n          [ 5.4303e-02,  2.1139e-01, -2.1290e-01,  ..., -2.3935e-01,\n            1.9690e-01, -1.4556e-01],\n          [ 6.2408e-01,  1.3162e-01,  5.5379e-02,  ..., -1.1796e-01,\n            1.4404e-01, -4.0336e-01]],\n\n         [[ 2.7190e-01,  1.4988e-02, -2.0839e-03,  ...,  3.9396e-03,\n           -1.3605e-02, -1.2758e-03],\n          [-2.7506e-01,  9.9242e-02,  5.8154e-03,  ..., -7.6369e-02,\n            3.0214e-01,  5.2639e-02],\n          [-7.1955e-01,  1.5563e-01, -2.2284e-01,  ...,  2.7428e-02,\n           -1.4784e-02,  1.6643e-01],\n          ...,\n          [-8.5632e-01, -1.5496e-01, -6.9391e-03,  ..., -3.6154e-01,\n           -1.1608e-01,  7.1647e-02],\n          [-4.2170e-01,  4.5486e-02,  1.2847e-01,  ..., -1.3755e-01,\n           -4.6646e-01,  1.7543e-01],\n          [-9.4145e-01, -2.0402e-02, -8.6336e-02,  ..., -6.6149e-02,\n           -3.6288e-01,  3.5880e-01]],\n\n         [[ 3.8737e-02,  1.7042e-02, -4.3715e-03,  ...,  3.2801e-02,\n            3.5758e-03, -2.1997e-01],\n          [ 6.4525e-02, -1.3032e-01, -7.0709e-02,  ..., -1.1451e-01,\n           -1.4236e-01, -5.1637e-01],\n          [ 1.2331e-01,  7.8746e-02, -9.9049e-03,  ..., -1.1019e-01,\n           -2.0571e-01, -1.6622e-01],\n          ...,\n          [-5.6203e-02, -3.0834e-02,  8.1040e-02,  ...,  2.3187e-01,\n           -3.1130e-01, -1.1976e-01],\n          [ 5.7486e-02, -1.6409e-01,  9.7397e-02,  ...,  8.6678e-02,\n           -1.8303e-01, -2.1524e-01],\n          [ 9.5699e-02, -9.4756e-02,  1.3789e-01,  ..., -1.6573e-01,\n           -5.1339e-02, -5.8899e-01]],\n\n         ...,\n\n         [[ 1.0134e-02,  8.3906e-03, -1.4678e-02,  ...,  2.0061e-02,\n            8.9872e-03, -4.0096e-03],\n          [-6.4184e-01,  2.4045e-01,  9.8672e-02,  ...,  4.4117e-02,\n            9.3468e-02, -7.7275e-01],\n          [-7.0650e-01, -4.5859e-01, -1.3923e-01,  ..., -4.0585e-01,\n           -2.4234e-02,  5.8095e-01],\n          ...,\n          [-7.0595e-01,  4.2824e-02, -1.0454e-01,  ..., -1.7282e-01,\n            3.1412e-03,  4.0021e-02],\n          [-1.8547e-01, -5.4997e-01,  5.4040e-01,  ...,  8.5589e-01,\n           -6.9312e-01, -6.8143e-02],\n          [-4.1477e-02, -3.4422e-01, -1.2055e-01,  ..., -5.7615e-01,\n           -6.1381e-01,  3.2405e-01]],\n\n         [[ 4.6773e-03, -6.1752e-03, -1.3456e-02,  ...,  2.2459e-03,\n           -1.4624e-02,  1.7275e-02],\n          [ 1.0933e-01, -2.4646e-01,  4.2456e-01,  ..., -5.0560e-02,\n           -5.1515e-01,  7.0203e-02],\n          [-5.8753e-01, -2.8683e-01,  5.6241e-01,  ...,  8.7894e-02,\n           -5.3783e-01, -1.3549e-01],\n          ...,\n          [-3.6709e-01,  1.0368e-02, -2.6492e-01,  ...,  3.0226e-01,\n            1.7847e-01, -3.9443e-01],\n          [-1.8174e-01, -1.5944e-01, -1.2444e-01,  ...,  1.7185e-01,\n           -1.4663e-01, -3.7696e-01],\n          [ 8.7055e-02, -4.1232e-02,  1.1925e-02,  ..., -4.3146e-02,\n           -3.0805e-01, -1.8966e-01]],\n\n         [[ 9.5875e-03,  1.9877e-03,  5.5560e-04,  ...,  1.6537e-02,\n            1.5532e-03,  1.2090e-02],\n          [-2.8249e-02,  3.7383e-02,  3.8696e-01,  ..., -5.9451e-02,\n            1.7590e-01, -3.6832e-02],\n          [-2.0332e-03, -1.6847e-01,  1.1968e-02,  ...,  6.5603e-02,\n           -1.3205e-01,  2.7796e-01],\n          ...,\n          [-5.3895e-01, -9.4709e-02, -2.4603e-01,  ..., -9.8091e-02,\n            1.9656e-01, -9.3505e-02],\n          [-1.3160e-01,  2.0801e-01, -1.3776e-01,  ..., -5.2161e-02,\n           -6.2677e-02,  3.4716e-02],\n          [-1.6502e-01,  1.7395e-01,  9.3074e-02,  ..., -6.8535e-02,\n           -1.7071e-01,  6.4562e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.5351e-02, -8.4757e-02, -6.0248e-02,  ...,  6.7227e-01,\n            1.3881e+00, -7.4223e-01],\n          [ 1.2616e+00, -1.2370e+00, -7.4736e-01,  ...,  1.2731e+00,\n           -1.0633e+00, -4.8718e-01],\n          [ 1.1614e-01,  1.6054e-01, -4.4539e-01,  ...,  4.4137e-01,\n           -1.3666e+00,  1.7992e-01],\n          ...,\n          [-5.4168e-01, -2.6959e-01,  6.0514e-01,  ..., -2.5940e-01,\n           -5.2639e-01, -1.0426e+00],\n          [-5.8534e-01, -1.8146e-01,  1.1400e+00,  ..., -2.3092e-01,\n           -1.5982e+00, -1.6913e-01],\n          [ 7.7766e-01,  1.6303e+00,  2.2796e+00,  ..., -5.4150e-01,\n           -1.6284e+00, -1.7295e+00]],\n\n         [[-1.8946e-02, -2.2157e-02,  2.3029e-02,  ...,  3.3776e-01,\n            1.0489e-01, -1.6274e+00],\n          [-2.2000e-01, -1.2279e-01,  4.6447e-01,  ...,  4.0435e-01,\n           -1.0897e+00,  5.0793e+00],\n          [ 3.8183e-01, -1.3501e-01,  9.4150e-01,  ...,  3.5868e+00,\n           -2.5323e+00,  3.0625e+00],\n          ...,\n          [ 7.1628e-01,  2.1930e-01,  6.7505e-01,  ...,  2.6556e+00,\n           -5.0718e-01,  4.5530e+00],\n          [-5.0165e-01, -7.0535e-01, -3.4468e-01,  ...,  3.8036e+00,\n           -1.9527e+00,  3.5220e+00],\n          [-6.0048e-01, -9.2246e-01, -7.6930e-01,  ...,  4.0905e+00,\n           -1.6349e+00,  3.0920e+00]],\n\n         [[ 2.1701e-02, -9.1714e-03,  9.7642e-02,  ...,  9.4590e-01,\n           -2.8308e-01, -5.0261e-01],\n          [ 6.7626e+00, -2.7791e+00,  3.9439e+00,  ...,  2.1430e+00,\n           -6.6616e-01,  9.2521e-02],\n          [ 5.1229e+00, -4.7000e+00,  2.7205e+00,  ...,  1.3188e+00,\n            1.8473e+00,  2.0944e+00],\n          ...,\n          [-6.0245e+00, -1.9102e+00,  4.0358e-01,  ...,  2.3604e+00,\n            1.3408e+00,  1.3355e+00],\n          [-5.4936e+00,  2.1390e+00, -1.3403e-01,  ...,  2.7181e+00,\n            2.1750e+00,  1.3737e+00],\n          [-1.2934e+00,  4.0860e+00, -2.1220e+00,  ...,  2.4340e+00,\n            3.3896e-01,  1.1140e+00]],\n\n         ...,\n\n         [[ 5.4727e-03,  1.4969e-02, -4.3817e-02,  ..., -3.0039e-01,\n            1.1738e-01,  2.3453e+00],\n          [-9.3585e-01,  1.6610e+00, -1.2045e+00,  ..., -1.0590e+00,\n            6.9103e-01, -2.5385e+00],\n          [-8.8610e-01,  2.2702e+00,  1.1747e+00,  ..., -1.7107e+00,\n           -1.5240e+00, -3.1733e+00],\n          ...,\n          [ 9.4467e-01, -1.8372e-01, -2.0256e-02,  ...,  2.1084e-01,\n           -2.7350e+00, -4.0278e+00],\n          [ 1.1101e+00, -2.2151e+00,  1.6296e+00,  ..., -1.1708e+00,\n           -1.5052e+00, -4.0883e+00],\n          [ 8.3699e-01, -2.0846e+00,  2.7970e-01,  ...,  8.6965e-01,\n           -2.0266e+00, -3.0972e+00]],\n\n         [[ 2.7228e-02,  2.5961e-02, -3.5073e-02,  ...,  5.4443e-01,\n            4.4475e-01,  8.4028e-01],\n          [ 2.2954e-01,  1.0210e+00, -1.6859e+00,  ...,  3.5193e+00,\n            6.4974e-01, -5.3879e-01],\n          [ 1.8964e-01,  9.0448e-02,  8.7046e-01,  ...,  2.1939e+00,\n           -3.3477e-01,  5.3138e-01],\n          ...,\n          [ 1.9603e-01, -2.4385e-01, -6.1381e-01,  ...,  2.2383e+00,\n           -1.2850e+00, -4.3273e-01],\n          [-1.3237e+00, -1.1143e+00, -2.3366e-01,  ...,  1.7414e+00,\n           -3.3808e-02, -9.9236e-01],\n          [-1.7050e+00, -6.5092e-02, -6.4913e-01,  ...,  1.6118e+00,\n           -6.3670e-01, -3.3138e-01]],\n\n         [[ 8.4090e-02, -6.8144e-02, -1.7702e-02,  ..., -3.3280e-01,\n            1.3528e+00,  6.3329e-01],\n          [ 1.9053e+00, -5.0859e+00,  1.6806e+00,  ..., -2.0356e-01,\n           -4.8222e+00,  9.6508e-01],\n          [-2.3532e+00, -4.2028e+00,  2.2080e+00,  ...,  1.7840e-01,\n           -5.2344e+00,  8.7347e-01],\n          ...,\n          [-2.2304e+00,  1.1961e+00,  1.8620e+00,  ..., -8.3341e-01,\n           -5.6237e+00,  9.5758e-01],\n          [ 1.8058e+00,  3.1312e+00,  1.1766e+00,  ...,  4.3110e-01,\n           -5.3096e+00, -4.7511e-01],\n          [ 3.0188e+00,  3.3326e+00,  9.3646e-01,  ..., -1.9392e-01,\n           -5.4617e+00, -5.2820e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-1.5132e-02,  1.8012e-02,  1.9541e-02,  ...,  9.7809e-03,\n            2.9872e-03, -3.5270e-02],\n          [ 1.4281e-01, -1.1690e-01, -2.1661e-01,  ..., -5.3711e-02,\n           -2.7509e-01,  1.8915e-01],\n          [ 2.4982e-01, -4.4841e-01, -1.2889e-01,  ..., -3.4453e-01,\n            3.0005e-01,  2.4777e-01],\n          ...,\n          [ 3.6535e-01, -1.3034e-01, -1.3958e-01,  ..., -2.6643e-01,\n            4.5300e-01,  2.8213e-01],\n          [ 2.4861e-01, -2.5750e-01, -1.0779e-01,  ..., -4.1795e-01,\n            1.6677e-01,  4.0313e-01],\n          [ 3.0019e-01, -3.3321e-01, -1.2242e-01,  ...,  1.9265e-01,\n            5.1266e-01, -1.3264e-01]],\n\n         [[-1.9224e-02, -3.4200e-03, -4.3395e-03,  ..., -7.5942e-03,\n           -1.1307e-02,  1.7963e-02],\n          [ 2.5455e-01, -1.8495e-01,  4.3413e-01,  ..., -5.0476e-01,\n           -5.0250e-01,  8.9971e-02],\n          [ 3.3818e-02, -3.1044e-02,  5.4943e-02,  ..., -8.5431e-02,\n           -2.6772e-02, -1.2708e-01],\n          ...,\n          [-2.8894e-01, -3.3197e-02,  1.5209e-01,  ..., -5.2791e-01,\n            1.9384e-01, -4.7756e-02],\n          [-1.1962e-01,  4.4641e-02, -1.0454e-01,  ..., -8.0905e-02,\n           -6.1556e-02,  1.7375e-01],\n          [ 2.4786e-01, -8.8134e-02, -2.4973e-01,  ..., -1.6788e-01,\n           -1.0659e-01,  3.0257e-02]],\n\n         [[-1.0082e-01, -3.0075e-02,  4.1509e-02,  ..., -7.7669e-01,\n           -1.3114e-02,  1.2001e-02],\n          [ 4.5432e-02,  3.4684e-02,  1.6137e-03,  ...,  2.4898e-01,\n            6.4173e-02,  8.2674e-03],\n          [ 3.7278e-02,  2.0497e-01,  1.1244e-01,  ...,  4.9411e-01,\n            7.0016e-02,  1.0976e-01],\n          ...,\n          [-2.1570e-01, -2.0606e-01, -2.4016e-01,  ...,  4.4447e-01,\n           -7.3380e-02,  6.9049e-02],\n          [-1.0900e-02, -3.4667e-01,  2.0258e-01,  ...,  1.1281e-01,\n           -2.3070e-01,  1.1161e-01],\n          [ 3.9688e-01, -3.2756e-01, -1.8547e-01,  ...,  7.6121e-01,\n           -1.7677e-01, -1.0005e-01]],\n\n         ...,\n\n         [[-5.7387e-03,  2.5267e-02, -2.2123e-02,  ...,  1.1183e-02,\n           -2.7630e-03,  1.7944e-02],\n          [-1.3462e-01, -4.6760e-01, -4.1073e-01,  ..., -3.3048e-02,\n           -1.3100e-01, -3.1595e-01],\n          [-7.4877e-03,  1.3933e-01, -5.2769e-02,  ..., -9.8085e-02,\n            1.9186e-01,  1.9385e-02],\n          ...,\n          [-1.2679e-01,  3.3661e-01, -4.8950e-02,  ...,  7.4261e-03,\n            2.2402e-01,  3.6038e-01],\n          [-1.7555e-01, -4.5212e-02,  6.4099e-02,  ...,  2.6169e-02,\n            1.2100e-01, -1.3538e-02],\n          [-8.9102e-02, -9.6494e-02,  8.1932e-02,  ...,  9.2361e-02,\n            2.7794e-01,  3.1251e-01]],\n\n         [[ 1.8212e-01, -1.8599e-02, -2.1570e-02,  ...,  3.7844e-03,\n            1.8826e-02, -2.6143e-02],\n          [-4.6325e-01,  1.6244e-01,  2.2072e-01,  ..., -6.9552e-02,\n            6.1167e-01,  2.6188e-02],\n          [-5.3045e-03,  8.4458e-02,  6.6047e-02,  ...,  1.6932e-02,\n            2.0987e-01,  9.1609e-02],\n          ...,\n          [-4.8462e-01,  1.1965e-01,  8.3614e-02,  ..., -4.0738e-02,\n            3.4050e-01, -1.3341e-01],\n          [-3.5824e-01,  2.9223e-01,  1.8767e-02,  ...,  2.0220e-01,\n            6.3876e-02, -3.2615e-02],\n          [-7.1602e-01,  2.7131e-01, -1.1609e-01,  ...,  2.3846e-01,\n           -2.6324e-01, -6.5402e-02]],\n\n         [[-6.5726e-03,  3.8399e-03, -1.3766e-02,  ...,  6.9220e-03,\n           -4.0670e-03, -1.2715e-02],\n          [ 3.6617e-01,  2.9218e-01,  2.4028e-02,  ..., -2.0067e-04,\n           -1.1130e-01, -1.2831e-01],\n          [ 4.3841e-01, -1.9646e-01, -2.7185e-02,  ..., -6.6065e-01,\n            3.1660e-02,  2.8755e-02],\n          ...,\n          [ 2.4336e-01,  1.5147e-01, -4.0727e-01,  ...,  1.4628e-01,\n            2.9974e-02, -1.9604e-01],\n          [ 6.2762e-01,  3.9751e-01, -2.2898e-01,  ..., -5.8547e-03,\n           -1.4339e-01, -2.3870e-01],\n          [ 6.9609e-01,  3.3760e-03, -1.3496e-01,  ...,  1.7550e-01,\n           -7.9604e-02, -1.1876e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0166e-01,  6.1919e-03, -9.3783e-03,  ..., -4.7718e-02,\n           -5.6272e-01, -5.0096e-01],\n          [ 2.0590e+00, -1.9600e+00, -1.1147e+00,  ...,  1.0343e+00,\n            9.8983e-01,  2.3284e+00],\n          [-1.7154e+00, -3.9402e+00, -2.2778e+00,  ...,  9.1845e-01,\n            3.9979e+00,  2.5265e+00],\n          ...,\n          [-2.0346e+00, -1.5620e+00, -2.1357e+00,  ...,  1.7008e+00,\n            6.1601e+00,  8.7485e-01],\n          [ 1.3886e+00,  1.3091e+00, -8.0861e-01,  ..., -2.7334e-01,\n            4.8732e+00,  1.8793e+00],\n          [ 2.9248e+00,  2.4528e+00, -9.1368e-01,  ...,  2.1388e-01,\n            3.8603e+00,  1.6429e+00]],\n\n         [[-5.6379e-02,  1.4577e-03,  1.8990e-02,  ..., -1.5364e-01,\n            3.8202e-01, -2.9403e-01],\n          [-1.1381e+00, -4.8696e-01,  1.2164e+00,  ...,  1.4825e+00,\n           -1.1934e+00, -2.6655e+00],\n          [ 9.1242e-01,  1.2252e-01, -3.6523e-01,  ...,  1.1115e-02,\n           -1.4870e+00, -2.2476e+00],\n          ...,\n          [-1.7791e-01, -3.7555e-01, -1.2161e+00,  ...,  5.8366e-02,\n           -2.0055e+00, -3.0933e+00],\n          [-1.6694e+00, -4.3770e-01, -8.6095e-01,  ..., -9.8965e-02,\n           -6.4224e-01, -2.4396e+00],\n          [-6.8991e-01,  6.9570e-01, -1.6520e+00,  ...,  6.1562e-01,\n           -9.9733e-01, -2.7517e+00]],\n\n         [[ 1.8466e-02,  1.4924e-03,  1.2577e-02,  ..., -3.4818e-01,\n           -2.3574e-01, -2.2659e-02],\n          [-3.1628e-01, -1.8836e-01, -2.3994e+00,  ..., -1.8448e-01,\n            3.8541e-01,  1.1383e+00],\n          [ 4.1570e-01, -8.1133e-01, -5.0007e-01,  ...,  1.0797e+00,\n           -1.2453e+00,  8.1130e-01],\n          ...,\n          [ 5.7306e-01, -1.7102e+00, -1.1349e+00,  ...,  6.5464e-01,\n           -7.6430e-01, -1.3933e-01],\n          [-6.3758e-01, -2.0501e-01, -1.1317e+00,  ...,  1.5231e+00,\n           -9.7394e-01, -3.4505e-01],\n          [-8.7936e-01,  1.6732e-01,  1.6500e-02,  ...,  1.8085e+00,\n            3.6900e-01,  4.4737e-01]],\n\n         ...,\n\n         [[ 2.0947e-02, -6.7295e-03, -6.2162e-02,  ..., -1.3262e-01,\n            1.1672e+00,  8.1977e-01],\n          [-4.1865e+00, -2.6542e+00, -2.7694e+00,  ...,  2.7021e-01,\n           -5.0194e+00, -8.1190e-01],\n          [-1.7897e+00, -8.9176e-01, -1.9546e+00,  ..., -8.6345e-01,\n           -5.0271e+00, -7.0862e-01],\n          ...,\n          [ 1.1249e+00,  8.6208e-01,  1.1570e+00,  ...,  5.2513e-02,\n           -6.3273e+00,  8.7164e-02],\n          [ 1.5300e+00,  1.0404e+00,  5.0218e-01,  ..., -8.9805e-01,\n           -3.1471e+00, -1.1067e+00],\n          [ 1.2062e+00,  1.5224e+00,  2.3872e+00,  ...,  3.7257e-02,\n           -6.3237e+00,  2.5327e-01]],\n\n         [[-1.0139e-02, -8.0883e-03, -4.1945e-02,  ...,  1.4360e+00,\n           -3.6248e-01,  9.7605e-01],\n          [ 8.9840e-01,  2.1517e+00, -3.8953e-01,  ..., -3.9081e+00,\n            1.1726e+00, -1.3648e+00],\n          [ 1.7977e+00,  7.8718e-01,  1.1889e+00,  ..., -4.4897e+00,\n            2.2454e+00, -1.9135e+00],\n          ...,\n          [-2.4455e-01,  4.0773e-04,  2.1442e-01,  ..., -5.0472e+00,\n            2.9887e+00, -1.8345e+00],\n          [-1.7546e+00, -9.5721e-01,  1.3035e+00,  ..., -5.8629e+00,\n            1.4618e+00, -1.4617e+00],\n          [-6.8578e-01, -1.5225e-01,  1.0087e+00,  ..., -5.0745e+00,\n            1.5732e+00, -1.4222e+00]],\n\n         [[-2.0678e-02, -2.5024e-05, -2.5933e-02,  ...,  9.0611e-01,\n            1.1658e+00, -1.5922e-02],\n          [ 5.4387e-01, -1.5937e+00, -4.2302e-01,  ...,  8.4128e-02,\n           -3.1649e+00, -6.3091e-01],\n          [ 2.1531e+00, -3.2726e-01, -8.0747e-01,  ..., -9.3588e-01,\n           -1.2575e+00, -2.0457e+00],\n          ...,\n          [-1.9525e+00,  9.4569e-01,  1.2079e+00,  ..., -2.9127e+00,\n           -6.6631e-01, -4.0533e+00],\n          [-2.2828e+00,  5.9731e-01,  1.1725e+00,  ..., -1.2854e+00,\n           -1.6468e+00, -2.0581e+00],\n          [-8.2825e-01, -4.9824e-01,  2.1773e+00,  ..., -6.2644e-01,\n           -1.7132e+00, -1.9862e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3526e-02, -7.9978e-03, -6.1671e-03,  ...,  1.7907e-02,\n           -1.5949e-02, -1.2638e-02],\n          [-1.6625e-01, -7.3503e-02,  3.6931e-01,  ..., -4.4330e-01,\n            1.7235e-01,  8.7753e-01],\n          [-1.5399e-01,  2.1759e-01,  1.5064e-01,  ...,  1.4731e-02,\n            1.5621e-02,  8.8263e-02],\n          ...,\n          [-3.0861e-01,  4.5957e-01,  1.2916e-01,  ..., -2.9938e-02,\n           -6.1752e-02, -5.2606e-01],\n          [-3.3056e-01,  2.1115e-01, -1.8068e-01,  ...,  5.4267e-02,\n            7.9759e-02, -3.4859e-01],\n          [ 3.3028e-02,  1.7922e-01,  1.0573e-01,  ..., -2.1721e-01,\n            5.2501e-01, -4.0131e-01]],\n\n         [[-4.8035e-02,  1.9834e-02,  3.9587e-03,  ...,  1.7674e-02,\n            1.0605e-02,  3.7298e-04],\n          [ 1.6199e-01, -1.5647e-01,  2.5685e-01,  ..., -5.2654e-01,\n           -2.7026e-02, -5.9948e-02],\n          [ 2.1545e-01, -1.0643e-01,  4.2412e-02,  ...,  3.6600e-02,\n           -9.2282e-02, -6.1496e-02],\n          ...,\n          [ 3.1842e-01,  1.9338e-01, -3.2992e-01,  ..., -4.8378e-01,\n            3.4042e-02, -2.8658e-01],\n          [ 2.1974e-01,  1.9647e-01, -8.0642e-02,  ..., -1.7587e-01,\n           -1.4707e-01,  3.5467e-01],\n          [ 4.8984e-01,  6.0971e-02, -2.0218e-01,  ..., -5.5265e-02,\n           -1.0686e-02, -4.7457e-01]],\n\n         [[-8.0104e-03, -2.1802e-02,  1.5194e-03,  ..., -4.4974e-02,\n            3.2744e-02,  2.1210e-02],\n          [ 1.2057e-01,  1.1131e-02, -1.6751e-02,  ...,  4.2662e-01,\n            3.3568e-01,  1.9689e-01],\n          [ 1.8086e-01,  3.0699e-01, -8.3206e-02,  ..., -6.2404e-02,\n            4.9114e-01,  7.2837e-03],\n          ...,\n          [ 2.0506e-01,  2.7964e-01,  2.9877e-01,  ...,  5.7278e-02,\n            5.4048e-01, -6.4822e-02],\n          [ 3.4822e-01,  8.7741e-02,  2.1238e-01,  ..., -1.2006e-01,\n            4.1560e-01, -7.6328e-02],\n          [-7.0437e-02,  5.7161e-02,  1.2167e-01,  ..., -1.1188e-01,\n            1.5424e-01, -9.5308e-02]],\n\n         ...,\n\n         [[ 3.5515e-03, -2.1943e-02, -2.0210e-03,  ..., -1.0336e-02,\n            2.3350e-02, -1.0455e-02],\n          [ 2.4865e-01,  4.3538e-01,  4.5029e-01,  ..., -7.2358e-02,\n            1.0188e-01, -1.8383e-01],\n          [-1.0518e-01,  5.2964e-01,  3.7270e-01,  ..., -6.7872e-02,\n            1.3131e-01, -2.8911e-01],\n          ...,\n          [ 3.0064e-01,  5.5310e-01, -5.4533e-01,  ...,  1.4647e-01,\n           -1.5924e-01, -1.5693e-01],\n          [ 3.8755e-01,  3.9562e-01, -5.5967e-01,  ...,  1.4331e-01,\n           -9.4329e-02, -3.2281e-01],\n          [-3.3547e-02,  3.9899e-01, -2.8447e-01,  ..., -6.4645e-02,\n            6.6053e-02, -1.9320e-01]],\n\n         [[ 1.4852e-03,  9.5778e-03,  2.6551e-03,  ..., -1.0983e-02,\n            3.9189e-03,  1.4490e-02],\n          [ 1.8509e-02,  1.6305e-01,  1.6417e-01,  ...,  1.5492e-02,\n           -1.6470e-01,  1.1245e-01],\n          [-4.3813e-01,  3.9587e-01, -1.9904e-01,  ..., -2.9899e-01,\n            3.3618e-02, -3.4078e-02],\n          ...,\n          [-3.3382e-01,  5.3112e-01, -2.7433e-01,  ...,  1.8164e-01,\n            6.9968e-02, -5.8562e-02],\n          [-3.6744e-01,  6.0500e-01, -4.2692e-01,  ...,  1.8678e-01,\n            2.2787e-01,  1.1091e-01],\n          [-5.3681e-01,  6.5478e-01, -2.8589e-01,  ..., -3.1608e-01,\n           -2.2584e-01, -3.8646e-01]],\n\n         [[-2.9645e-01, -1.9910e-02, -1.7448e-02,  ...,  1.3408e-02,\n           -3.6502e-02, -1.3350e-02],\n          [ 7.8251e-01, -7.2778e-01,  9.3247e-01,  ...,  4.0642e-01,\n           -2.4694e-01,  6.0712e-01],\n          [ 6.3364e-01, -1.7825e-01,  2.1098e-01,  ..., -1.4767e-01,\n           -6.5873e-02,  1.0685e-02],\n          ...,\n          [ 2.3564e-01,  4.9659e-03,  2.9801e-01,  ..., -9.7692e-02,\n           -2.0164e-01,  1.4818e-01],\n          [ 5.6062e-01, -1.8731e-01, -3.0178e-01,  ..., -1.6078e-01,\n            5.3026e-04,  1.7745e-01],\n          [ 3.5423e-01, -2.3672e-01, -7.3669e-02,  ...,  1.7488e-01,\n            2.8239e-01,  1.1818e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-4.9875e-02,  2.4972e-03, -1.3115e-02,  ..., -1.0506e-01,\n           -1.0948e+00,  7.7103e-01],\n          [ 1.4863e+00,  1.5200e+00, -1.1959e+00,  ..., -1.3442e+00,\n            3.9415e+00, -5.7495e-01],\n          [-1.0452e-01,  3.3289e-01,  1.0234e-01,  ..., -2.8023e+00,\n            4.7230e+00, -3.0324e+00],\n          ...,\n          [-1.1804e-01, -2.6029e-01,  5.8443e-01,  ..., -9.4457e-01,\n            3.2178e+00, -3.4486e+00],\n          [-2.7280e-02, -4.2073e-01,  8.1108e-01,  ..., -3.5189e-01,\n            4.3311e+00, -3.7826e+00],\n          [-7.3906e-02, -1.3747e+00,  3.9041e-02,  ..., -6.8392e-01,\n            3.2543e+00, -2.6435e+00]],\n\n         [[-3.5304e-02,  2.9584e-03, -2.7550e-02,  ..., -3.4000e-01,\n            1.1422e+00,  2.6492e-02],\n          [-5.6816e-02,  4.5117e-03, -1.3083e+00,  ...,  1.1244e+00,\n           -9.8961e-02,  1.7672e+00],\n          [ 1.8967e+00,  6.0789e-01,  1.9745e-01,  ..., -6.9852e-01,\n           -1.4396e+00,  2.8527e-01],\n          ...,\n          [-5.7954e-01,  6.8881e-01,  7.5959e-01,  ..., -6.2134e-01,\n           -1.2529e+00,  5.6745e-01],\n          [-1.5116e+00,  3.8163e-01,  1.6837e+00,  ...,  4.5959e-01,\n           -1.1081e+00,  9.8418e-01],\n          [-1.2084e+00, -1.3505e-01,  2.4064e+00,  ..., -2.4949e-01,\n            7.0691e-01,  7.7508e-01]],\n\n         [[ 3.7110e-02,  1.0985e-01,  7.0407e-02,  ..., -9.0719e-01,\n           -1.1212e+00,  1.1719e+00],\n          [-4.4360e+00,  2.4716e+00,  1.2563e+00,  ...,  3.4452e+00,\n            1.7526e+00, -2.0145e-01],\n          [-3.4237e+00,  1.0477e+00,  2.8551e+00,  ...,  2.7749e+00,\n            9.4620e-01, -1.3547e+00],\n          ...,\n          [ 2.9569e+00, -1.6506e+00,  4.2026e+00,  ...,  4.4533e+00,\n            1.0390e+00, -1.0966e+00],\n          [ 4.2534e+00, -2.2310e+00,  2.0792e+00,  ...,  2.6211e+00,\n            1.2067e+00, -7.3345e-01],\n          [ 3.0529e+00, -1.4761e+00,  4.9123e-01,  ...,  3.2055e+00,\n            1.5274e+00, -1.3190e+00]],\n\n         ...,\n\n         [[ 2.9833e-03, -9.0822e-03,  1.8295e-02,  ...,  6.1501e-01,\n           -1.1246e-01,  8.5372e-01],\n          [ 3.3304e-01, -6.4003e-01, -1.0934e+00,  ..., -3.9152e+00,\n            4.6173e-01, -5.7613e-01],\n          [-5.1271e-02, -3.1483e-01, -2.7131e-01,  ..., -3.7386e+00,\n           -2.9720e-01, -2.6095e+00],\n          ...,\n          [-2.3231e-01,  7.7547e-01, -3.5580e-01,  ..., -4.7908e+00,\n            1.2966e+00, -2.2226e+00],\n          [-2.2539e-03,  1.0059e+00,  2.9037e-01,  ..., -3.9186e+00,\n            3.1690e+00, -2.5507e+00],\n          [-2.3137e-01,  4.1141e-01,  6.0183e-01,  ..., -4.8506e+00,\n            3.3978e+00, -3.1643e+00]],\n\n         [[ 3.1923e-01,  6.5008e-02, -1.5732e-01,  ...,  2.5957e-01,\n           -6.0922e-01,  5.0128e-01],\n          [ 9.7237e+00,  4.0054e+00, -2.1038e+00,  ...,  1.1219e+00,\n           -1.1703e+00,  1.6707e-01],\n          [ 7.4607e-01,  4.6296e+00, -1.1777e+00,  ...,  1.3249e+00,\n           -1.6541e+00,  1.2219e+00],\n          ...,\n          [-1.0187e+01, -5.7691e-01,  2.1336e+00,  ..., -3.7384e-02,\n           -9.9703e-01,  1.5344e+00],\n          [-2.8958e+00, -4.0741e+00,  2.4365e+00,  ...,  1.3634e-01,\n           -1.3923e+00,  1.4176e+00],\n          [ 7.5061e+00, -3.6183e+00,  3.2269e+00,  ...,  6.6178e-01,\n           -1.4693e+00,  1.5031e+00]],\n\n         [[-4.7712e-02, -1.6998e-02,  4.7513e-02,  ..., -2.0291e+00,\n           -3.1721e-01, -1.9669e+00],\n          [ 4.6644e-01, -2.0919e-01,  3.7741e+00,  ...,  1.8680e+00,\n            9.3282e-01,  4.8208e-01],\n          [ 1.7686e+00, -1.0605e+00,  2.7540e+00,  ...,  4.3334e+00,\n           -7.2462e-01,  1.3663e+00],\n          ...,\n          [ 7.0148e-01, -1.6251e-01,  9.9140e-02,  ...,  4.4427e+00,\n           -2.3735e+00,  2.5939e+00],\n          [-1.2951e+00,  1.2367e+00,  2.1290e-03,  ...,  4.9486e+00,\n           -1.9682e-01,  2.4470e+00],\n          [-1.8347e+00,  2.0486e+00,  7.5327e-02,  ...,  4.4172e+00,\n            5.1234e-01,  1.1785e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.5693e-03,  3.5057e-02,  1.6987e-02,  ..., -3.8882e-02,\n           -8.0818e-02,  3.6926e-02],\n          [ 8.1845e-02,  6.0510e-01,  6.9635e-01,  ..., -8.5384e-01,\n            5.5322e-02, -1.0471e+00],\n          [-9.1735e-02,  2.2055e-01, -3.3842e-01,  ..., -3.2321e-01,\n            8.8266e-01,  3.1458e-01],\n          ...,\n          [-2.6894e-01, -3.2540e-01, -4.2203e-01,  ..., -2.5452e-01,\n            4.4596e-02, -5.8919e-01],\n          [ 1.0207e-01,  1.5196e-01, -8.9608e-01,  ...,  1.8337e-01,\n           -1.6134e-01, -1.1361e-02],\n          [ 2.3689e-01,  9.0550e-01,  2.4622e-01,  ...,  2.5552e-01,\n            1.2596e-01,  2.7388e-01]],\n\n         [[-8.8539e-03, -1.8422e-02, -2.0399e-02,  ...,  2.3280e-02,\n           -7.1595e-03, -4.6043e-03],\n          [ 3.3777e-03, -1.4677e-01,  3.8548e-01,  ..., -2.8452e-03,\n           -2.4198e-01,  2.2888e-01],\n          [-4.7083e-02, -3.2774e-02,  3.5672e-01,  ...,  8.3398e-02,\n            3.9802e-01,  9.0960e-02],\n          ...,\n          [ 2.5897e-01,  2.0614e-01, -2.2329e-01,  ...,  2.8248e-01,\n            3.0558e-01,  4.1906e-01],\n          [ 1.6578e-01,  1.4408e-01,  1.6520e-01,  ...,  3.1801e-01,\n            1.2584e-01,  1.9883e-01],\n          [-5.2341e-01, -2.3311e-02,  1.2760e-01,  ...,  2.7007e-01,\n           -1.2230e-01,  1.1001e-01]],\n\n         [[-6.7410e-02, -4.4090e-03,  1.6212e-02,  ..., -1.1219e-02,\n            2.1429e-03, -1.5527e-03],\n          [-7.4422e-02, -4.2096e-02,  2.0127e-01,  ..., -7.1574e-02,\n            9.6221e-02,  3.5584e-01],\n          [-1.4204e-01,  8.8811e-02,  1.2295e-01,  ...,  5.4589e-02,\n           -2.7241e-01,  2.8080e-01],\n          ...,\n          [-1.0015e+00,  2.3462e-01,  1.7334e-01,  ...,  1.5034e-01,\n            2.8653e-01, -2.0339e-01],\n          [-4.9001e-01,  4.7482e-01, -6.2534e-02,  ...,  1.9007e-01,\n           -1.1071e-01, -1.2971e-01],\n          [ 4.0019e-01,  4.1475e-01, -2.3828e-01,  ...,  3.3566e-01,\n           -2.9233e-01, -2.2862e-01]],\n\n         ...,\n\n         [[ 4.7063e-04,  3.1449e-02, -1.7503e-02,  ...,  8.2695e-03,\n           -1.0970e-02, -1.1543e-02],\n          [-1.2277e+00, -3.2604e-01, -6.1393e-02,  ..., -3.8778e-02,\n           -9.9841e-02, -5.4652e-01],\n          [ 9.1386e-02,  4.2278e-01,  9.2652e-02,  ...,  8.1694e-03,\n           -3.0594e-02,  1.4136e-01],\n          ...,\n          [-2.2290e-01, -6.7235e-02,  5.7267e-02,  ...,  2.3843e-01,\n           -2.7630e-02, -1.2387e-02],\n          [ 2.0408e-01,  3.7040e-01,  2.9052e-02,  ..., -4.8617e-02,\n            2.0986e-01,  3.6221e-01],\n          [ 4.6196e-01, -4.6471e-01, -4.3433e-02,  ...,  1.3075e-01,\n            5.5972e-02,  2.5477e-01]],\n\n         [[ 9.3577e-03,  3.9018e-03,  9.3528e-04,  ..., -5.8127e-03,\n           -1.1375e-03,  1.1215e-03],\n          [-4.8039e-01, -1.3629e-01, -7.1690e-01,  ...,  5.1239e-01,\n            3.5236e-01, -1.1517e-01],\n          [-6.2005e-01, -4.2717e-01, -4.7873e-01,  ..., -3.8481e-03,\n           -4.0798e-01, -8.2157e-01],\n          ...,\n          [-5.1856e-01, -1.4422e-01, -2.7818e-01,  ...,  1.5228e-01,\n           -2.3082e-01, -3.3898e-01],\n          [-1.9378e-01,  5.9553e-01, -1.1286e-01,  ...,  5.0783e-01,\n            4.6441e-02,  8.9126e-02],\n          [-1.1452e-01, -1.2341e-01,  1.7265e-01,  ..., -9.1119e-02,\n           -1.0231e-02,  1.4952e-01]],\n\n         [[ 1.1949e-02, -2.2031e-02,  8.2400e-03,  ...,  8.8179e-03,\n            8.7592e-03,  1.6473e-04],\n          [ 3.2632e-01, -2.0327e-01, -2.1340e-01,  ...,  8.7580e-02,\n           -2.2788e-01,  3.6569e-01],\n          [ 2.6529e-01, -2.1978e-01, -2.1967e-01,  ..., -1.4477e-01,\n           -1.6774e-01,  6.5762e-02],\n          ...,\n          [ 2.4309e-01, -2.5524e-01, -2.3644e-01,  ..., -1.4594e-01,\n           -2.1775e-01, -5.8270e-01],\n          [ 3.2517e-01, -5.5770e-01, -4.9247e-01,  ...,  8.8617e-02,\n           -3.3503e-01,  1.0231e-01],\n          [ 1.8709e-01, -2.3732e-01, -2.8045e-01,  ..., -7.9458e-02,\n           -1.2273e-01,  9.1866e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0001e-02,  5.3479e-03, -9.2337e-03,  ...,  2.9982e-02,\n            8.8365e-01, -1.0928e+00],\n          [ 8.7294e-01, -2.6140e-01,  4.2966e-01,  ..., -1.0903e+00,\n           -2.5277e+00,  1.1819e+00],\n          [ 4.6817e-01,  9.8864e-01,  7.3982e-01,  ..., -4.6623e-01,\n           -2.0306e+00,  2.4614e+00],\n          ...,\n          [-2.7831e-01,  5.0420e-01,  1.2072e+00,  ...,  3.8389e-01,\n           -1.8511e+00,  2.0652e+00],\n          [-7.9975e-01,  6.8814e-01,  6.6837e-01,  ...,  1.7069e+00,\n           -2.4216e+00,  3.3887e+00],\n          [-7.2989e-01, -6.6200e-01,  2.8208e-01,  ...,  1.5478e+00,\n           -3.1226e+00,  2.1741e+00]],\n\n         [[-6.1644e-02,  2.6081e-02,  6.2417e-02,  ...,  6.3455e-01,\n            3.6131e-02,  2.4065e-01],\n          [-2.1200e+00, -1.3107e-01, -1.4443e-01,  ..., -3.1277e-01,\n           -1.2450e+00,  1.0522e+00],\n          [ 1.3968e+00, -1.1912e+00, -2.1702e+00,  ..., -9.9641e-01,\n           -2.4775e-01,  3.1098e-01],\n          ...,\n          [ 9.3467e-01, -2.4026e+00, -1.5782e+00,  ..., -6.1785e-01,\n           -1.6054e+00,  1.0960e+00],\n          [-3.8580e-01, -6.8748e-01, -1.3747e+00,  ..., -4.0310e-01,\n           -3.4155e-01,  4.8981e-01],\n          [-1.8219e+00,  6.2990e-01, -2.6710e-02,  ...,  9.7308e-01,\n           -1.9349e+00,  1.1708e+00]],\n\n         [[-1.5109e-02, -3.5196e-03,  6.1797e-02,  ...,  6.3407e-01,\n           -5.1091e-01, -8.2480e-01],\n          [ 1.5792e+00, -1.2319e+00,  1.1765e+00,  ..., -6.3072e-01,\n            1.3554e+00,  1.6394e+00],\n          [ 1.4461e+00, -4.4069e-01,  9.2721e-01,  ..., -3.9494e-01,\n            5.3216e-01,  7.6567e-01],\n          ...,\n          [-1.2211e+00,  3.1120e-02, -1.2459e+00,  ..., -2.1893e+00,\n            2.6633e+00,  1.2852e-01],\n          [-1.3804e+00,  1.4137e+00, -1.5049e+00,  ..., -1.5894e+00,\n            1.4744e+00,  7.6856e-01],\n          [-8.3890e-01,  4.5816e-01, -1.0203e+00,  ..., -2.3383e+00,\n            2.0474e-01,  8.8204e-01]],\n\n         ...,\n\n         [[ 2.0095e-02, -1.9391e-02, -1.4458e-03,  ..., -6.3438e-01,\n           -7.4186e-02,  1.2699e+00],\n          [-7.9215e-01, -1.0645e+00, -8.1103e-01,  ...,  3.6834e-01,\n            1.0701e+00, -2.1562e+00],\n          [-1.5776e-01, -1.8322e-01, -2.8168e-01,  ..., -2.7338e-02,\n           -1.2710e+00, -2.8762e+00],\n          ...,\n          [-2.1154e-01, -1.1368e-01, -4.2548e-01,  ...,  2.2172e-01,\n            1.3162e+00, -6.0494e+00],\n          [ 4.8693e-01,  6.2472e-01, -5.3281e-01,  ..., -8.7062e-01,\n           -2.0747e+00, -4.9780e+00],\n          [-3.7886e-01,  1.0626e+00,  3.1687e-01,  ...,  2.7412e-01,\n           -5.9731e-01, -3.4808e+00]],\n\n         [[ 1.5396e-01,  1.1387e-02,  1.6595e-02,  ..., -3.2535e-01,\n            1.6872e-01, -1.8801e+00],\n          [ 6.0560e-01, -1.1477e+00, -7.4180e-01,  ...,  1.6262e-01,\n            2.3737e+00,  4.6392e-01],\n          [-2.6162e-01, -2.2885e+00, -1.3641e+00,  ..., -3.8918e-01,\n            1.7129e+00,  4.5273e+00],\n          ...,\n          [-1.7206e+00, -1.4077e+00, -1.1191e+00,  ..., -6.5098e-01,\n            7.7733e-01,  3.0853e+00],\n          [ 5.1052e-01, -4.2147e-01, -1.0513e+00,  ..., -1.1379e+00,\n            1.7694e+00,  3.7997e+00],\n          [ 2.3640e+00,  1.4077e+00, -6.7150e-01,  ..., -1.7631e-01,\n            2.6162e+00,  4.0207e+00]],\n\n         [[-9.3324e-03,  1.2826e-02,  1.0013e-02,  ...,  1.6950e-01,\n            5.9047e-01,  9.9431e-01],\n          [ 1.0309e-01,  5.6140e-02,  2.8503e-02,  ..., -2.4224e+00,\n           -2.5067e+00, -2.2298e+00],\n          [-2.9537e-01,  2.1882e-01, -2.3786e-01,  ..., -1.1905e+00,\n           -2.1368e+00, -3.1597e+00],\n          ...,\n          [-4.5620e-02, -2.2910e-02, -7.9293e-02,  ..., -1.2766e+00,\n           -2.2234e+00, -2.7992e+00],\n          [ 5.6757e-01,  4.9995e-02,  4.2694e-02,  ...,  1.8118e+00,\n           -2.6661e+00, -2.8046e+00],\n          [ 1.2093e-01,  3.0445e-01, -7.8137e-02,  ...,  4.3429e-03,\n           -1.2910e+00, -1.7973e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 0.0155, -0.0141,  0.0053,  ..., -0.0020, -0.0138, -0.0081],\n          [ 0.8893, -0.2964,  0.6060,  ..., -0.1289,  0.1849,  0.7236],\n          [ 0.0516,  0.4002,  0.5555,  ...,  0.0818, -0.7465,  0.1008],\n          ...,\n          [ 0.0592, -0.2722, -0.1607,  ..., -0.1261, -0.0721, -0.5363],\n          [ 0.5351, -0.0285,  0.3295,  ..., -0.1638, -0.3833, -0.1046],\n          [-0.8901,  0.3488,  0.5780,  ..., -0.4821,  0.0581, -0.2783]],\n\n         [[ 0.1125,  0.0091, -0.0121,  ..., -0.0521, -0.0187,  0.0434],\n          [-0.1717, -0.0280,  0.0977,  ...,  0.2039,  0.0755,  0.0779],\n          [-0.0912, -0.1047, -0.0915,  ...,  0.2498,  0.2045, -0.1584],\n          ...,\n          [ 0.1055,  0.1303, -0.6172,  ...,  0.2406, -0.3415,  0.1561],\n          [ 0.1275,  0.6940, -0.4025,  ...,  0.4515, -0.3610,  0.4613],\n          [-0.0474,  0.1060, -0.5638,  ..., -0.1677, -0.0342, -0.1656]],\n\n         [[-0.0043,  0.0186, -0.1110,  ..., -0.0808, -0.0089, -0.0305],\n          [-0.3964, -0.1324,  0.5198,  ...,  1.0464, -0.9780,  0.0426],\n          [-0.1032,  0.1052,  0.0733,  ..., -0.1005, -0.2119, -0.1483],\n          ...,\n          [ 0.5971, -0.0133, -0.1638,  ...,  0.7413, -0.2981,  0.5895],\n          [-0.3874,  0.2512,  0.1428,  ..., -0.1238, -0.3346,  0.1015],\n          [-0.5534,  0.6762, -0.0895,  ...,  0.5219, -1.0554, -0.1006]],\n\n         ...,\n\n         [[-0.0131,  0.0143, -0.0275,  ..., -0.0653, -0.0193,  0.0376],\n          [ 0.8291, -0.1804, -0.8852,  ..., -0.2940,  0.9659,  1.1397],\n          [-0.0343, -0.3794,  0.1225,  ..., -0.3974,  0.3114, -0.2152],\n          ...,\n          [ 0.2208, -0.3272, -0.4170,  ...,  0.4110, -0.2584,  0.1152],\n          [ 0.3429,  0.2526,  0.5178,  ...,  0.0889, -0.2551, -0.0285],\n          [-0.3247,  0.7592, -0.4231,  ..., -0.3010,  0.5201, -0.0014]],\n\n         [[-0.0173, -0.0193, -0.0596,  ..., -0.0225, -0.0123,  0.6400],\n          [-0.3333, -0.1722, -0.1961,  ...,  0.1566, -0.3654, -0.0344],\n          [ 0.1247,  0.0793,  0.1349,  ...,  0.2280,  0.2710, -0.4460],\n          ...,\n          [-0.0558, -0.2370,  0.1744,  ...,  0.4919, -0.3033, -0.4331],\n          [-0.0634,  0.1525,  0.0596,  ...,  0.4171,  0.0781, -0.6077],\n          [-0.1919, -0.1908,  0.0544,  ...,  0.5652, -0.0541, -0.7545]],\n\n         [[ 0.0253, -0.0222, -0.0151,  ...,  0.0164, -0.0272,  0.0803],\n          [ 0.2412,  0.8380, -0.4417,  ...,  0.6459, -0.6842,  0.2706],\n          [-0.2601,  0.0518, -0.1952,  ...,  0.2642, -0.4465,  0.4295],\n          ...,\n          [ 0.2313, -1.3138,  0.6014,  ..., -0.3423,  0.3664, -1.1917],\n          [ 0.5604,  0.0668,  0.1291,  ..., -0.2465,  0.0116, -0.3682],\n          [-0.4762,  0.0341,  0.3839,  ..., -0.5269, -0.5123,  0.2965]]]],\n       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.5414e-02,  4.6433e-02,  1.4184e-01,  ...,  5.4812e-01,\n           -4.8990e-01,  7.3753e-01],\n          [ 4.3876e+00,  5.3694e-01,  1.1721e+00,  ...,  1.3193e+00,\n           -1.3237e+00, -2.3558e-01],\n          [ 9.9249e-01,  3.1166e-02,  3.6171e-01,  ..., -1.5124e+00,\n           -2.4290e+00, -6.7962e-01],\n          ...,\n          [-1.1431e+00, -2.3096e+00, -3.1234e-01,  ..., -2.1145e+00,\n           -1.1770e+00, -4.5518e-01],\n          [-1.2081e+00, -1.9764e+00, -1.5585e-01,  ..., -2.3098e+00,\n           -2.8112e+00, -5.7975e-01],\n          [-1.8406e+00, -1.0024e+00, -2.8180e+00,  ..., -2.2056e+00,\n           -2.6257e+00, -4.7722e-01]],\n\n         [[ 1.2824e-02, -1.6541e-02,  1.6095e-03,  ..., -1.4238e-02,\n           -1.3309e+00,  2.2263e+00],\n          [-1.3956e+00,  8.9412e-02,  1.3159e-01,  ..., -4.4196e-01,\n            8.5787e-01, -9.0655e+00],\n          [-9.7274e-01, -5.4072e-01, -4.7400e-02,  ..., -3.0700e-01,\n            2.3624e+00, -5.6773e+00],\n          ...,\n          [ 1.3576e+00, -6.6439e-01,  1.1036e+00,  ..., -1.6988e+00,\n            3.5039e+00, -9.4972e+00],\n          [ 5.6010e-01, -9.3117e-01,  2.8272e+00,  ...,  5.8490e-01,\n            3.5013e+00, -5.3245e+00],\n          [ 5.9122e-01,  8.4591e-02,  2.3271e+00,  ...,  1.8021e+00,\n            3.4855e+00, -7.5563e+00]],\n\n         [[-1.3162e-01, -1.1779e-01, -8.8633e-02,  ...,  1.2191e+00,\n           -4.0260e-01, -9.9839e-01],\n          [-2.8954e+00, -3.4110e+00, -2.7247e+00,  ...,  4.0143e-01,\n           -2.4058e-01, -8.6029e-01],\n          [ 1.9712e+00, -1.7039e-02,  1.4560e-01,  ...,  1.4807e+00,\n           -6.6160e-01,  7.3650e-01],\n          ...,\n          [ 2.6792e+00,  3.1427e+00,  1.5565e+00,  ...,  1.3970e+00,\n            2.8025e-01,  4.6396e-01],\n          [-1.7699e+00,  3.4014e+00,  2.0562e+00,  ...,  1.0994e+00,\n            7.3417e-01, -1.7674e-01],\n          [-3.7179e+00,  2.6620e+00,  2.3894e+00,  ...,  1.0780e+00,\n            7.0768e-01,  8.1163e-01]],\n\n         ...,\n\n         [[ 2.6366e-02, -1.0544e-02,  6.2874e-04,  ...,  1.4033e-01,\n           -8.2465e-01, -3.7078e-01],\n          [ 1.6810e+00, -8.1752e-03, -1.3071e+00,  ...,  1.3426e+00,\n            2.8860e+00, -9.1175e-01],\n          [ 3.1420e+00,  1.8545e+00, -2.0833e+00,  ...,  2.5170e+00,\n            2.2813e+00, -2.3463e-01],\n          ...,\n          [-3.6326e+00,  2.8218e+00, -1.7481e+00,  ...,  4.2701e+00,\n            4.2890e+00, -3.0365e+00],\n          [-3.9798e+00,  1.4441e+00, -2.9004e-01,  ...,  4.0922e+00,\n            3.2314e+00, -1.7111e+00],\n          [ 4.9226e-01, -1.0174e+00, -2.0602e-01,  ...,  4.6128e+00,\n            3.7229e-01, -7.3213e-01]],\n\n         [[ 5.1096e-02,  1.2720e-01,  6.0000e-02,  ..., -1.7026e+00,\n            2.6214e-01, -1.8377e+00],\n          [-3.9598e+00,  1.7712e+00,  2.9831e+00,  ...,  2.2901e+00,\n           -7.9707e-01,  2.7464e+00],\n          [-2.6725e+00, -1.8800e-01,  2.2646e+00,  ...,  1.1920e+00,\n           -4.4188e-01,  2.6991e+00],\n          ...,\n          [ 1.1788e+00, -3.0626e+00,  8.2353e-01,  ...,  4.1268e+00,\n           -5.6293e-01,  2.0691e+00],\n          [ 3.1263e+00, -1.6974e+00, -2.8394e+00,  ...,  1.7192e+00,\n            1.1469e+00,  4.7347e+00],\n          [ 1.9080e+00, -1.7398e+00, -3.5397e+00,  ...,  3.0596e+00,\n            5.7570e-01,  1.9988e+00]],\n\n         [[ 1.9741e-02, -1.3131e-02,  1.8652e-02,  ..., -9.7016e-01,\n           -5.0637e-02,  4.6040e-01],\n          [-2.7769e-01, -5.1565e-01, -1.4570e-01,  ...,  2.3636e+00,\n            2.2137e+00, -1.0835e+00],\n          [-4.5133e-01,  1.6131e-01,  6.5179e-01,  ...,  3.1053e+00,\n            9.6139e-01,  3.5502e-01],\n          ...,\n          [ 3.0989e-01,  2.3544e-01, -5.7656e-01,  ...,  1.9273e+00,\n            8.0125e-01, -1.4189e+00],\n          [ 6.2809e-01,  1.4889e-01, -2.7151e-01,  ...,  3.2865e+00,\n            2.2598e-01, -1.6323e+00],\n          [ 6.7133e-02,  5.2969e-01, -7.0295e-01,  ...,  2.6760e+00,\n            4.5591e-02, -2.5868e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.4963e-02,  2.4116e-02, -1.4267e-02,  ...,  1.2166e-03,\n            2.0411e-02, -9.7046e-04],\n          [ 2.2547e-01,  1.4090e-01, -1.3826e-01,  ...,  2.4552e-02,\n           -2.0530e-01, -1.1754e-01],\n          [ 5.6882e-02,  1.7094e-01, -4.2025e-01,  ..., -9.0925e-02,\n           -1.7636e-01,  2.0896e-01],\n          ...,\n          [-2.9027e-01,  9.1443e-03, -4.6778e-01,  ...,  2.0404e-02,\n           -7.6829e-01, -8.6308e-02],\n          [-4.3335e-01,  3.2009e-01, -6.4599e-01,  ...,  2.0640e-01,\n           -6.1962e-01,  6.1385e-02],\n          [-3.5915e-01,  2.0407e-01, -4.2703e-01,  ...,  5.8975e-02,\n           -4.5818e-01,  4.4285e-01]],\n\n         [[ 3.4210e-02, -3.4803e-03, -1.2506e-03,  ..., -2.4340e-02,\n           -3.4289e-02,  4.0968e-02],\n          [-2.5548e-02,  8.6896e-01, -3.9595e-01,  ...,  5.3780e-01,\n           -2.1199e-01,  1.3437e+00],\n          [-1.8097e-01,  2.0235e-01,  3.2570e-02,  ..., -9.3057e-02,\n            2.3465e-01,  3.6229e-01],\n          ...,\n          [-5.9932e-02,  6.5331e-01,  2.2758e-01,  ..., -8.5440e-02,\n           -1.8942e-02,  5.7877e-01],\n          [ 1.5629e-01,  3.0888e-01,  1.4111e-01,  ...,  3.8784e-01,\n            3.4181e-01,  3.7496e-01],\n          [ 3.0771e-01, -1.3075e-01,  2.9143e-01,  ..., -2.0617e-01,\n           -3.7792e-01,  2.2385e-01]],\n\n         [[ 9.4470e-02,  1.9155e-01,  3.5195e-02,  ..., -3.0459e-02,\n            3.4772e-01, -7.3498e-03],\n          [-5.1428e-01,  3.0442e-01,  4.2104e-01,  ..., -5.7540e-01,\n           -1.5479e+00,  3.6643e-01],\n          [ 7.6056e-02,  1.6991e-01,  4.9027e-01,  ...,  1.1887e-01,\n           -2.2015e+00,  4.4847e-01],\n          ...,\n          [ 1.4213e-01,  5.0866e-01,  1.2624e-01,  ..., -2.8348e-01,\n           -2.0461e+00,  2.5732e-01],\n          [ 1.4330e-02, -2.6393e-01,  4.7206e-01,  ..., -2.3471e-01,\n           -2.9259e+00, -3.8417e-01],\n          [-1.4677e-01,  2.3432e-01,  1.5036e-02,  ..., -2.7339e-01,\n           -2.3934e+00,  2.3624e-02]],\n\n         ...,\n\n         [[-8.7270e-02, -6.6465e-02, -3.0371e-02,  ...,  1.7711e-02,\n            6.4740e-02, -3.5456e-03],\n          [-6.0243e-01,  3.2345e-02, -9.2731e-01,  ..., -2.6678e-01,\n            2.1238e+00, -8.8268e-01],\n          [-2.0218e-02, -2.0306e-01, -2.8669e-01,  ...,  4.7327e-01,\n           -2.6078e-01,  7.2375e-03],\n          ...,\n          [ 3.9089e-01,  1.1831e+00,  5.6502e-01,  ..., -7.5588e-01,\n           -7.9102e-01,  7.6052e-01],\n          [-1.4055e+00, -6.4177e-01, -5.4610e-01,  ..., -6.1377e-01,\n            5.9486e-02,  8.7491e-01],\n          [ 9.0679e-01,  5.2043e-02,  8.9971e-01,  ...,  3.3745e-01,\n            1.2728e+00, -7.4647e-02]],\n\n         [[ 2.7315e-02,  9.0491e-02, -7.3440e-02,  ..., -2.2172e-03,\n            4.7945e-02,  2.3947e-02],\n          [-4.5239e-02, -4.2668e-01,  4.3743e-01,  ..., -2.0161e-01,\n           -7.7504e-02, -3.8835e-01],\n          [-2.5332e-01, -1.4399e-01, -8.7746e-02,  ...,  2.9873e-01,\n           -1.4090e-01,  1.6136e-01],\n          ...,\n          [ 3.7944e-01,  4.1455e-01,  3.3392e-01,  ..., -2.2405e-01,\n           -4.0171e-01,  1.4341e-01],\n          [-2.0055e-01,  5.0283e-01, -2.5512e-01,  ..., -1.3557e-01,\n           -3.7245e-01, -1.0968e-01],\n          [ 8.1803e-02, -1.1857e+00,  8.5675e-01,  ..., -2.0428e-01,\n            1.8285e-01, -1.6003e-02]],\n\n         [[ 2.5544e-02, -4.4024e-02, -3.5656e-02,  ...,  9.9875e-03,\n            5.0772e-02,  1.3128e-03],\n          [-8.6449e-01, -1.1419e+00,  1.4385e+00,  ..., -2.5004e-01,\n           -1.8690e-01,  6.5207e-01],\n          [-2.7724e-01,  1.3995e-01,  2.3331e-01,  ..., -2.2554e-01,\n            2.5031e-01,  4.4207e-02],\n          ...,\n          [-9.7353e-02, -1.2743e-01,  3.8092e-01,  ..., -1.7040e+00,\n            1.8006e-01,  6.6705e-01],\n          [ 9.8546e-01,  2.8762e-01, -2.3287e-01,  ..., -2.0138e-01,\n           -2.4919e-01,  6.5736e-02],\n          [-1.5538e+00,  9.0598e-01,  6.9267e-01,  ...,  6.6153e-02,\n            3.3056e-01, -3.1213e-01]]]], grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "traced_model = torch.jit.trace(model.model, (inputs.input_ids, inputs.attention_mask))\n",
    "traced_model.save(\"llama3.2-1b-quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tvm\n",
      "  Downloading tvm-1.0.0.tar.gz (5.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting appdirs (from tvm)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting docopt (from tvm)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting inform (from tvm)\n",
      "  Downloading inform-1.32-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting quantiphy (from tvm)\n",
      "  Downloading quantiphy-2.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting arrow (from inform->tvm)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages (from inform->tvm) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/miniconda3/envs/vpr_env/lib/python3.12/site-packages (from arrow->inform->tvm) (2.9.0.post0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->inform->tvm)\n",
      "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading inform-1.32-py3-none-any.whl (39 kB)\n",
      "Downloading quantiphy-2.20-py3-none-any.whl (41 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
      "Building wheels for collected packages: tvm, docopt\n",
      "  Building wheel for tvm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tvm: filename=tvm-1.0.0-py3-none-any.whl size=5086 sha256=604430a85a3ac36987f17d497afe16000c863840592e759c61ae06f5863ac071\n",
      "  Stored in directory: /Users/olivergrainge/Library/Caches/pip/wheels/3c/5e/e6/2a4aace02fdd238400c8dc25848cda8b5a6367f1295453f3f9\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=b489680a2d406cce71bd00171d52061e370f6469f21bc59ca35b7a9fc6fb8bb6\n",
      "  Stored in directory: /Users/olivergrainge/Library/Caches/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built tvm docopt\n",
      "Installing collected packages: docopt, appdirs, types-python-dateutil, quantiphy, arrow, inform, tvm\n",
      "Successfully installed appdirs-1.4.4 arrow-1.3.0 docopt-0.6.2 inform-1.32 quantiphy-2.20 tvm-1.0.0 types-python-dateutil-2.9.0.20241003\n"
     ]
    }
   ],
   "source": [
    "!pip install tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
