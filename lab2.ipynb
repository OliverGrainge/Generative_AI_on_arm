{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdb0e60fecb4a31869b80c510761e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c56e5efed844c27a9e1ca69ad213212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdfd777dcdb4002a73464dc9e3bd368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053877cb754d4c6cbf8ec756d281134e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32aeda0e984425cace7595fd4234d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e233c38cc8457ab7cc2d79f1b45ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_openelm.py:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M-Instruct:\n",
      "- configuration_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a7d2bc16c64960a6550f55654b4747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_openelm.py:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M-Instruct:\n",
      "- modeling_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaf5f5506db44e997607c43f174e13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52d9bdb103543da8422cea22936009f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (proj_2): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2048, bias=False)\n",
       "          (proj_2): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2560, bias=False)\n",
       "          (proj_2): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (proj_2): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7680, bias=False)\n",
       "          (proj_2): Linear(in_features=3840, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9728, bias=False)\n",
       "          (proj_2): Linear(in_features=4864, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect largest matmul in LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected weight matrix has shape 32000x1280 (output_channels x input_channels)\n",
      "The collected activation tensor has shape 6x1280 (sequence_length x input_channels)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Initialize dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "prompt = \"Arm is a company that\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Define a hook function to capture input\n",
    "def get_activation_input(name):\n",
    "    def hook(model, input, output):\n",
    "        # 'input' is a tuple; we take the first element for the input tensor\n",
    "        activations[name] = output[0]\n",
    "    return hook\n",
    "\n",
    "# Select the transformer block and register the hook\n",
    "layer_idx = 12  # Select the transformer block of your choosing\n",
    "layer = model.transformer.norm\n",
    "hook_handle = layer.register_forward_hook(get_activation_input('token_activations'))\n",
    "\n",
    "# Run the model forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "# Extract the weight matrix and the input activation tensor from the hook\n",
    "W = model.transformer.token_embeddings.weight.data.clone()\n",
    "X = activations['token_activations']\n",
    "hook_handle.remove()\n",
    "print(f\"The collected weight matrix has shape {W.shape[0]}x{W.shape[1]} (output_channels x input_channels)\")\n",
    "print(f\"The collected activation tensor has shape {X.shape[0]}x{X.shape[1]} (sequence_length x input_channels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark torch latency for the matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.7 ms ± 114 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "with torch.no_grad():\n",
    "    y = F.linear(X, W, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data out so we can benchmark with c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src/cpp/assets/\n",
    "\n",
    "X.numpy().tofile(\"src/cpp/assets/x_fp32.bin\")\n",
    "W.numpy().tofile(\"src/cpp/assets/w_fp32.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Matmul Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/naive/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/naive/kernel.cpp\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "void matrix_multiply_naive(float* A, float* B, float* C, int M, int K, int N) {\n",
    "    // # M: the number of rows in the left hand matric (M = A.shape[0])\n",
    "    // # K: the inner dimensions of the multiple (K == A.shape[1] == B.shape[0])\n",
    "    // # N: the number of columns in the right hand matrix (N == B.shape[1])\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            C[i * N + j] = 0;\n",
    "            for (int k = 0; k < K; k++) {\n",
    "                C[i * N + j] += A[i * K + k] * B[k * N + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Naive Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Generative_AI_on_arm/src/cpp/naive/build\n",
      "-- Configuring done (0.0s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/ubuntu/Generative_AI_on_arm/src/cpp/naive/build\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_naive.dir/kernel.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable benchmark_naive\u001b[0m\n",
      "[100%] Built target benchmark_naive\n",
      "======> Running benchmark\n",
      "Time taken: 1199 milliseconds\n",
      "/home/ubuntu/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!mkdir -p src/cpp/naive/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/naive/build\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_naive\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone KleidiAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'kleidiai' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://git.gitlab.arm.com/kleidi/kleidiai.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the KleidiAI Micro-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Kleidi Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP32 NEON MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/f32_f32_f32p/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_f32_f32p/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "\n",
    "#include \"kai_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla.h\"\n",
    "#include \"kai_matmul_clamp_f32_f32_f32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_f32_f32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_n_step_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_nr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_kr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_sr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_lhs_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_dst_size_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_run_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Generative_AI_on_arm/src/cpp/f32_f32_f32p/build\n",
      "-- Configuring done (0.0s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/ubuntu/Generative_AI_on_arm/src/cpp/f32_f32_f32p/build\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_f32.dir/benchmark_f32.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_f32.dir/kernel.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable benchmark_f32\u001b[0m\n",
      "[100%] Built target benchmark_f32\n",
      "======> Running benchmark\n",
      "Time taken: 4.268 milliseconds\n",
      "/home/ubuntu/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_f32_f32p/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_f32_f32p/build\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_f32\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16 NEON MLA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/cpp/f16_f16_f16p/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "\n",
    "#include \"kai_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla.h\"\n",
    "#include \"kai_matmul_clamp_f16_f16_f16p_interface.h\"\n",
    "\n",
    "constexpr kai_matmul_clamp_f16_f16_f16p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_n_step_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_nr_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_kr_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_sr_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_lhs_offset_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_dst_offset_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_get_dst_size_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla,\n",
    "    kai_run_matmul_clamp_f16_f16_f16p16x1biasf16_6x16x8_neon_mla};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/cpp/f16_f16_f16p/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f16_f16_f16p/build\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_f16\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DotProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cpp/f32_i8_i4_dotprod/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_i8_i4_dotprod/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <string>\n",
    "\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h\"\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_n_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_mr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_nr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_kr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_sr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_run_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Generative_AI_on_arm/src/cpp/f32_i8_i4_dotprod/build\n",
      "-- The C compiler identification is GNU 13.3.0\n",
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Configuring done (0.3s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/ubuntu/Generative_AI_on_arm/src/cpp/f32_i8_i4_dotprod/build\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_dotprod.dir/benchmark_dotprod.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_dotprod.dir/kernel.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding C object CMakeFiles/benchmark_dotprod.dir/home/ubuntu/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c.o\u001b[0m\n",
      "\u001b[01m\u001b[K/home/ubuntu/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c:8:2:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"Dotprod extension required to compile this micro-kernel\"\n",
      "    8 | #\u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"Dotprod extension required to compile this micro-kernel\"\n",
      "      |  \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "make[2]: *** [CMakeFiles/benchmark_dotprod.dir/build.make:104: CMakeFiles/benchmark_dotprod.dir/home/ubuntu/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c.o] Error 1\n",
      "make[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/benchmark_dotprod.dir/all] Error 2\n",
      "make: *** [Makefile:91: all] Error 2\n",
      "======> Running benchmark\n",
      "/bin/bash: line 1: ./benchmark_dotprod: No such file or directory\n",
      "/home/ubuntu/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_i8_i4_dotprod/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_i8_i4_dotprod/build\n",
    "\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_dotprod\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I8MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cpp/f32_i8_i4_i8mm/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_i8_i4_i8mm/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm.h\"\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_n_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_mr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_nr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_kr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_sr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_run_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm};\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'src/cpp/f32_i8_i4_i8mm/build'\n",
      "/home/ubuntu/Generative_AI_on_arm\n",
      "\u001b[33mCMake Warning:\n",
      "  Ignoring extra path from command line:\n",
      "\n",
      "   \"..\"\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[0mCMake Error: The source directory \"/home/ubuntu\" does not appear to contain CMakeLists.txt.\n",
      "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n",
      "make: *** No targets specified and no makefile found.  Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Generative_AI_on_arm/my_env/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> Running benchmark\n",
      "/bin/bash: line 1: ./benchmark_i8mm: No such file or directory\n",
      "/home/ubuntu/Generative_AI_on_arm/src/cpp/f32_i8_i4_dotprod/build\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_i8_i4_dotprod/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_i8_i4_i8mm/build\n",
    "\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_i8mm\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
