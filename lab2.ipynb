{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (proj_2): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2048, bias=False)\n",
       "          (proj_2): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2560, bias=False)\n",
       "          (proj_2): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (proj_2): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7680, bias=False)\n",
       "          (proj_2): Linear(in_features=3840, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9728, bias=False)\n",
       "          (proj_2): Linear(in_features=4864, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect largest matmul in LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected weight matrix has shape 32000x1280 (output_channels x input_channels)\n",
      "The collected activation tensor has shape 6x1280 (sequence_length x input_channels)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Initialize dictionary to store activations\n",
    "activations = {}\n",
    "\n",
    "prompt = \"Arm is a company that\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Define a hook function to capture input\n",
    "def get_activation_input(name):\n",
    "    def hook(model, input, output):\n",
    "        # 'input' is a tuple; we take the first element for the input tensor\n",
    "        activations[name] = output[0]\n",
    "    return hook\n",
    "\n",
    "# Select the transformer block and register the hook\n",
    "layer_idx = 12  # Select the transformer block of your choosing\n",
    "layer = model.transformer.norm\n",
    "hook_handle = layer.register_forward_hook(get_activation_input('token_activations'))\n",
    "\n",
    "# Run the model forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "# Extract the weight matrix and the input activation tensor from the hook\n",
    "W = model.transformer.token_embeddings.weight.data.clone()\n",
    "X = activations['token_activations']\n",
    "hook_handle.remove()\n",
    "print(f\"The collected weight matrix has shape {W.shape[0]}x{W.shape[1]} (output_channels x input_channels)\")\n",
    "print(f\"The collected activation tensor has shape {X.shape[0]}x{X.shape[1]} (sequence_length x input_channels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark torch latency for the matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.93 ms ± 44.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "with torch.no_grad():\n",
    "    y = F.linear(X, W, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data out so we can benchmark with c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: src/cpp/assets/: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir src/cpp/assets/\n",
    "\n",
    "X.numpy().tofile(\"src/cpp/assets/x_fp32.bin\")\n",
    "W.numpy().tofile(\"src/cpp/assets/w_fp32.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Matmul Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/naive/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/naive/kernel.cpp\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "void matrix_multiply_naive(float* A, float* B, float* C, int M, int K, int N) {\n",
    "    // # M: the number of rows in the left hand matric (M = A.shape[0])\n",
    "    // # K: the inner dimensions of the multiple (K == A.shape[1] == B.shape[0])\n",
    "    // # N: the number of columns in the right hand matrix (N == B.shape[1])\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            C[i * N + j] = 0;\n",
    "            for (int k = 0; k < K; k++) {\n",
    "                C[i * N + j] += A[i * N + k] * B[k * N + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Benchmark the operator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/naive/build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMake Error: The current CMakeCache.txt directory /Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/naive/build/CMakeCache.txt is different than the directory /Users/olivergrainge/Documents/github/archiv/Generative_AI_on_arm/src/cpp/naive/build where CMakeCache.txt was created. This may result in binaries being created in the wrong place. If you are not sure, reedit the CMakeCache.txt\u001b[0m\n",
      "\u001b[0mCMake Error: The source \"/Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/naive/CMakeLists.txt\" does not match the source \"/Users/olivergrainge/Documents/github/archiv/Generative_AI_on_arm/src/cpp/naive/CMakeLists.txt\" used to generate cache.  Re-run cmake with a different source directory.\u001b[0m\n",
      "[100%] Built target benchmark_naive\n",
      "======> Running benchmark\n",
      "Time taken: 1011.33 milliseconds\n",
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!mkdir -p src/cpp/naive/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/naive/build\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_naive\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone KleidiAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'kleidiai' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://git.gitlab.arm.com/kleidi/kleidiai.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the KleidiAI Micro-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Kleidi Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEON MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/f32_f32_f32p/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_f32_f32p/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "\n",
    "#include \"kai_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla.h\"\n",
    "#include \"kai_matmul_clamp_f32_f32_f32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_f32_f32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_n_step_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_nr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_kr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_sr_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_lhs_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_get_dst_size_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla,\n",
    "    kai_run_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_f32_f32p/build\n",
      "-- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "-- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Configuring done (1.1s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_f32_f32p/build\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_f32.dir/benchmark_f32.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_f32.dir/kernel.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding C object CMakeFiles/benchmark_f32.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_f32_f32p/kai_matmul_clamp_f32_f32_f32p8x1biasf32_6x8x4_neon_mla.c.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding C object CMakeFiles/benchmark_f32.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/pack/kai_rhs_pack_kxn_f32p8x1biasf32_f32_f32_neon.c.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable benchmark_f32\u001b[0m\n",
      "[100%] Built target benchmark_f32\n",
      "======> Running benchmark\n",
      "Time taken: 2.322 milliseconds\n",
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_f32_f32p/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_f32_f32p/build\n",
    "\n",
    "!rm -r *\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_f32\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DotProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/f32_i8_i4_dotprod/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_i8_i4_dotprod/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include <string>\n",
    "\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h\"\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_n_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_mr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_nr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_kr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_sr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod,\n",
    "    kai_run_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_i8_i4_dotprod/build\n",
      "-- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "-- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Configuring done (0.6s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_i8_i4_dotprod/build\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_dotprod.dir/benchmark_dotprod.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_dotprod.dir/kernel.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding C object CMakeFiles/benchmark_dotprod.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding C object CMakeFiles/benchmark_dotprod.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.c.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding C object CMakeFiles/benchmark_dotprod.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32.c.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable benchmark_dotprod\u001b[0m\n",
      "[100%] Built target benchmark_dotprod\n",
      "======> Running benchmark\n",
      "Time taken: 2.260 milliseconds\n",
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_i8_i4_dotprod/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_i8_i4_dotprod/build\n",
    "\n",
    "!rm -r *\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_dotprod\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I8MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpp/f32_i8_i4_i8mm/kernel.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpp/f32_i8_i4_i8mm/kernel.cpp\n",
    "\n",
    "#include <arm_neon.h>\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm.h\"\n",
    "#include \"kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h\"\n",
    "\n",
    "\n",
    "constexpr kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel{\n",
    "    kai_get_m_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_n_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_mr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_nr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_kr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_sr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm,\n",
    "    kai_run_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm};\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_i8_i4_i8mm/build\n",
      "-- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "-- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Configuring done (0.6s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /Users/olivergrainge/Documents/github/Generative_AI_on_arm/src/cpp/f32_i8_i4_i8mm/build\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_i8mm.dir/benchmark_i8mm.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/benchmark_i8mm.dir/kernel.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding C object CMakeFiles/benchmark_i8mm.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm.c.o\u001b[0m\n",
      "\u001b[1m/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm.c:8:2: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1m\"i8mm extension required to compile this micro-kernel\"\u001b[0m\n",
      "#error \"i8mm extension required to compile this micro-kernel\"\n",
      "\u001b[0;1;32m ^\n",
      "\u001b[0m1 error generated.\n",
      "make[2]: *** [CMakeFiles/benchmark_i8mm.dir/Users/olivergrainge/Documents/github/Generative_AI_on_arm/kleidiai/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_8x4x32_neon_i8mm.c.o] Error 1\n",
      "make[1]: *** [CMakeFiles/benchmark_i8mm.dir/all] Error 2\n",
      "make: *** [all] Error 2\n",
      "======> Running benchmark\n",
      "/bin/bash: ./benchmark_dotprod: No such file or directory\n",
      "/Users/olivergrainge/Documents/github/Generative_AI_on_arm\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p src/cpp/f32_i8_i4_dotprod/build \n",
    "\n",
    "# Navigate to the build directory\n",
    "%cd src/cpp/f32_i8_i4_i8mm/build\n",
    "\n",
    "!rm -r *\n",
    "\n",
    "# Run cmake\n",
    "!cmake ..\n",
    "\n",
    "# Build the project using make\n",
    "!make\n",
    "\n",
    "print(\"======> Running benchmark\")\n",
    "# Run the benchmark binary\n",
    "!./benchmark_dotprod\n",
    "\n",
    "# Navigate back to the original directory after execution\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
