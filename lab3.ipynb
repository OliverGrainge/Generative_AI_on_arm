{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9c5e41-4487-4e98-bb08-45f3f0874a16",
   "metadata": {},
   "source": [
    "# **LAB 3: Comparative Inference Benchmarking on ARM Server and Edge Devices**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Lab 3 in our series on **Optimizing Generative AI Workloads with ARM Processors**! In this lab, we shift our focus to evaluating the performance constraints of generative AI inference between ARM-based server and edge devices. Specifically, we'll use the **OpenELM-3B** model alongside the popular **llama.cpp** framework to benchmark and compare inference characteristics on two different ARM environments: a high-performance Graviton server and a resource-constrained Raspberry Pi.\n",
    "\n",
    "Throughout this lab, you will:\n",
    "\n",
    "- **Benchmark Latency Metrics**: Measure and compare the overall inference latency, time-to-first-token (TTFT), and per-token decoding latency across devices.\n",
    "- **Evaluate Memory Consumption**: Analyze how the OpenELM-3B model's memory footprint varies between an ARM server and an ARM-based edge device.\n",
    "- **Compare ARM CPU Constraints**: Understand the practical limitations and trade-offs when deploying large language models on different ARM CPUs, from powerful server-class processors to limited edge devices.\n",
    "- **Utilize llama.cpp Framework**: Leverage the llama.cpp framework for efficient model inference and performance measurement, allowing for cross-device comparisons.\n",
    "\n",
    "This lab will guide you through setting up the necessary environments, running inference benchmarks on both the Raspberry Pi and Graviton server, and interpreting the results. By the end of the lab, you'll gain insights into:\n",
    "\n",
    "- How hardware differences affect model performance\n",
    "- Strategies for optimizing inference on both server and edge ARM devices\n",
    "- Best practices for deploying large language models across diverse ARM architectures\n",
    "\n",
    "---\n",
    "\n",
    "### **Lab Objectives**\n",
    "\n",
    "1. **Set Up Diverse ARM Environments**  \n",
    "   Prepare and configure both a Raspberry Pi (edge device) and an ARM Graviton server for inference benchmarking with the OpenELM-3B model and llama.cpp framework.\n",
    "\n",
    "2. **Benchmark Latency Metrics**  \n",
    "   Measure key performance indicators, including total inference latency, time-to-first-token (TTFT), and per-token decoding latency, on both devices.\n",
    "\n",
    "3. **Analyze Memory Consumption**  \n",
    "   Monitor and compare memory utilization during model inference to assess resource constraints on server and edge devices.\n",
    "\n",
    "4. **Compare Performance Constraints**  \n",
    "   Understand the practical implications of deploying large language models on resource-constrained edge devices versus powerful server environments. Identify bottlenecks and optimization opportunities for each scenario.\n",
    "\n",
    "5. **Document Findings and Insights**  \n",
    "   Compile your observations, compare the differences in performance, and suggest strategies to mitigate limitations on edge devices while leveraging server capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "- **Basic Understanding of Linux**: Familiarity with Linux command-line operations and system configuration.\n",
    "- **Experience with C/C++ and Python Programming**: Ability to modify, compile, and execute code across different ARM platforms.\n",
    "- **Fundamentals of AI Inference**: Knowledge of how inference works in large language models.\n",
    "- **Completion of Labs 1 and 2 (Recommended)**: Prior experience with ARM intrinsics, quantization, and performance benchmarking for generative AI workloads.\n",
    "\n",
    "---\n",
    "\n",
    "**By the end of this lab**, you will have a comprehensive understanding of how ARM server and edge CPUs handle inference for large language models like OpenELM-3B. You will be equipped with the skills to benchmark and analyze latency, time-to-first-token, decoding performance, and memory consumption, enabling you to make informed decisions when deploying AI models across diverse ARM architectures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setting Up the Raspberry Pi for Inference Benchmarking\n",
    "\n",
    "Before proceeding, ensure you have completed the [Lab 1 setup script](#) on your Raspberry Pi. If not, start by running the setup script:\n",
    "\n",
    "```bash\n",
    "./setup_pi5.sh\n",
    "```\n",
    "\n",
    "Once the script completes, activate the newly created virtual environment:\n",
    "\n",
    "```bash\n",
    "source pi5_env/bin/activate\n",
    "```\n",
    "\n",
    "This ensures that all required packages for the lab are preinstalled.\n",
    "\n",
    "### Building the llama.cpp Inference Framework\n",
    "\n",
    "If you haven't already, download and build the [llama.cpp](https://github.com/ggerganov/llama.cpp) inference framework with ARM CPU optimizations:\n",
    "\n",
    "1. **Clone the repository:**\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/ggerganov/llama.cpp\n",
    "   ```\n",
    "\n",
    "2. **Configure the build:**\n",
    "\n",
    "   ```bash\n",
    "   cmake -B llama.cpp/build -S llama.cpp -DCMAKE_CXX_FLAGS=\"-mcpu=native\" -DCMAKE_C_FLAGS=\"-mcpu=native\"\n",
    "   ```\n",
    "\n",
    "3. **Compile the framework:**\n",
    "\n",
    "   ```bash\n",
    "   cmake --build llama.cpp/build --config Release -j$(nproc) -v\n",
    "   ```\n",
    "\n",
    "These commands build the framework optimized for your ARM CPU.\n",
    "\n",
    "### Downloading the OpenELM-3B Model Weights\n",
    "\n",
    "Next, download the large language model weights from Hugging Face. Follow these steps:\n",
    "\n",
    "1. Create a Python file named `download_openelm.py` in the root directory with the following content:\n",
    "\n",
    "   ```python\n",
    "   from huggingface_hub import snapshot_download\n",
    "\n",
    "   # Specify the target directory for downloading the model\n",
    "   target_directory = \"models/hf_models/OpenELM-3B-Instruct\"\n",
    "\n",
    "   # Download a snapshot of the model repository\n",
    "   snapshot_download(\n",
    "       repo_id=\"apple/OpenELM-3B-Instruct\",\n",
    "       local_dir=target_directory,\n",
    "       revision=\"main\",  # Optional: specify a branch, tag, or commit hash\n",
    "       local_dir_use_symlinks=False  # Set to True if you want symlinks instead of file copies\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. Run the script to start downloading the model weights:\n",
    "\n",
    "   ```bash\n",
    "   python download_openelm.py\n",
    "   ```\n",
    "\n",
    "   > **Note:** The download may take around 20 minutes, depending on your internet speed, as the model is large (~3 billion parameters).\n",
    "\n",
    "### Converting and Quantizing the Model\n",
    "\n",
    "Once the model is downloaded, convert it to the GGUF format and quantize it for inference:\n",
    "\n",
    "1. **Convert the model to GGUF format:**\n",
    "\n",
    "   ```bash\n",
    "   mkdir -p models/gguf_models\n",
    "\n",
    "   python llama.cpp/convert_hf_to_gguf.py models/hf_models/OpenELM-3B-Instruct/ \\\n",
    "       --outfile models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       --outtype f16\n",
    "   ```\n",
    "\n",
    "2. **Quantize the model using llama-quantize:**\n",
    "\n",
    "   The quantization process reduces the model's size and computational requirements, which can improve inference speed on resource-constrained devices like the Raspberry Pi. Two types of quantization are demonstrated here:\n",
    "   \n",
    "   - **Q8_0 Quantization:** This reduces the model to 8-bit precision, balancing model size and performance. It typically offers faster inference with a moderate impact on accuracy.\n",
    "   - **Q4_0 Quantization:** This reduces the model further to 4-bit precision, providing greater speed and memory savings at the expense of some accuracy.\n",
    "\n",
    "   For Q8_0 quantization:\n",
    "   ```bash\n",
    "   llama.cpp/build/bin/llama-quantize \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-q8_0.gguf Q8_0\n",
    "   ```\n",
    "\n",
    "   For Q4_0 quantization:\n",
    "   ```bash\n",
    "   llama.cpp/build/bin/llama-quantize \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-q4_0.gguf Q4_0\n",
    "   ```\n",
    "\n",
    "### Prompting the Model\n",
    "\n",
    "After quantization, you can prompt the different models. For example, to use the Q8_0 quantized model:\n",
    "\n",
    "```bash\n",
    "llama.cpp/build/bin/llama-cli \\\n",
    "    -m models/gguf_models/OpenELM-3B-Instruct-q8_0.gguf \\\n",
    "    -p \"Could you write a very simple program in C++ to print 'Hello, World!' in less than 10 lines of code?\"\n",
    "```\n",
    "\n",
    "Follow similar steps for other prompts or to test the Q4_0 model. This setup process on the Raspberry Pi allows you to benchmark LLM inference on an edge device and later compare performance with an ARM Graviton server.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Setting Up the AWS Arm Graviton Server for Inference Benchmarking\n",
    "\n",
    "If you have not already followed these steps in Lab 2, start by creating an AWS account and logging in as the root user. Next, set up an EC2 instance using a Graviton processor by following these steps:\n",
    "\n",
    "1. Search for **EC2** in the AWS Management Console and click on **Launch Instance**.\n",
    "2. Choose your operating system (AMI). Select **Ubuntu** and toggle the architecture box to **arm64**.\n",
    "3. For the instance type, select a **m7g.xlarge** and allocate **32 GB** of persistent storage.\n",
    "4. Launch the instance. Once the instance is created, click **Connect** to get the SSH command. It will look similar to:\n",
    "   ```bash\n",
    "   ssh -i \"GravitonUbuntu.pem\" ubuntu@ec2-13-53-171-129.eu-north-1.compute.amazonaws.com\n",
    "   ```\n",
    "5. Use the provided SSH command to connect to the instance.\n",
    "\n",
    "### Setting Up the Server Environment\n",
    "\n",
    "Once connected:\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/OliverGrainge/Generative_AI_on_arm.git\n",
    "   ```\n",
    "2. Change directory and run the server setup script:\n",
    "   ```bash\n",
    "   cd Generative_AI_on_arm\n",
    "   ./setup_graviton.sh\n",
    "   ```\n",
    "\n",
    "### Building the llama.cpp Inference Framework\n",
    "\n",
    "After setting up the server environment, clone and build the `llama.cpp` framework:\n",
    "\n",
    "1. **Clone the repository:**\n",
    "   ```bash\n",
    "   git clone https://github.com/ggerganov/llama.cpp\n",
    "   ```\n",
    "2. **Configure the build:**\n",
    "   ```bash\n",
    "   cmake -B llama.cpp/build -S llama.cpp -DCMAKE_CXX_FLAGS=\"-mcpu=native\" -DCMAKE_C_FLAGS=\"-mcpu=native\"\n",
    "   ```\n",
    "3. **Compile the framework:**\n",
    "   ```bash\n",
    "   cmake --build llama.cpp/build --config Release -j$(nproc) -v\n",
    "   ```\n",
    "\n",
    "### Downloading the OpenELM-3B Model Weights\n",
    "\n",
    "Assuming you've already created the `download_openelm.py` file from Section 1, proceed with:\n",
    "\n",
    "1. Activate the virtual environment:\n",
    "   ```bash\n",
    "   source graviton_env/bin/activate\n",
    "   ```\n",
    "2. Run the download script:\n",
    "   ```bash\n",
    "   python scripts/download_openelm.py\n",
    "   ```\n",
    "\n",
    "### Converting and Quantizing the Model\n",
    "\n",
    "Once the model has been downloaded:\n",
    "\n",
    "1. **Convert the model to GGUF format:**\n",
    "   ```bash\n",
    "   mkdir models/gguf_models/\n",
    "   python llama.cpp/convert_hf_to_gguf.py models/hf_models/OpenELM-3B-Instruct/ \\\n",
    "       --outfile models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       --outtype f16\n",
    "   ```\n",
    "\n",
    "2. **Quantize the model:**\n",
    "\n",
    "   For **Q8_0 quantization:**\n",
    "   ```bash\n",
    "   llama.cpp/build/bin/llama-quantize \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-q8_0.gguf Q8_0\n",
    "   ```\n",
    "\n",
    "   For **Q4_0 quantization:**\n",
    "   ```bash\n",
    "   llama.cpp/build/bin/llama-quantize \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-f16.gguf \\\n",
    "       models/gguf_models/OpenELM-3B-Instruct-q4_0.gguf Q4_0\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Great if you have completed all the steps up untill now you should have a compile llama.cpp inference framework on both your local raspberry pi and a remote Arm Graviton Server with the 3B OpenELM model in floating point 16, Int8 and Int4 precisions. Next to compare how the performance differs between them we should calculate some metrics from them. A good metric to measure is the time to first token. This is the time it takes for the LLM to process the entire input prompt and output a single token. It is critical as it significantly impacts user experience and can for large input prompts be a large computational burden as the llm must process every single token in the input. Another important metric to measure is the generation latency. An LLM autoregressively predicts the next most likely character or token based on the previous context e.g the prompt and any previous responce tokens. It is usually measured in tokens per second and in accelerated by caching the previous activations required to generate the new token. This is called KV Caching as the keys and values of the previous tokens are required to compute the next one. Lucky for us, llama.cpp offers a way to measure this for us, using llama-bench. We can use it with the following command. I will run it locally as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb762d8c-e180-454a-9a10-81883900e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model                          |       size |     params | backend    | threads |          test |                  t/s |\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       8 |           pp4 |        130.44 ± 0.82 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       8 |           tg2 |         65.69 ± 0.43 |\n",
      "\n",
      "build: 0ccd7f3 (1)\n"
     ]
    }
   ],
   "source": [
    "!llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-q4_0.gguf -p 4 -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845d9ab-d976-4c44-9a1c-342960b943a6",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "\n",
    "| model                          |       size |     params | backend    | threads |          test |                  t/s |\n",
    "| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
    "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       8 |           pp4 |        130.44 ± 0.82 |\n",
    "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       8 |           tg2 |         65.69 ± 0.43 |\n",
    "\n",
    "In this example, there are two types of tests: **pp4** and **tg2**.\n",
    "\n",
    "- **pp4**: This indicates the prefill stage and measures the time in tokens per second (t/s) it takes to run the forward pass on the input prompt. The number `4` in `pp4` represents the 4 input tokens used, specified by the `-p 4` argument passed to `llama-bench`.\n",
    "\n",
    "- **tg2**: This indicates the generation stage and measures the time it takes to generate new tokens (t/s). The number `2` in `tg2` represents the number of tokens generated, specified by the `-n 2` argument passed to `llama-bench`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644185b-c1af-4db1-9381-e8df1c568d5c",
   "metadata": {},
   "source": [
    "We can now run a number of experiments. Remember in lab 2, we where able to partition matrix multiplications into different piecies of work, each of them able to be parallelized across KleidiAI microkernels. We can also acchieve this parallelization in the llama.cpp framework by using the --threads argument. Lets setup a benchmark file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab0ae2-7c2a-4c4e-aca7-18d2f1d714bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec2e933-d9e9-466b-940a-53df35d3eea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/benchmark_openelm.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/benchmark_openelm.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# benchmark the floating point 16 model and write it into \n",
    "mkdir openelm_results\n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-f16.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/f16_pi_results.txt \n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-q8_0.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/q8_pi_results.txt \n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-q4_0.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/q4_pi_results.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74446af5-d62a-4575-9655-098407fa48a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model                          |       size |     params | backend    | threads |          test |                  t/s |\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       1 |          pp12 |        198.08 ± 0.53 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       1 |           tg6 |         25.90 ± 0.06 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       2 |          pp12 |        198.36 ± 0.33 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       2 |           tg6 |         24.84 ± 0.62 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       3 |          pp12 |        194.55 ± 0.33 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       3 |           tg6 |         24.94 ± 0.59 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       4 |          pp12 |        198.21 ± 0.31 |\n",
      "| openelm 3B F16                 |   5.66 GiB |     3.04 B | Metal,BLAS |       4 |           tg6 |         25.70 ± 0.11 |\n",
      "\n",
      "build: 0ccd7f3 (1)\n",
      "| model                          |       size |     params | backend    | threads |          test |                  t/s |\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       1 |          pp12 |        183.90 ± 0.33 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       1 |           tg6 |         42.96 ± 0.22 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       2 |          pp12 |        184.56 ± 0.91 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       2 |           tg6 |         42.91 ± 0.25 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       3 |          pp12 |        184.65 ± 0.71 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       3 |           tg6 |         43.17 ± 0.32 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       4 |          pp12 |        184.01 ± 0.73 |\n",
      "| openelm 3B Q8_0                |   3.01 GiB |     3.04 B | Metal,BLAS |       4 |           tg6 |         42.75 ± 0.45 |\n",
      "\n",
      "build: 0ccd7f3 (1)\n",
      "| model                          |       size |     params | backend    | threads |          test |                  t/s |\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       1 |          pp12 |        186.83 ± 0.93 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       1 |           tg6 |         64.47 ± 0.73 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       2 |          pp12 |        186.42 ± 0.93 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       2 |           tg6 |         63.26 ± 0.37 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       3 |          pp12 |        187.42 ± 3.05 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       3 |           tg6 |         65.79 ± 0.72 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       4 |          pp12 |        188.15 ± 0.65 |\n",
      "| openelm 3B Q4_0                |   1.62 GiB |     3.04 B | Metal,BLAS |       4 |           tg6 |         65.39 ± 0.12 |\n",
      "\n",
      "build: 0ccd7f3 (1)\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./scripts/benchmark_openelm.sh\n",
    "!./scripts/benchmark_openelm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118511bf-049e-4446-a70c-c2c6354addd8",
   "metadata": {},
   "source": [
    "now you have got the local results. You should copy and past the following code into a file inside scripts/ on your graviton server you have connected to via ssh. e.g. Copy the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5f383-c043-46b3-b00f-573cf5162d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "mkdir openelm_results\n",
    "\n",
    "# benchmark the floating point 16 model and write it into \n",
    "mkdir openelm_results\n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-f16.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/f16_grav_results.txt \n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-q8_0.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/q8_grav_results.txt \n",
    "llama.cpp/build/bin/llama-bench -m models/gguf_models/OpenELM-3B-Instruct-q4_0.gguf -p 12 -n 6 --threads 1,2,3,4 | tee openelm_results/q4_grav_results.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef8bc4-3da2-4b54-a99a-995afb6f94af",
   "metadata": {},
   "source": [
    "then open a file with \n",
    "\n",
    "nano scripts/benchmark_openelm.sh \n",
    "\n",
    "here you can then paste your the contents of the above cell. Then to save and exit the file \n",
    "ctrl + o \n",
    "followed by \n",
    "ctrl + x\n",
    "\n",
    "then you should make it executable then run it. \n",
    "\n",
    "chmod +x ./scripts/benchmark_openelm.sh \n",
    "./scripts/benchmark_openelm.sh\n",
    "\n",
    "once that script has finished we can copy the results back to our pi for further analysis and comparison with the raspberry pi. you can do this with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "187d8d22-aa0c-4101-b447-f5e00537b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f16_grav_results.txt                          100% 1262    12.8KB/s   00:00    \n",
      "q4_grav_results.txt                           100% 1262    11.6KB/s   00:00    \n",
      "q8_grav_results.txt                           100% 1262    12.7KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp -i \"/Users/olivergrainge/keys/GravitonUbuntu.pem\"  -r ubuntu@ec2-13-53-171-129.eu-north-1.compute.amazonaws.com:Generative_AI_on_arm/openelm_results/* ./openelm_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87613841-ebb9-419d-8551-72cda00bca2e",
   "metadata": {},
   "source": [
    "now we have the results for both the local machine and the remote we can parse the results with the script already included in your repository "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb1fbfb7-258d-4887-8789-12dd4183dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/parse_results.py openelm_results/f16_pi_results.txt\n",
    "!python scripts/parse_results.py openelm_results/q8_pi_results.txt\n",
    "!python scripts/parse_results.py openelm_results/q4_pi_results.txt\n",
    "\n",
    "!python scripts/parse_results.py openelm_results/f16_grav_results.txt\n",
    "!python scripts/parse_results.py openelm_results/q8_grav_results.txt\n",
    "!python scripts/parse_results.py openelm_results/q4_grav_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51195633-5314-498f-926c-9945d743d2af",
   "metadata": {},
   "source": [
    "Now that should have passed the benchmarking results into a pandas dataframe we can now plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deca2eb1-aacb-4da0-ab18-52846cc9c026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size (GiB)</th>\n",
       "      <th>Params (B)</th>\n",
       "      <th>Threads</th>\n",
       "      <th>Test</th>\n",
       "      <th>Speed (t/s)</th>\n",
       "      <th>Error (t/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>pp12</td>\n",
       "      <td>27.33</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>tg6</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2</td>\n",
       "      <td>pp12</td>\n",
       "      <td>47.95</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2</td>\n",
       "      <td>tg6</td>\n",
       "      <td>18.24</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3</td>\n",
       "      <td>pp12</td>\n",
       "      <td>68.97</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3</td>\n",
       "      <td>tg6</td>\n",
       "      <td>25.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4</td>\n",
       "      <td>pp12</td>\n",
       "      <td>88.43</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>openelm 3B Q4_0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4</td>\n",
       "      <td>tg6</td>\n",
       "      <td>33.18</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Size (GiB)  Params (B)  Threads  Test  Speed (t/s)  \\\n",
       "0  openelm 3B Q4_0        1.62        3.04        1  pp12        27.33   \n",
       "1  openelm 3B Q4_0        1.62        3.04        1   tg6        10.23   \n",
       "2  openelm 3B Q4_0        1.62        3.04        2  pp12        47.95   \n",
       "3  openelm 3B Q4_0        1.62        3.04        2   tg6        18.24   \n",
       "4  openelm 3B Q4_0        1.62        3.04        3  pp12        68.97   \n",
       "5  openelm 3B Q4_0        1.62        3.04        3   tg6        25.99   \n",
       "6  openelm 3B Q4_0        1.62        3.04        4  pp12        88.43   \n",
       "7  openelm 3B Q4_0        1.62        3.04        4   tg6        33.18   \n",
       "\n",
       "   Error (t/s)  \n",
       "0         0.06  \n",
       "1         0.01  \n",
       "2         0.23  \n",
       "3         0.01  \n",
       "4         0.03  \n",
       "5         0.01  \n",
       "6         0.41  \n",
       "7         0.02  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_pi_f16 = pd.read_csv(\"openelm_results/f16_pi_results.csv\")\n",
    "df_pi_q8 = pd.read_csv(\"openelm_results/q8_pi_results.csv\")\n",
    "df_pi_q4 = pd.read_csv(\"openelm_results/q4_pi_results.csv\")\n",
    "\n",
    "df_grav_f16 = pd.read_csv(\"openelm_results/f16_grav_results.csv\")\n",
    "df_grav_q8 = pd.read_csv(\"openelm_results/q8_grav_results.csv\")\n",
    "df_grav_q4 = pd.read_csv(\"openelm_results/q4_grav_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da3312-21d2-46f8-9219-706b5bf0e592",
   "metadata": {},
   "source": [
    "Now lets plot the prefill stage latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb000a-95b5-4ab7-a0de-0895c728abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filtered_df = df[df['Test'].str.startswith('pp')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
